{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to TheHive 5 documentation website # For full documentation visit mkdocs.org .","title":"Home"},{"location":"#welcome-to-thehive-5-documentation-website","text":"For full documentation visit mkdocs.org .","title":"Welcome to TheHive 5 documentation website"},{"location":"customer-portal/","text":"","title":"Customer portal"},{"location":"support/","text":"","title":"Support"},{"location":"thehive/","text":"TheHive #","title":"TheHive"},{"location":"thehive/#thehive","text":"","title":"TheHive"},{"location":"thehive/administration/contacting-support/","text":"Contact Support #","title":"Contact Support"},{"location":"thehive/administration/contacting-support/#contact-support","text":"","title":"Contact Support"},{"location":"thehive/administration/getting-started/","text":"Getting Started # TheHive supports different roles for users. Whether you are an administrator of the platform, an organization, or an analyst, you can have access and run different actions on the platform. This user guide aims at describing all major howtos when you sign in as a Super Admin.","title":"Getting Started"},{"location":"thehive/administration/getting-started/#getting-started","text":"TheHive supports different roles for users. Whether you are an administrator of the platform, an organization, or an analyst, you can have access and run different actions on the platform. This user guide aims at describing all major howtos when you sign in as a Super Admin.","title":"Getting Started"},{"location":"thehive/administration/introduction/","text":"Introduction # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports differents roles for users. Depending on your role, you can have access and run differents actions in the platform. Administrators are users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. This user guide aims at describing all major howtos for Administrators.","title":"Introduction"},{"location":"thehive/administration/introduction/#introduction","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports differents roles for users. Depending on your role, you can have access and run differents actions in the platform. Administrators are users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. This user guide aims at describing all major howtos for Administrators.","title":"Introduction"},{"location":"thehive/administration/sign-in-as-an-admin/","text":"Sign in as Super Admin # When you sign in as a super admin, you can manage the organizations(customers) that use TheHive. A single instance can be used by many customers. Each customer can have its own data. The data cannot be exchanged between the organizations. To Sign in as a Super Admin: Open the Hive application in your browser. Enter user credentials, Login name and password . Click the Let me In button.","title":"Sign in as Super Admin"},{"location":"thehive/administration/sign-in-as-an-admin/#sign-in-as-super-admin","text":"When you sign in as a super admin, you can manage the organizations(customers) that use TheHive. A single instance can be used by many customers. Each customer can have its own data. The data cannot be exchanged between the organizations. To Sign in as a Super Admin: Open the Hive application in your browser. Enter user credentials, Login name and password . Click the Let me In button.","title":"Sign in as Super Admin"},{"location":"thehive/administration/account-settings/","text":"","title":"Index"},{"location":"thehive/administration/account-settings/configure-mfa/","text":"Configure MFA # Multi-factor authentication (MFA) is a security technology that requires at least two authentication methods from different types of credentials to verify a user's identity for login to an application or other transaction. Multi-factor authentication strengthens your account security significantly. You can download any one of the following Multi-Factor Authentication apps: iOS devices: Authy Android devices: Authy Windows devices: Microsoft Authenticator To Configure MFA: Submit a token using the QR code or the Secret key to validate MFA activation. If you copy the secret key a message is displayed.","title":"Configure MFA"},{"location":"thehive/administration/account-settings/configure-mfa/#configure-mfa","text":"Multi-factor authentication (MFA) is a security technology that requires at least two authentication methods from different types of credentials to verify a user's identity for login to an application or other transaction. Multi-factor authentication strengthens your account security significantly. You can download any one of the following Multi-Factor Authentication apps: iOS devices: Authy Android devices: Authy Windows devices: Microsoft Authenticator To Configure MFA: Submit a token using the QR code or the Secret key to validate MFA activation. If you copy the secret key a message is displayed.","title":"Configure MFA"},{"location":"thehive/administration/account-settings/options/","text":"","title":"Options"},{"location":"thehive/administration/account-settings/switch-workspace/","text":"","title":"Switch workspace"},{"location":"thehive/administration/account-settings/api-key/create/","text":"Create API Key # In this section, you can find information about creating an API key in account settings. To create an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Create button to create a new API key. A new API key is generated and a message is displayed.","title":"Create API Key"},{"location":"thehive/administration/account-settings/api-key/create/#create-api-key","text":"In this section, you can find information about creating an API key in account settings. To create an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Create button to create a new API key. A new API key is generated and a message is displayed.","title":"Create API Key"},{"location":"thehive/administration/account-settings/api-key/renew/","text":"Renew API Key # In this section, you can find information about renewing an API key in account settings. To renew an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Renew button to renew API key. API key created message appears.","title":"Renew API Key"},{"location":"thehive/administration/account-settings/api-key/renew/#renew-api-key","text":"In this section, you can find information about renewing an API key in account settings. To renew an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Renew button to renew API key. API key created message appears.","title":"Renew API Key"},{"location":"thehive/administration/account-settings/api-key/reveal/","text":"Reveal API Key # In this section, you can find information about revealing the API key in account settings. To reveal the API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Reveal button to reveal the API key.","title":"Reveal API Key"},{"location":"thehive/administration/account-settings/api-key/reveal/#reveal-api-key","text":"In this section, you can find information about revealing the API key in account settings. To reveal the API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Reveal button to reveal the API key.","title":"Reveal API Key"},{"location":"thehive/administration/account-settings/api-key/revoke/","text":"Revoke API Key # In this section, you can find information about revoking an API key in account settings. To revoke an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Revoke button to revoke the API key. API key revoked message appears.","title":"Revoke API Key"},{"location":"thehive/administration/account-settings/api-key/revoke/#revoke-api-key","text":"In this section, you can find information about revoking an API key in account settings. To revoke an API key: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the API Key tab. Click the Revoke button to revoke the API key. API key revoked message appears.","title":"Revoke API Key"},{"location":"thehive/administration/account-settings/infos/change-name/","text":"Change User Name # In this section, you can find information about changing the user name in account settings. To change the user name: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Change the Name if required. Click the Save button. User edited successfully message appears.","title":"Change User Name"},{"location":"thehive/administration/account-settings/infos/change-name/#change-user-name","text":"In this section, you can find information about changing the user name in account settings. To change the user name: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Change the Name if required. Click the Save button. User edited successfully message appears.","title":"Change User Name"},{"location":"thehive/administration/account-settings/infos/view-permission-list/","text":"View Permission List # In this section, you can find information about the permission list in account settings. To view the permissions: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. You can view the permissions list in the infos tab.","title":"View Permission List"},{"location":"thehive/administration/account-settings/infos/view-permission-list/#view-permission-list","text":"In this section, you can find information about the permission list in account settings. To view the permissions: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. You can view the permissions list in the infos tab.","title":"View Permission List"},{"location":"thehive/administration/account-settings/password/manage-password/","text":"Manage Password # In this section, you can find information about changing the password in account settings. To change the password: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the Password tab. Enter the Current password . Enter the New Password and Confirm new password . Click the Save button.","title":"Manage Password"},{"location":"thehive/administration/account-settings/password/manage-password/#manage-password","text":"In this section, you can find information about changing the password in account settings. To change the password: On the home page, click the arrow next to your profile picture. A drop down menu opens. Click Settings . The infos tab in the Account settings window opens. Select the Password tab. Enter the Current password . Enter the New Password and Confirm new password . Click the Save button.","title":"Manage Password"},{"location":"thehive/administration/entities-management/","text":"","title":"Index"},{"location":"thehive/administration/entities-management/about-entities-management/","text":"About Entities Management # The Entities Management section allows you to configure all the reference data used like the list of statuses, list of profiles, and list of custom fields. There are different sections for every type of object.","title":"About Entities Management"},{"location":"thehive/administration/entities-management/about-entities-management/#about-entities-management","text":"The Entities Management section allows you to configure all the reference data used like the list of statuses, list of profiles, and list of custom fields. There are different sections for every type of object.","title":"About Entities Management"},{"location":"thehive/administration/entities-management/alert-status/about-alert-status/","text":"About Alert Status # By default, the TheHive application comes with a list of pre-defined alert statuses. An alert status describes the status of an alert where the stage value is New, InProgress, Closed, or imported. You can also customize the labels of the alert statuses. e.g., You can customize the InProgress status as \"Under Investigation\" or \"Pending Customer Reply\" value, depending on your requirement. You can see the list of all alert statuses on the Entities Management page, in the alert Status tab.","title":"About Alert Status"},{"location":"thehive/administration/entities-management/alert-status/about-alert-status/#about-alert-status","text":"By default, the TheHive application comes with a list of pre-defined alert statuses. An alert status describes the status of an alert where the stage value is New, InProgress, Closed, or imported. You can also customize the labels of the alert statuses. e.g., You can customize the InProgress status as \"Under Investigation\" or \"Pending Customer Reply\" value, depending on your requirement. You can see the list of all alert statuses on the Entities Management page, in the alert Status tab.","title":"About Alert Status"},{"location":"thehive/administration/entities-management/alert-status/add-edit-delete-alert-status/","text":"Manage Alert Status # In this section, you can find information about managing the alert status. You can add, delete, and edit the alert status. Add Custom Status # To add a custom alert: On the Entities Management page, in the Alert status tab, click the + button. Add a custom status window opens. Select the Stage from the list. Enter a custom Value if you want to customize the case label. Select a Color for the label. Click the Confirm custom status creation button. A custom status created successfully message appears. Edit Custom Status # To edit a custom status: On the Entities Management page, in the Alert status tab, click the ellipsis(...) corresponding to the value you want to edit. Click Edit . Edit a custom status window opens. Edit the required fields. Click the Confirm custom status update button. Custom status edited succesfully message appears. Delete Custom Status # To delete a custom Alert: On the Entities Management page, in the Alert status tab, click the ellipsis(...) corresponding to the value you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Alert Status deleted succesfully message appears.","title":"Manage Alert Status"},{"location":"thehive/administration/entities-management/alert-status/add-edit-delete-alert-status/#manage-alert-status","text":"In this section, you can find information about managing the alert status. You can add, delete, and edit the alert status.","title":"Manage Alert Status"},{"location":"thehive/administration/entities-management/alert-status/add-edit-delete-alert-status/#add-custom-status","text":"To add a custom alert: On the Entities Management page, in the Alert status tab, click the + button. Add a custom status window opens. Select the Stage from the list. Enter a custom Value if you want to customize the case label. Select a Color for the label. Click the Confirm custom status creation button. A custom status created successfully message appears.","title":"Add Custom Status"},{"location":"thehive/administration/entities-management/alert-status/add-edit-delete-alert-status/#edit-custom-status","text":"To edit a custom status: On the Entities Management page, in the Alert status tab, click the ellipsis(...) corresponding to the value you want to edit. Click Edit . Edit a custom status window opens. Edit the required fields. Click the Confirm custom status update button. Custom status edited succesfully message appears.","title":"Edit Custom Status"},{"location":"thehive/administration/entities-management/alert-status/add-edit-delete-alert-status/#delete-custom-status","text":"To delete a custom Alert: On the Entities Management page, in the Alert status tab, click the ellipsis(...) corresponding to the value you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Alert Status deleted succesfully message appears.","title":"Delete Custom Status"},{"location":"thehive/administration/entities-management/alert-status/types-of-status/","text":"Types of Status # By default, the TheHive application comes with the following pre-defined values for the case statuses: New InProgress Closed Imported You can customize these values as per your requirements.","title":"Types of Status"},{"location":"thehive/administration/entities-management/alert-status/types-of-status/#types-of-status","text":"By default, the TheHive application comes with the following pre-defined values for the case statuses: New InProgress Closed Imported You can customize these values as per your requirements.","title":"Types of Status"},{"location":"thehive/administration/entities-management/analyzer-templates/about-analyzer-templates/","text":"About Analyzer Templates # Analyzers templates are angular js code used to display a human-readable report of an analyzer job report. In TheHive, you can add observables. They are technical elements that can be analyzed using analyzers. Analyzers will take the technical element as input and produce a JSON formatted report as output. This JSON report may not be human-friendly when looking for information into this report, and this report can be very long. While analysts use these analyzers to get accurate information in a timely manner, we provide analyzers-template to format this JSON report and provide something more human-readable or put the most accurate information on the top of the list/more visible for the analysts. This analyzers-template can be added, modified, or deleted in TheHive for each existing analyzer.","title":"About Analyzer Templates"},{"location":"thehive/administration/entities-management/analyzer-templates/about-analyzer-templates/#about-analyzer-templates","text":"Analyzers templates are angular js code used to display a human-readable report of an analyzer job report. In TheHive, you can add observables. They are technical elements that can be analyzed using analyzers. Analyzers will take the technical element as input and produce a JSON formatted report as output. This JSON report may not be human-friendly when looking for information into this report, and this report can be very long. While analysts use these analyzers to get accurate information in a timely manner, we provide analyzers-template to format this JSON report and provide something more human-readable or put the most accurate information on the top of the list/more visible for the analysts. This analyzers-template can be added, modified, or deleted in TheHive for each existing analyzer.","title":"About Analyzer Templates"},{"location":"thehive/administration/entities-management/analyzer-templates/delete-template/","text":"Delete Template # In this section, you can find information about deleting a template. On the Entities Management page, in the Analyzer templates tab, click the ellipsis(...) corresponding to the template you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Template removed succesfully message appears.","title":"Delete Template"},{"location":"thehive/administration/entities-management/analyzer-templates/delete-template/#delete-template","text":"In this section, you can find information about deleting a template. On the Entities Management page, in the Analyzer templates tab, click the ellipsis(...) corresponding to the template you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Template removed succesfully message appears.","title":"Delete Template"},{"location":"thehive/administration/entities-management/analyzer-templates/describes-each-analyzer-template/","text":"Describe Each Analyzer Template # There are two types of analyzers templates: Long template: The one you will see when opening the analysis report on an observable page Short template: Small tag that will be added to an observable. You can see them on the observables page. Long template example: # The following is a \"raw\" report. It is an analyzer report that hasn't been formatted by an analyzer template. The same report after formatted by the \"long\" analyzer-template is: Short template example: # These are short reports, providing a very concise summary of the analysis report.","title":"Describe Each Analyzer Template"},{"location":"thehive/administration/entities-management/analyzer-templates/describes-each-analyzer-template/#describe-each-analyzer-template","text":"There are two types of analyzers templates: Long template: The one you will see when opening the analysis report on an observable page Short template: Small tag that will be added to an observable. You can see them on the observables page.","title":"Describe Each Analyzer Template"},{"location":"thehive/administration/entities-management/analyzer-templates/describes-each-analyzer-template/#long-template-example","text":"The following is a \"raw\" report. It is an analyzer report that hasn't been formatted by an analyzer template. The same report after formatted by the \"long\" analyzer-template is:","title":"Long template example:"},{"location":"thehive/administration/entities-management/analyzer-templates/describes-each-analyzer-template/#short-template-example","text":"These are short reports, providing a very concise summary of the analysis report.","title":"Short template example:"},{"location":"thehive/administration/entities-management/analyzer-templates/import-template-archive/","text":"Import Templates Archive # In this section, you can find information about importing templates archive. To import templates archive: On the Entities Management page, in the Analyzer templates tab, click the Import templates archive button. Import templates archive window opens. Select the templates archive file. Click the Import button.","title":"Import Templates Archive"},{"location":"thehive/administration/entities-management/analyzer-templates/import-template-archive/#import-templates-archive","text":"In this section, you can find information about importing templates archive. To import templates archive: On the Entities Management page, in the Analyzer templates tab, click the Import templates archive button. Import templates archive window opens. Select the templates archive file. Click the Import button.","title":"Import Templates Archive"},{"location":"thehive/administration/entities-management/analyzer-templates/update-template/","text":"Update Template # In this section, you can find information about updating a template. On the Entities Management page, in the Analyzer templates tab, click the ellipsis(...) corresponding to the template you want to update. Click Template . Update analyzer template window opens. Update the analyzer template details. Click Confirm . Template edited succesfully message appears.","title":"Update Template"},{"location":"thehive/administration/entities-management/analyzer-templates/update-template/#update-template","text":"In this section, you can find information about updating a template. On the Entities Management page, in the Analyzer templates tab, click the ellipsis(...) corresponding to the template you want to update. Click Template . Update analyzer template window opens. Update the analyzer template details. Click Confirm . Template edited succesfully message appears.","title":"Update Template"},{"location":"thehive/administration/entities-management/attack-patterns/about-attack-patterns/","text":"About Attack Patterns # MITRE ATT&CK is an organization that describes the behaviors that attackers can use to make a security attack. They classify those behaviors on tactics, and within each tactic, they define many techniques. These techniques are the cells in the MITRE ATT&CK matrix. ATT&CK, which stands for Adversarial Tactics, Techniques, and Common Knowledge, can help you understand how cyber attackers think and work. The matrix allows you to give an overview of how the attacker did the attack, by which phases they went through. In TheHIve application, you can import all the catalogs of the MITRE ATT&CK matrix and attach the attack patterns to your incidence. you can view all the attack patterns on the Entities Management page, in the Attack Patterns tab.","title":"About Attack Patterns"},{"location":"thehive/administration/entities-management/attack-patterns/about-attack-patterns/#about-attack-patterns","text":"MITRE ATT&CK is an organization that describes the behaviors that attackers can use to make a security attack. They classify those behaviors on tactics, and within each tactic, they define many techniques. These techniques are the cells in the MITRE ATT&CK matrix. ATT&CK, which stands for Adversarial Tactics, Techniques, and Common Knowledge, can help you understand how cyber attackers think and work. The matrix allows you to give an overview of how the attacker did the attack, by which phases they went through. In TheHIve application, you can import all the catalogs of the MITRE ATT&CK matrix and attach the attack patterns to your incidence. you can view all the attack patterns on the Entities Management page, in the Attack Patterns tab.","title":"About Attack Patterns"},{"location":"thehive/administration/entities-management/attack-patterns/filter-the-list/","text":"Filter the List # In this section, you can find information about applying filters to attack patterns. To apply filter to a attack pattern: On the Entities Management page, in the Attack Patterns tab, click the link of the attack pattern. Switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter the List"},{"location":"thehive/administration/entities-management/attack-patterns/filter-the-list/#filter-the-list","text":"In this section, you can find information about applying filters to attack patterns. To apply filter to a attack pattern: On the Entities Management page, in the Attack Patterns tab, click the link of the attack pattern. Switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter the List"},{"location":"thehive/administration/entities-management/attack-patterns/import-mitre-attack-patterns/","text":"Import Mitre Attack Patterns # In this section, you can find information about importing Mitre Attck Patterns. To import the Mitre Attack Patterns: On the Entities Management page, in the Attack Patterns tab, click the Import MITRE ATT&CK patterns button. Import MITRE ATT&CK patterns window opens. Select the Name of the catalog from the list. Click the Import button.","title":"Import Mitre Attack Patterns"},{"location":"thehive/administration/entities-management/attack-patterns/import-mitre-attack-patterns/#import-mitre-attack-patterns","text":"In this section, you can find information about importing Mitre Attck Patterns. To import the Mitre Attack Patterns: On the Entities Management page, in the Attack Patterns tab, click the Import MITRE ATT&CK patterns button. Import MITRE ATT&CK patterns window opens. Select the Name of the catalog from the list. Click the Import button.","title":"Import Mitre Attack Patterns"},{"location":"thehive/administration/entities-management/attack-patterns/view-the-attack-details/","text":"View The Attack Details # In this section, you can find information about viewing the attack pattern details. To view the details of an attack pattern: On the Entities Management page, in the Attack Patterns tab, click the link of the attack pattern whose details you want to see. The attack pattern details window opens. You can see the sub technique ID, name, technique, tactics, and description. Preview Technique Details # To preview the technique details: In the attack pattern details window, click the Preview button corresponding to the sub-technique name to see details. A new window opens where you can all the details of the sub-technique.","title":"View The Attack Details"},{"location":"thehive/administration/entities-management/attack-patterns/view-the-attack-details/#view-the-attack-details","text":"In this section, you can find information about viewing the attack pattern details. To view the details of an attack pattern: On the Entities Management page, in the Attack Patterns tab, click the link of the attack pattern whose details you want to see. The attack pattern details window opens. You can see the sub technique ID, name, technique, tactics, and description.","title":"View The Attack Details"},{"location":"thehive/administration/entities-management/attack-patterns/view-the-attack-details/#preview-technique-details","text":"To preview the technique details: In the attack pattern details window, click the Preview button corresponding to the sub-technique name to see details. A new window opens where you can all the details of the sub-technique.","title":"Preview Technique Details"},{"location":"thehive/administration/entities-management/case-status/about-case-status/","text":"About Case Status # By default, the TheHive application comes with a list of pre-defined case statuses. A case status describes the status of a case where the stage is New, InProgress, or Closed. You can customize the labels of the statuses. e.g., You can customize the InProgress status as \"Under Investigation\" or \"Pending Customer Reply\" value, depending on your requirement. You can see the list of all Case Statuses on the Entities Management page, in the Case Status tab.","title":"About Case Status"},{"location":"thehive/administration/entities-management/case-status/about-case-status/#about-case-status","text":"By default, the TheHive application comes with a list of pre-defined case statuses. A case status describes the status of a case where the stage is New, InProgress, or Closed. You can customize the labels of the statuses. e.g., You can customize the InProgress status as \"Under Investigation\" or \"Pending Customer Reply\" value, depending on your requirement. You can see the list of all Case Statuses on the Entities Management page, in the Case Status tab.","title":"About Case Status"},{"location":"thehive/administration/entities-management/case-status/add-edit-delete-custom-status/","text":"Manage Custom Status # In this section, you can find information about managing the case status. You can add, delete, and edit the case status. Add Custom Status # To add a custom status: On the Entities Management page, in the Case Status tab, click the + button. Add a Custom Status window opens. Select the Stage from the list. Enter a custom Value if you want to customize the case label. Select a Color for the label. Click the Confirm custom status creation button. A custom status created successfully message appears. Edit Custom Status # To edit a custom status: On the Entities Management page, in the case status tab, click the ellipsis(...) corresponding to the value you want to edit. Click Edit . Edit a Custom Case window opens. Edit the required fields. Click the Confirm custom status update button. Case status edited succesfully message appears. Delete Custom Status # To delete a custom status: On the Entities Management page, in the case status tab, click the ellipsis(...) corresponding to the value you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Custom Status deleted succesfully message appears.","title":"Manage Custom Status"},{"location":"thehive/administration/entities-management/case-status/add-edit-delete-custom-status/#manage-custom-status","text":"In this section, you can find information about managing the case status. You can add, delete, and edit the case status.","title":"Manage Custom Status"},{"location":"thehive/administration/entities-management/case-status/add-edit-delete-custom-status/#add-custom-status","text":"To add a custom status: On the Entities Management page, in the Case Status tab, click the + button. Add a Custom Status window opens. Select the Stage from the list. Enter a custom Value if you want to customize the case label. Select a Color for the label. Click the Confirm custom status creation button. A custom status created successfully message appears.","title":"Add Custom Status"},{"location":"thehive/administration/entities-management/case-status/add-edit-delete-custom-status/#edit-custom-status","text":"To edit a custom status: On the Entities Management page, in the case status tab, click the ellipsis(...) corresponding to the value you want to edit. Click Edit . Edit a Custom Case window opens. Edit the required fields. Click the Confirm custom status update button. Case status edited succesfully message appears.","title":"Edit Custom Status"},{"location":"thehive/administration/entities-management/case-status/add-edit-delete-custom-status/#delete-custom-status","text":"To delete a custom status: On the Entities Management page, in the case status tab, click the ellipsis(...) corresponding to the value you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Custom Status deleted succesfully message appears.","title":"Delete Custom Status"},{"location":"thehive/administration/entities-management/case-status/types-of-status/","text":"Types of Status # By default, the TheHive application comes with the following pre-defined values for the case statuses: New InProgress Closed You can customize these values as per your requirements.","title":"Types of Status"},{"location":"thehive/administration/entities-management/case-status/types-of-status/#types-of-status","text":"By default, the TheHive application comes with the following pre-defined values for the case statuses: New InProgress Closed You can customize these values as per your requirements.","title":"Types of Status"},{"location":"thehive/administration/entities-management/custom-fields/about-custom-fields/","text":"About Custom Fields # Custom fields are like custom properties that we can attach to cases and alerts. By default, in the TheHive platform, cases and alerts have pre-defined fields. If you need to add a new field to the alert or case, you can create a custom field. You can view the custom fields list in the Custom Fields tab of the Entities Management page. NOTE : Custom fields are global to all the organizations.","title":"About Custom Fields"},{"location":"thehive/administration/entities-management/custom-fields/about-custom-fields/#about-custom-fields","text":"Custom fields are like custom properties that we can attach to cases and alerts. By default, in the TheHive platform, cases and alerts have pre-defined fields. If you need to add a new field to the alert or case, you can create a custom field. You can view the custom fields list in the Custom Fields tab of the Entities Management page. NOTE : Custom fields are global to all the organizations.","title":"About Custom Fields"},{"location":"thehive/administration/entities-management/custom-fields/add-edit-delete-custom-field/","text":"Manage Custom Fields # In this section, you can find information about managing the custom fields. You can add, delete, and edit the custom fields. Add Custom Field # To add a new custom field: On the Entities Management page, in the Custom Fields tab, click the + button. Adding a Custom Field window opens. Enter a Display name for the custom field. Enter a Technical name . Enter a Description . Select the Group from the list. Select a Type from the list. Switch on the Mandatory button. When you make a custom field as a mandatory, you won't be able to resolve a case without populating the mandatory custom field. Click the Confirm custom field creation button. A custom field created successfully message appears. Edit Custom Field # To edit a custom field: On the Entities Management page, in the Custom Fields tab, click the ellipsis(...) corresponding to the field you want to edit. Click Edit . Editing a Custom Field window opens. Edit the required details. Click the Confirm custom field edition button. Custom field edited successfully message appears. Delete Custom Field # To delete a custom field: On the Entities Management page, in the Custom Fields tab, click the ellipsis(...) corresponding to the field you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Custom Field deleted successfully message appears.","title":"Manage Custom Fields"},{"location":"thehive/administration/entities-management/custom-fields/add-edit-delete-custom-field/#manage-custom-fields","text":"In this section, you can find information about managing the custom fields. You can add, delete, and edit the custom fields.","title":"Manage Custom Fields"},{"location":"thehive/administration/entities-management/custom-fields/add-edit-delete-custom-field/#add-custom-field","text":"To add a new custom field: On the Entities Management page, in the Custom Fields tab, click the + button. Adding a Custom Field window opens. Enter a Display name for the custom field. Enter a Technical name . Enter a Description . Select the Group from the list. Select a Type from the list. Switch on the Mandatory button. When you make a custom field as a mandatory, you won't be able to resolve a case without populating the mandatory custom field. Click the Confirm custom field creation button. A custom field created successfully message appears.","title":"Add Custom Field"},{"location":"thehive/administration/entities-management/custom-fields/add-edit-delete-custom-field/#edit-custom-field","text":"To edit a custom field: On the Entities Management page, in the Custom Fields tab, click the ellipsis(...) corresponding to the field you want to edit. Click Edit . Editing a Custom Field window opens. Edit the required details. Click the Confirm custom field edition button. Custom field edited successfully message appears.","title":"Edit Custom Field"},{"location":"thehive/administration/entities-management/custom-fields/add-edit-delete-custom-field/#delete-custom-field","text":"To delete a custom field: On the Entities Management page, in the Custom Fields tab, click the ellipsis(...) corresponding to the field you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Custom Field deleted successfully message appears.","title":"Delete Custom Field"},{"location":"thehive/administration/entities-management/custom-fields/filter-custom-field/","text":"Apply Filters to Custom Field # In this section, you can find information about applying filters to a custom field. To apply filter to a custom field: On the Entities Management page, in the Custom Field tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Apply Filters to Custom Field"},{"location":"thehive/administration/entities-management/custom-fields/filter-custom-field/#apply-filters-to-custom-field","text":"In this section, you can find information about applying filters to a custom field. To apply filter to a custom field: On the Entities Management page, in the Custom Field tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Apply Filters to Custom Field"},{"location":"thehive/administration/entities-management/observable-types/about-observable-type/","text":"About Observable Type # When you install the TheHive application, it comes with a set of pre-defined observables such as IP and email addresses, URLs, domain names, files or hashes. observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). You can define your own observable type. You can see the list of all Observable Types on the Entities Management page, in the Observable Types tab.","title":"About Observable Type"},{"location":"thehive/administration/entities-management/observable-types/about-observable-type/#about-observable-type","text":"When you install the TheHive application, it comes with a set of pre-defined observables such as IP and email addresses, URLs, domain names, files or hashes. observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). You can define your own observable type. You can see the list of all Observable Types on the Entities Management page, in the Observable Types tab.","title":"About Observable Type"},{"location":"thehive/administration/entities-management/observable-types/add-edit-delete-observable-type/","text":"Manage Observable Type # In this section, you can find information about managing the observable type. You can add, delete, and edit an observable type. Add Observable Type # To add a new observable type: On the Entities Management page, in the Observable Types tab, click the + button. Adding an Observable Type window opens. Enter a Name for an observable type. Switch on the Attachment button to attach a file. Click the Confirm observable type creation button. A observable type created successfully message apperars. Delete Observable Type # To delete an observable type: On the Entities Management page, in the observable types tab, click the ellipsis(...) corresponding to the type you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Observable Type deleted succesfully message appears.","title":"Manage Observable Type"},{"location":"thehive/administration/entities-management/observable-types/add-edit-delete-observable-type/#manage-observable-type","text":"In this section, you can find information about managing the observable type. You can add, delete, and edit an observable type.","title":"Manage Observable Type"},{"location":"thehive/administration/entities-management/observable-types/add-edit-delete-observable-type/#add-observable-type","text":"To add a new observable type: On the Entities Management page, in the Observable Types tab, click the + button. Adding an Observable Type window opens. Enter a Name for an observable type. Switch on the Attachment button to attach a file. Click the Confirm observable type creation button. A observable type created successfully message apperars.","title":"Add Observable Type"},{"location":"thehive/administration/entities-management/observable-types/add-edit-delete-observable-type/#delete-observable-type","text":"To delete an observable type: On the Entities Management page, in the observable types tab, click the ellipsis(...) corresponding to the type you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Observable Type deleted succesfully message appears.","title":"Delete Observable Type"},{"location":"thehive/administration/entities-management/observable-types/filter-observable-type/","text":"Filter Observable Type # In this section, you can find information about applying filters to an observable type. To apply filter to an observable type: On the Entities Management page, in the Observable Types tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Observable Type"},{"location":"thehive/administration/entities-management/observable-types/filter-observable-type/#filter-observable-type","text":"In this section, you can find information about applying filters to an observable type. To apply filter to an observable type: On the Entities Management page, in the Observable Types tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Observable Type"},{"location":"thehive/administration/entities-management/profiles/about-permissions/","text":"About Permissions # A set of permissions defines every user profile. When you install TheHive, you have some pre-defined profiles like admin, analyst, org-admin, and read-only. Every profile has a set of permissions assigned to it. You can view the profiles and their set of permissions in the Profiles tab of the Entities Management page. There are two Profile Types; Administration and Organization. Each Profile Type has a different set of permissions. Administration profile has a set of permissions which help to manage the platform. The types of permissions for Administration profile are: Organization profile is for users like the security analysts who wno't manage the platform but they will be a part of the organization that make incident response and make a case management. The types of permissions for Organization profile are:","title":"About Permissions"},{"location":"thehive/administration/entities-management/profiles/about-permissions/#about-permissions","text":"A set of permissions defines every user profile. When you install TheHive, you have some pre-defined profiles like admin, analyst, org-admin, and read-only. Every profile has a set of permissions assigned to it. You can view the profiles and their set of permissions in the Profiles tab of the Entities Management page. There are two Profile Types; Administration and Organization. Each Profile Type has a different set of permissions. Administration profile has a set of permissions which help to manage the platform. The types of permissions for Administration profile are: Organization profile is for users like the security analysts who wno't manage the platform but they will be a part of the organization that make incident response and make a case management. The types of permissions for Organization profile are:","title":"About Permissions"},{"location":"thehive/administration/entities-management/profiles/about-profile/","text":"About Profile # When you install TheHive, you have many predefined profiles like the admin, analyst, org-admin, and read-only. Every profile is defined by a set of permissions. You can also create a new profile with a new set of permissions.","title":"About Profile"},{"location":"thehive/administration/entities-management/profiles/about-profile/#about-profile","text":"When you install TheHive, you have many predefined profiles like the admin, analyst, org-admin, and read-only. Every profile is defined by a set of permissions. You can also create a new profile with a new set of permissions.","title":"About Profile"},{"location":"thehive/administration/entities-management/profiles/add-edit-delete-profile/","text":"Manage Profiles # In this section, you can find information about managing the profiles. You can add, delete, and edit the user profiles. Add Profile # To add a new profile: On the Entities Management page, in the Profiles tab, click the + button. Adding a Profile window opens. Enter a Name for the new profile. Choose Profile type . Select the Permissions for that profile type. Click the Confirm profile creation button. A profile created successfully message appears. Edit Profile # To edit a profile: On the Entities Management page, in the Profiles tab, click the ellipsis(...) corresponding to the profile you want to edit. Click Edit . Editing a Profile window opens. Edit the required details. Click the Confirm profile edition button. Delete Profile # To delete a profile: On the Entities Management page, in the Profiles tab, click the ellipsis(...) corresponding to the profile you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Profile deleted succesfully message appears.","title":"Manage Profiles"},{"location":"thehive/administration/entities-management/profiles/add-edit-delete-profile/#manage-profiles","text":"In this section, you can find information about managing the profiles. You can add, delete, and edit the user profiles.","title":"Manage Profiles"},{"location":"thehive/administration/entities-management/profiles/add-edit-delete-profile/#add-profile","text":"To add a new profile: On the Entities Management page, in the Profiles tab, click the + button. Adding a Profile window opens. Enter a Name for the new profile. Choose Profile type . Select the Permissions for that profile type. Click the Confirm profile creation button. A profile created successfully message appears.","title":"Add Profile"},{"location":"thehive/administration/entities-management/profiles/add-edit-delete-profile/#edit-profile","text":"To edit a profile: On the Entities Management page, in the Profiles tab, click the ellipsis(...) corresponding to the profile you want to edit. Click Edit . Editing a Profile window opens. Edit the required details. Click the Confirm profile edition button.","title":"Edit Profile"},{"location":"thehive/administration/entities-management/profiles/add-edit-delete-profile/#delete-profile","text":"To delete a profile: On the Entities Management page, in the Profiles tab, click the ellipsis(...) corresponding to the profile you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . Profile deleted succesfully message appears.","title":"Delete Profile"},{"location":"thehive/administration/entities-management/profiles/filter-profile/","text":"Filter Profile # In this section, you can find information about applying filters to the profile. To apply filter to a profile: On the Entities Management page, in the Profile tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Profile"},{"location":"thehive/administration/entities-management/profiles/filter-profile/#filter-profile","text":"In this section, you can find information about applying filters to the profile. To apply filter to a profile: On the Entities Management page, in the Profile tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Profile"},{"location":"thehive/administration/entities-management/profiles/manage-views/","text":"Manange Profile View # In this section you can find information about how to save and manage the current profile list view. To save the current profile list view: On the Entities Management page, in the Profiles tab, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message. To delete a view: On the Entities Management page, in the Profiles tab, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Manange Profile View"},{"location":"thehive/administration/entities-management/profiles/manage-views/#manange-profile-view","text":"In this section you can find information about how to save and manage the current profile list view. To save the current profile list view: On the Entities Management page, in the Profiles tab, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message. To delete a view: On the Entities Management page, in the Profiles tab, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Manange Profile View"},{"location":"thehive/administration/entities-management/profiles/view-user-profiles/","text":"View User Profiles # After you log in to the application, you are on the Organization list page by default. To view the user profiles: On the left side of the window, click the Entities Management tab. The Profiles tab on the Entities Management page opens. In the Profiles section, you can see the list of all profile names and their corresponding permissions.","title":"View User Profiles"},{"location":"thehive/administration/entities-management/profiles/view-user-profiles/#view-user-profiles","text":"After you log in to the application, you are on the Organization list page by default. To view the user profiles: On the left side of the window, click the Entities Management tab. The Profiles tab on the Entities Management page opens. In the Profiles section, you can see the list of all profile names and their corresponding permissions.","title":"View User Profiles"},{"location":"thehive/administration/entities-management/taxonomies/about-taxonomies/","text":"About Taxonomies # The TheHive application introduces the support of Taxonomies as it is defined and published by MISP. These set of classification libraries can be used in THeHive to tag Cases, Observables and Alerts. Taxonomies are likes groups or catalogues or custom packs. You can import the list of taxonomies and use them to classify the cases and alerts using the pre-defined tags from the taxonomies. You can enable the taxonomies you want to use. By default, TheHive does not contain any taxonomy. You can view the list of all imported taxonomies on the Entities Management page, in the Taxonomies tab.","title":"About Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/about-taxonomies/#about-taxonomies","text":"The TheHive application introduces the support of Taxonomies as it is defined and published by MISP. These set of classification libraries can be used in THeHive to tag Cases, Observables and Alerts. Taxonomies are likes groups or catalogues or custom packs. You can import the list of taxonomies and use them to classify the cases and alerts using the pre-defined tags from the taxonomies. You can enable the taxonomies you want to use. By default, TheHive does not contain any taxonomy. You can view the list of all imported taxonomies on the Entities Management page, in the Taxonomies tab.","title":"About Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/activate-taxonomy/","text":"Activate Taxonomy # In this section, you can find information about activating/deactivating the taxonomies. To activate a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to activate. Click Activate . Taxonomy eanbled message appears. Deactivate Taxonomy # To deactivate a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to deactivate. Click Deactivate . Taxonomy disabled message appears.","title":"Activate Taxonomy"},{"location":"thehive/administration/entities-management/taxonomies/activate-taxonomy/#activate-taxonomy","text":"In this section, you can find information about activating/deactivating the taxonomies. To activate a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to activate. Click Activate . Taxonomy eanbled message appears.","title":"Activate Taxonomy"},{"location":"thehive/administration/entities-management/taxonomies/activate-taxonomy/#deactivate-taxonomy","text":"To deactivate a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to deactivate. Click Deactivate . Taxonomy disabled message appears.","title":"Deactivate Taxonomy"},{"location":"thehive/administration/entities-management/taxonomies/delete-taxonomy/","text":"Delete Taxonomy # In this section, you can find information about deleting the taxonomies. To delete a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to delete. Click Delete .","title":"Delete Taxonomy"},{"location":"thehive/administration/entities-management/taxonomies/delete-taxonomy/#delete-taxonomy","text":"In this section, you can find information about deleting the taxonomies. To delete a taxonomy: On the Entities Management page, in the Taxonomies tab, click the ellipsis (...) corresponding to the taxonomy you want to delete. Click Delete .","title":"Delete Taxonomy"},{"location":"thehive/administration/entities-management/taxonomies/filter-taxonomies-list/","text":"Filter Taxonomies # In this section, you can find information about applying filters to taxonomies. To apply filter to taxonomies: On the Entities Management page, in the Taxonomies tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/filter-taxonomies-list/#filter-taxonomies","text":"In this section, you can find information about applying filters to taxonomies. To apply filter to taxonomies: On the Entities Management page, in the Taxonomies tab, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/import-taxonomies/","text":"Import Taxonomies # In this section, you can find information about importing taxonomies. To access and import taxonomies: On the Entities Management page, in the Taxonomies tab, click the Import taxonomies button. Import taxonomies archive window opens. Select the file containing the libraries. Click the Import button.","title":"Import Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/import-taxonomies/#import-taxonomies","text":"In this section, you can find information about importing taxonomies. To access and import taxonomies: On the Entities Management page, in the Taxonomies tab, click the Import taxonomies button. Import taxonomies archive window opens. Select the file containing the libraries. Click the Import button.","title":"Import Taxonomies"},{"location":"thehive/administration/entities-management/taxonomies/view-taxonomy-details/","text":"View Taxonomy Details # In this section, you can find information about viewing taxonomy details. To view taxonomy details: On the Entities Management page, select the Taxonomies tab. You can view the list of all imported taxonomies. Click the NAMESPACE (Taxonomy Name) whose details you want to see. The taxonomy details window opens.","title":"View Taxonomy Details"},{"location":"thehive/administration/entities-management/taxonomies/view-taxonomy-details/#view-taxonomy-details","text":"In this section, you can find information about viewing taxonomy details. To view taxonomy details: On the Entities Management page, select the Taxonomies tab. You can view the list of all imported taxonomies. Click the NAMESPACE (Taxonomy Name) whose details you want to see. The taxonomy details window opens.","title":"View Taxonomy Details"},{"location":"thehive/administration/organization/","text":"","title":"Index"},{"location":"thehive/administration/organization/about-organization/","text":"About Organization # Organizations are the customers or tenants who are using the TheHive application. TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other and can't share data. On the home page, you can see the organization List.","title":"About Organization"},{"location":"thehive/administration/organization/about-organization/#about-organization","text":"Organizations are the customers or tenants who are using the TheHive application. TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other and can't share data. On the home page, you can see the organization List.","title":"About Organization"},{"location":"thehive/administration/organization/add-edit-delete-organization/","text":"Manage Organizations # In this section, you can find information about managing the organization. You can add, delete, and edit the organization details. Add New Organization # To add a new organization: On the Organization List page, click the + button. Adding an Organization window opens. Enter a Name and Description for the new organization. Choose the Task rule and Observable rule from the lists. Click the Confirm organization creation button. Edit Organization # To edit an organization: On the Organization List page, click the Preview button corresponding to the organization you want to edit. Organization Preview window opens. Edit the required details. Click the Save button.","title":"Manage Organizations"},{"location":"thehive/administration/organization/add-edit-delete-organization/#manage-organizations","text":"In this section, you can find information about managing the organization. You can add, delete, and edit the organization details.","title":"Manage Organizations"},{"location":"thehive/administration/organization/add-edit-delete-organization/#add-new-organization","text":"To add a new organization: On the Organization List page, click the + button. Adding an Organization window opens. Enter a Name and Description for the new organization. Choose the Task rule and Observable rule from the lists. Click the Confirm organization creation button.","title":"Add New Organization"},{"location":"thehive/administration/organization/add-edit-delete-organization/#edit-organization","text":"To edit an organization: On the Organization List page, click the Preview button corresponding to the organization you want to edit. Organization Preview window opens. Edit the required details. Click the Save button.","title":"Edit Organization"},{"location":"thehive/administration/organization/apply-filters-to-organization/","text":"Apply Filters to Organization # In this section, you can find information about applying filters to the organization. To apply filter to an organization: On the Organization List page, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Apply Filters to Organization"},{"location":"thehive/administration/organization/apply-filters-to-organization/#apply-filters-to-organization","text":"In this section, you can find information about applying filters to the organization. To apply filter to an organization: On the Organization List page, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Apply Filters to Organization"},{"location":"thehive/administration/organization/lock-organization/","text":"Lock Organization # In this section you can find information about how to lock the organization. You can disable the organization by locking it. To lock an organization: Go to the preview Organization window. Switch on the Locked button.","title":"Lock Organization"},{"location":"thehive/administration/organization/lock-organization/#lock-organization","text":"In this section you can find information about how to lock the organization. You can disable the organization by locking it. To lock an organization: Go to the preview Organization window. Switch on the Locked button.","title":"Lock Organization"},{"location":"thehive/administration/organization/manage-organizations-list-view/","text":"Manage Organizations List View # In this section you can find information about how to save and manage the current organization list view. Save Organization List View # To save the current organization list view: On the organization list page, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message. Delete Organization List View # To delete a view: On the organization list page, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Manage Organizations List View"},{"location":"thehive/administration/organization/manage-organizations-list-view/#manage-organizations-list-view","text":"In this section you can find information about how to save and manage the current organization list view.","title":"Manage Organizations List View"},{"location":"thehive/administration/organization/manage-organizations-list-view/#save-organization-list-view","text":"To save the current organization list view: On the organization list page, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message.","title":"Save Organization List View"},{"location":"thehive/administration/organization/manage-organizations-list-view/#delete-organization-list-view","text":"To delete a view: On the organization list page, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Delete Organization List View"},{"location":"thehive/administration/organization/preview-organizations/","text":"Preview Organizations # After you login to TheHive application, you can see all organizations on the Organization List page. You can see the details of a specific organization. To preview the organization details: On the Organization List page, click the Preview button corresponding to the specific organization name. The Organization preview window opens. You can see details like the name, decription, logo, rules, and locked status of the organization. You can also view the list of users who belong to the organization.","title":"Preview Organizations"},{"location":"thehive/administration/organization/preview-organizations/#preview-organizations","text":"After you login to TheHive application, you can see all organizations on the Organization List page. You can see the details of a specific organization. To preview the organization details: On the Organization List page, click the Preview button corresponding to the specific organization name. The Organization preview window opens. You can see details like the name, decription, logo, rules, and locked status of the organization. You can also view the list of users who belong to the organization.","title":"Preview Organizations"},{"location":"thehive/administration/organization/rules-for-organization/","text":"Rules for Organization # You can link an organization to other organizations. Linking an organization allows it to share a case and all its data with other organizations in order to work together on the same investigation. When you create an organization, you can define the default way of sharing tasks and observables. e.g., if an organization \"XYZ\" is linked with another organization \"QPR\" and if you want to share a case from organization \"XYZ,\" the rules that can apply to the sharing of tasks and observables are manual, existingOnly, upcomingOnly, or all. Manual - When you share a case with another organization, all the tasks and observables are not shared with the other organization by default. You have to share each task and observable manually. existingOnly - When you share a case with another organization, all the existing tasks and observables are shared with the other organization. upcomingOnly - When you share a case with another organization, the existing tasks and observables won't be shared, but the upcoming ones will be shared automatically with the other organization. all - All the existing an upcoming tasks and obervables are shared.","title":"Rules for Organization"},{"location":"thehive/administration/organization/rules-for-organization/#rules-for-organization","text":"You can link an organization to other organizations. Linking an organization allows it to share a case and all its data with other organizations in order to work together on the same investigation. When you create an organization, you can define the default way of sharing tasks and observables. e.g., if an organization \"XYZ\" is linked with another organization \"QPR\" and if you want to share a case from organization \"XYZ,\" the rules that can apply to the sharing of tasks and observables are manual, existingOnly, upcomingOnly, or all. Manual - When you share a case with another organization, all the tasks and observables are not shared with the other organization by default. You have to share each task and observable manually. existingOnly - When you share a case with another organization, all the existing tasks and observables are shared with the other organization. upcomingOnly - When you share a case with another organization, the existing tasks and observables won't be shared, but the upcoming ones will be shared automatically with the other organization. all - All the existing an upcoming tasks and obervables are shared.","title":"Rules for Organization"},{"location":"thehive/administration/organization/view-organizations-list/","text":"View Organizations List # After you login to TheHive application, you can see all organizations on the Organization List page.","title":"View Organizations List"},{"location":"thehive/administration/organization/view-organizations-list/#view-organizations-list","text":"After you login to TheHive application, you can see all organizations on the Organization List page.","title":"View Organizations List"},{"location":"thehive/administration/organization/view-users-in-the-organization/","text":"View Users in the Organization # After you login to TheHive application, you can see all organizations on the Organization List page. You can see all the users of a specific organization. To see the users: On the Organization List page, click the Preview button corresponding to the specific organization name. The Organization preview window opens. You can see the list of all users that belong to that organization.","title":"View Users in the Organization"},{"location":"thehive/administration/organization/view-users-in-the-organization/#view-users-in-the-organization","text":"After you login to TheHive application, you can see all organizations on the Organization List page. You can see all the users of a specific organization. To see the users: On the Organization List page, click the Preview button corresponding to the specific organization name. The Organization preview window opens. You can see the list of all users that belong to that organization.","title":"View Users in the Organization"},{"location":"thehive/administration/platform-management/","text":"","title":"Index"},{"location":"thehive/administration/platform-management/about-platform-management/","text":"About Platform Management # Platform Management helps you understand the issues related to TheHive health status. It contains details about: Database schema version. Status of database indexes. Status of database integrity checks. It also allows: Exporting a JSON report of the health status including more details then what is displayed on the UI. Reindexing the database. Triggering the database integrity checks.","title":"About Platform Management"},{"location":"thehive/administration/platform-management/about-platform-management/#about-platform-management","text":"Platform Management helps you understand the issues related to TheHive health status. It contains details about: Database schema version. Status of database indexes. Status of database integrity checks. It also allows: Exporting a JSON report of the health status including more details then what is displayed on the UI. Reindexing the database. Triggering the database integrity checks.","title":"About Platform Management"},{"location":"thehive/administration/platform-management/authentication/advanced-settings/","text":"Advanced Settings # Authentication consists of a set of modules. Each module tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. In this section you will find information about configuring the advanced authentication settings. To configure the advanced authentication settings: On the Platform Management page, in the Authentication tab, switch on the following buttons as per your requirement: Enable Basic Authentication : Authenticates HTTP requests using the login and password provided. Enable API Key authentication : Authenticates HTTP requests using an API key provided. Enable HTTP Header Authentication : Authenticates HTTP requests using a HTTP header containing the user login. Enable Multifactor authentication : Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page. Manage Authentication Providers # On the Platform Management page, in the Authentication tab, you can see the list of following Authentication providers. Local Authentication : You can manage a local user database where you can configure the password policy. Directories Authentication : You can configure The Hive to use either active directory or LDAP to authorize the users. If you manage your users through a active directory, then you can configure TheHive in a way that every time a user enters the username, you will authenticate the crredentials in the configured active directory and if active directory accepts the credentials then you authorize the user. Active Directory Configuration: LDAP Configuration: OAuth 2 Authentication : You can configure TheHive to sign in using single signon through external OAuth2 authenticator server.","title":"Advanced Settings"},{"location":"thehive/administration/platform-management/authentication/advanced-settings/#advanced-settings","text":"Authentication consists of a set of modules. Each module tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. In this section you will find information about configuring the advanced authentication settings. To configure the advanced authentication settings: On the Platform Management page, in the Authentication tab, switch on the following buttons as per your requirement: Enable Basic Authentication : Authenticates HTTP requests using the login and password provided. Enable API Key authentication : Authenticates HTTP requests using an API key provided. Enable HTTP Header Authentication : Authenticates HTTP requests using a HTTP header containing the user login. Enable Multifactor authentication : Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page.","title":"Advanced Settings"},{"location":"thehive/administration/platform-management/authentication/advanced-settings/#manage-authentication-providers","text":"On the Platform Management page, in the Authentication tab, you can see the list of following Authentication providers. Local Authentication : You can manage a local user database where you can configure the password policy. Directories Authentication : You can configure The Hive to use either active directory or LDAP to authorize the users. If you manage your users through a active directory, then you can configure TheHive in a way that every time a user enters the username, you will authenticate the crredentials in the configured active directory and if active directory accepts the credentials then you authorize the user. Active Directory Configuration: LDAP Configuration: OAuth 2 Authentication : You can configure TheHive to sign in using single signon through external OAuth2 authenticator server.","title":"Manage Authentication Providers"},{"location":"thehive/administration/platform-management/authentication/configuring-session/","text":"Configuring Sessions # In this section you will find information about configuring the user session settings. Session authenticates HTTP requests using a cookie. To configure the session settings: On the Platform Management page, in the Authentication tab, enter the value for Duration for inactivity before session expiration in seconds/minutes/hours. On the Platform Management page, in the Authentication tab, enter the value for Warning message display time, before session expiration in seconds/minutes/hours.","title":"Configuring Sessions"},{"location":"thehive/administration/platform-management/authentication/configuring-session/#configuring-sessions","text":"In this section you will find information about configuring the user session settings. Session authenticates HTTP requests using a cookie. To configure the session settings: On the Platform Management page, in the Authentication tab, enter the value for Duration for inactivity before session expiration in seconds/minutes/hours. On the Platform Management page, in the Authentication tab, enter the value for Warning message display time, before session expiration in seconds/minutes/hours.","title":"Configuring Sessions"},{"location":"thehive/administration/platform-management/branding/configure-branding/","text":"Configure Branding # In this section you will find information about customizing your brand. You can change the brand name, login page logo, navigation page logo, and the favicon. To customize branding: On the Platform Management icon, select the Branding tab. Make the necessary changes. Click Confirm .","title":"Configure Branding"},{"location":"thehive/administration/platform-management/branding/configure-branding/#configure-branding","text":"In this section you will find information about customizing your brand. You can change the brand name, login page logo, navigation page logo, and the favicon. To customize branding: On the Platform Management icon, select the Branding tab. Make the necessary changes. Click Confirm .","title":"Configure Branding"},{"location":"thehive/administration/platform-management/cortex/about-cortex/","text":"About Cortex # TheHive uses Cortex to have access to analyzers and responsders. Analyzers can be launched against observables to get more details about a given observable. Responders can be launched against case, tasks, observables, logs, and alerts to execute an action. One or more Cortex instances can be connected to TheHive. You can connect your TheHive instance with many servers of cortex, and every server has its own configuration.","title":"About Cortex"},{"location":"thehive/administration/platform-management/cortex/about-cortex/#about-cortex","text":"TheHive uses Cortex to have access to analyzers and responsders. Analyzers can be launched against observables to get more details about a given observable. Responders can be launched against case, tasks, observables, logs, and alerts to execute an action. One or more Cortex instances can be connected to TheHive. You can connect your TheHive instance with many servers of cortex, and every server has its own configuration.","title":"About Cortex"},{"location":"thehive/administration/platform-management/cortex/add-edit-delete-cortex-server/","text":"Manage the Cortex Server # In this section, you can find information about configuring the cortex integration. Add a new Cortex server # To set up a new server: On the Platform Management page, in the Cortex tab, click the + button. Set up the new server window opens. In the General settings section, Enter the Server name , Server url , and API key . In the Proxy settings section: select the Type of protocol from the list. Enter the Address . witch on the buttons for Athentification , Do not check certificate authority , and Disable hostname verification if required. In the Advanced settings section, select the filter for the TheHive organizations from the list. You can make the cortex server available only to selected organizations or make it available to all organizations. Click the Add button. Do you want to save the modifications message appears. Click Confirm . Cortex configuration successfully saved message appears. Delete a Cortex Server # On the Platform Management page, in the Cortex tab, click the ellipsis(...) corresponding to the server you want to delete. A delete confirmation dialog box appears. Click OK . Do you want to save the modifications message appears. Click Confirm . Cortex configuration successfully saved message appears. Edit Cortex Server # On the Platform Management page, in the Cortex tab, click the cortex server name you want to edit. Set up the server for that particular server window opens. Edit the required details. Click Update . Do you want to save the modifications message appears. Click Confirm Cortex configuration successfully saved message appears.","title":"Manage the Cortex Server"},{"location":"thehive/administration/platform-management/cortex/add-edit-delete-cortex-server/#manage-the-cortex-server","text":"In this section, you can find information about configuring the cortex integration.","title":"Manage the Cortex Server"},{"location":"thehive/administration/platform-management/cortex/add-edit-delete-cortex-server/#add-a-new-cortex-server","text":"To set up a new server: On the Platform Management page, in the Cortex tab, click the + button. Set up the new server window opens. In the General settings section, Enter the Server name , Server url , and API key . In the Proxy settings section: select the Type of protocol from the list. Enter the Address . witch on the buttons for Athentification , Do not check certificate authority , and Disable hostname verification if required. In the Advanced settings section, select the filter for the TheHive organizations from the list. You can make the cortex server available only to selected organizations or make it available to all organizations. Click the Add button. Do you want to save the modifications message appears. Click Confirm . Cortex configuration successfully saved message appears.","title":"Add a new Cortex server"},{"location":"thehive/administration/platform-management/cortex/add-edit-delete-cortex-server/#delete-a-cortex-server","text":"On the Platform Management page, in the Cortex tab, click the ellipsis(...) corresponding to the server you want to delete. A delete confirmation dialog box appears. Click OK . Do you want to save the modifications message appears. Click Confirm . Cortex configuration successfully saved message appears.","title":"Delete a Cortex Server"},{"location":"thehive/administration/platform-management/cortex/add-edit-delete-cortex-server/#edit-cortex-server","text":"On the Platform Management page, in the Cortex tab, click the cortex server name you want to edit. Set up the server for that particular server window opens. Edit the required details. Click Update . Do you want to save the modifications message appears. Click Confirm Cortex configuration successfully saved message appears.","title":"Edit Cortex Server"},{"location":"thehive/administration/platform-management/cortex/view-cortex-details/","text":"View Cortex Details # You can view the list of cortex servers on the Platform Management page, in the Cortex tab. You can view the details of a specific server by clicking the name of that server.","title":"View Cortex Details"},{"location":"thehive/administration/platform-management/cortex/view-cortex-details/#view-cortex-details","text":"You can view the list of cortex servers on the Platform Management page, in the Cortex tab. You can view the details of a specific server by clicking the name of that server.","title":"View Cortex Details"},{"location":"thehive/administration/platform-management/global-endpoints/about-endpoints/","text":"About Endpoints # Global endpoints are endpoints of type Webhook, Mattermost, Slack, or HTTP. These endpoints can be used by every organization. TheHive can notify external system or third party tool of the modification events (case creation, alert update, task assignment). For example, every time you create a case in TheHIve, create a ticket in JIRA.","title":"About Endpoints"},{"location":"thehive/administration/platform-management/global-endpoints/about-endpoints/#about-endpoints","text":"Global endpoints are endpoints of type Webhook, Mattermost, Slack, or HTTP. These endpoints can be used by every organization. TheHive can notify external system or third party tool of the modification events (case creation, alert update, task assignment). For example, every time you create a case in TheHIve, create a ticket in JIRA.","title":"About Endpoints"},{"location":"thehive/administration/platform-management/global-endpoints/add-edit-delete-endpoint/","text":"Manage Endpoints # In this section, you can find information about managing the endpoints. You can add, delete, and edit an endpoint. Add a new Endpoint # To add a new endpoint: On the Platform Management page, in the Global Endpoints tab, click the + button or click the Add a new endpoint link. Endpoint Creation window opens. Choose a Connectore form the available connectors. Enter the required information for that connector. Click the Confirm button. Endpoint successfully added message appears. Delete an Endpoint # On the Platform Management page, in the Global Endpoints tab, click the ellipsis(...) corresponding to the endpoint you want to delete. Click Delete . The endpoint successfully deleted message appears.","title":"Manage Endpoints"},{"location":"thehive/administration/platform-management/global-endpoints/add-edit-delete-endpoint/#manage-endpoints","text":"In this section, you can find information about managing the endpoints. You can add, delete, and edit an endpoint.","title":"Manage Endpoints"},{"location":"thehive/administration/platform-management/global-endpoints/add-edit-delete-endpoint/#add-a-new-endpoint","text":"To add a new endpoint: On the Platform Management page, in the Global Endpoints tab, click the + button or click the Add a new endpoint link. Endpoint Creation window opens. Choose a Connectore form the available connectors. Enter the required information for that connector. Click the Confirm button. Endpoint successfully added message appears.","title":"Add a new Endpoint"},{"location":"thehive/administration/platform-management/global-endpoints/add-edit-delete-endpoint/#delete-an-endpoint","text":"On the Platform Management page, in the Global Endpoints tab, click the ellipsis(...) corresponding to the endpoint you want to delete. Click Delete . The endpoint successfully deleted message appears.","title":"Delete an Endpoint"},{"location":"thehive/administration/platform-management/ldap-servers/configure-ldap-server/","text":"Configure LDAP Server # LDAP (Lightweight Directory Access Protocol) is an open standard application protocol to access and maintain distributed directory information services over an IP network. LDAP is a request-response type of protocol where each request is followed by a response. Directory services allow information about users, systems, networks, services, and applications to be shared throughout the network. In this section you will find information about configuring the LDAP server. In order to use LDAP to assign a group policy to a user, you must map an LDAP attribute. To configure the LDAP server: On the Platform Management page, in the LDAP tab, click the + button or click the Add LDAP server link. The LDAP Configuration window opens. Enter the information in General settings. Enter data for Map of LDAP Attributes. Add a Map Group field in Group Mappings. Click the Confirm button.","title":"Configure LDAP Server"},{"location":"thehive/administration/platform-management/ldap-servers/configure-ldap-server/#configure-ldap-server","text":"LDAP (Lightweight Directory Access Protocol) is an open standard application protocol to access and maintain distributed directory information services over an IP network. LDAP is a request-response type of protocol where each request is followed by a response. Directory services allow information about users, systems, networks, services, and applications to be shared throughout the network. In this section you will find information about configuring the LDAP server. In order to use LDAP to assign a group policy to a user, you must map an LDAP attribute. To configure the LDAP server: On the Platform Management page, in the LDAP tab, click the + button or click the Add LDAP server link. The LDAP Configuration window opens. Enter the information in General settings. Enter data for Map of LDAP Attributes. Add a Map Group field in Group Mappings. Click the Confirm button.","title":"Configure LDAP Server"},{"location":"thehive/administration/platform-management/license/view-update-license/","text":"View Update License # In this section you will find information about updating your license. When you install and TheHive, by default it will include the community edition license. If you want to activate the license, you nees to buy the license from StrangeBee. WHen you buy the license from StrangeBee, StrangeBee will create an account for you on the customer portal that will allow you to activate the license. To activate the license: On the Platform Management page, in the License tab, click the Update the current license button. Set a License key window opens. You can see the challenge in the window. Click Copy this challenge . You will see the challenge copied message. After you copy the challenge, you go to your account on the StrangeBee customer portal and activate the license using this challenge and the customer portal will give you an activation license key. Enter the activation key in the License field. Click the Activate the license key button. This will activate the license and update your instance with all the features included with that license. The license is defined by the following capabilities: It defines how many users you can create in your platform. The license is based on the number of users and the number of organizations. It has a validation and an expiration date. It allows unlimited number of Readonly users and Service users. Service users are those who do not have access to the TheHive interface but they use an API key to call all the APIs.","title":"View Update License"},{"location":"thehive/administration/platform-management/license/view-update-license/#view-update-license","text":"In this section you will find information about updating your license. When you install and TheHive, by default it will include the community edition license. If you want to activate the license, you nees to buy the license from StrangeBee. WHen you buy the license from StrangeBee, StrangeBee will create an account for you on the customer portal that will allow you to activate the license. To activate the license: On the Platform Management page, in the License tab, click the Update the current license button. Set a License key window opens. You can see the challenge in the window. Click Copy this challenge . You will see the challenge copied message. After you copy the challenge, you go to your account on the StrangeBee customer portal and activate the license using this challenge and the customer portal will give you an activation license key. Enter the activation key in the License field. Click the Activate the license key button. This will activate the license and update your instance with all the features included with that license. The license is defined by the following capabilities: It defines how many users you can create in your platform. The license is based on the number of users and the number of organizations. It has a validation and an expiration date. It allows unlimited number of Readonly users and Service users. Service users are those who do not have access to the TheHive interface but they use an API key to call all the APIs.","title":"View Update License"},{"location":"thehive/administration/platform-management/misp/about-misp/","text":"About MISP # MISP(Malware Information Sharing Platform) is an open-source platform that allows sharing, storing, and correlating Indicators of Compromise (IOCs) of targeted attacks, threat intelligence, financial fraud information, vulnerability information, or even counter-terrorism information. Indicators of compromise managed by MISP may originate from various sources, including internal incident investigation teams, intelligence sharing partners, or commercial intelligence sources. Numerous independent organizations from variuos industries, utilize MISP for threat feeds. The purpose of MISP is to create a platform of trust by locally storing threat information and enhancing malware detection to encourage information exchange among organizations. TheHive has the ability to connect to one or several MISP instances in order to import and export events. TheHive is able to: Receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. Export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url, the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers.","title":"About MISP"},{"location":"thehive/administration/platform-management/misp/about-misp/#about-misp","text":"MISP(Malware Information Sharing Platform) is an open-source platform that allows sharing, storing, and correlating Indicators of Compromise (IOCs) of targeted attacks, threat intelligence, financial fraud information, vulnerability information, or even counter-terrorism information. Indicators of compromise managed by MISP may originate from various sources, including internal incident investigation teams, intelligence sharing partners, or commercial intelligence sources. Numerous independent organizations from variuos industries, utilize MISP for threat feeds. The purpose of MISP is to create a platform of trust by locally storing threat information and enhancing malware detection to encourage information exchange among organizations. TheHive has the ability to connect to one or several MISP instances in order to import and export events. TheHive is able to: Receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. Export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url, the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers.","title":"About MISP"},{"location":"thehive/administration/platform-management/misp/add-edit-delete-server/","text":"Setup New Server # In this section, you can find information about configuring the MISP integration. MISP and TheHive can interact between each other in both ways: TheHive is able to import events from a MISP instance as alerts and create cases from them. TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes. Add a new server # To set up a new server: On the Platform Management page, in the MISP tab, click the + button. Set up the new server window opens. In the General settings section, Enter the Server name , Server url , and API key . Select the Purpose from the list. In the Proxy settings section, select the Type of protocol from the list. Enter the Address . Switch on the buttons for Athentification , Do not check certificate authority , and Disable hostname verification if required. In the Advanced settings section, select the filter for the TheHive organizations from the list. You can make the server available only to selected organizations or make it available to all organizations. Click the Update button. Do you want to save the modifications message appears. Click Confirm . MISP configuration successfully saved message appears. Delete a MISP Server # On the Platform Management page, in the MISP tab, click the ellipsis(...) corresponnding to the server you want to delete. A delete confirmation dialog box appears. Click OK . Do you want to save the modifications message appears. Click Confirm . MISP configuration successfully saved message appears. Edit MISP Server # On the Platform Management page, in the MISP tab, click the server name you want to edit. Set up the server for that particular server window opens. Edit the required details. Click Update . Do you want to save the modifications message appears. Click Confirm Cortex configuration successfully saved message appears.","title":"Setup New Server"},{"location":"thehive/administration/platform-management/misp/add-edit-delete-server/#setup-new-server","text":"In this section, you can find information about configuring the MISP integration. MISP and TheHive can interact between each other in both ways: TheHive is able to import events from a MISP instance as alerts and create cases from them. TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes.","title":"Setup New Server"},{"location":"thehive/administration/platform-management/misp/add-edit-delete-server/#add-a-new-server","text":"To set up a new server: On the Platform Management page, in the MISP tab, click the + button. Set up the new server window opens. In the General settings section, Enter the Server name , Server url , and API key . Select the Purpose from the list. In the Proxy settings section, select the Type of protocol from the list. Enter the Address . Switch on the buttons for Athentification , Do not check certificate authority , and Disable hostname verification if required. In the Advanced settings section, select the filter for the TheHive organizations from the list. You can make the server available only to selected organizations or make it available to all organizations. Click the Update button. Do you want to save the modifications message appears. Click Confirm . MISP configuration successfully saved message appears.","title":"Add a new server"},{"location":"thehive/administration/platform-management/misp/add-edit-delete-server/#delete-a-misp-server","text":"On the Platform Management page, in the MISP tab, click the ellipsis(...) corresponnding to the server you want to delete. A delete confirmation dialog box appears. Click OK . Do you want to save the modifications message appears. Click Confirm . MISP configuration successfully saved message appears.","title":"Delete a MISP Server"},{"location":"thehive/administration/platform-management/misp/add-edit-delete-server/#edit-misp-server","text":"On the Platform Management page, in the MISP tab, click the server name you want to edit. Set up the server for that particular server window opens. Edit the required details. Click Update . Do you want to save the modifications message appears. Click Confirm Cortex configuration successfully saved message appears.","title":"Edit MISP Server"},{"location":"thehive/administration/platform-management/misp/view-details/","text":"View Details # You can view the list of MISP servers on the Platform Management page, in the MISP tab. You can view the details of a specific server by clicking the name of that server.","title":"View Details"},{"location":"thehive/administration/platform-management/misp/view-details/#view-details","text":"You can view the list of MISP servers on the Platform Management page, in the MISP tab. You can view the details of a specific server by clicking the name of that server.","title":"View Details"},{"location":"thehive/administration/platform-management/smtp/about-smtp/","text":"About SMTP # The Simple Mail Transfer Protocol (SMTP) is an internet standard communication protocol for electronic mail transmission. SMTP is a delivery protocol only. SMTP servers typically require authentication of clients by credentials before allowing access. Many internet systems use SMTP as a method to transfer mail from one user to another. There are many SMTP service providers out there in the market. Following are some of the advantages of using a SMTP server: It is a secure environment for sending emails. It has dedicated IP addresses and flexible API and SMTP setup. User friendly software.","title":"About SMTP"},{"location":"thehive/administration/platform-management/smtp/about-smtp/#about-smtp","text":"The Simple Mail Transfer Protocol (SMTP) is an internet standard communication protocol for electronic mail transmission. SMTP is a delivery protocol only. SMTP servers typically require authentication of clients by credentials before allowing access. Many internet systems use SMTP as a method to transfer mail from one user to another. There are many SMTP service providers out there in the market. Following are some of the advantages of using a SMTP server: It is a secure environment for sending emails. It has dedicated IP addresses and flexible API and SMTP setup. User friendly software.","title":"About SMTP"},{"location":"thehive/administration/platform-management/smtp/configure-smtp/","text":"Configure SMTP # You can configure the SMTP server that would sent emails from TheHive platform. To configure the SMTP server: On the Platform Management page, select the SMTP tab. Configure Server Settings: Enter the Server name or IP address . Enter the Port . Enter the email id in Send emails from . Configure Security and Authentication settings: Select the Connection security from the list. Enter the Username amd Password . Do you want to save the modifications message appears. Click confirm .","title":"Configure SMTP"},{"location":"thehive/administration/platform-management/smtp/configure-smtp/#configure-smtp","text":"You can configure the SMTP server that would sent emails from TheHive platform. To configure the SMTP server: On the Platform Management page, select the SMTP tab. Configure Server Settings: Enter the Server name or IP address . Enter the Port . Enter the email id in Send emails from . Configure Security and Authentication settings: Select the Connection security from the list. Enter the Username amd Password . Do you want to save the modifications message appears. Click confirm .","title":"Configure SMTP"},{"location":"thehive/administration/platform-management/status/drop-and-rebuild-the-index/","text":"Drop and Rebuild the Index # In this section you will find information about dropping and rebuilding your database index. To drop and rebuild the database index: On the Platform Management page, in the Status tab, click Drop and rebuild the index . A global data rebuild successfully message appears.","title":"Drop and Rebuild the Index"},{"location":"thehive/administration/platform-management/status/drop-and-rebuild-the-index/#drop-and-rebuild-the-index","text":"In this section you will find information about dropping and rebuilding your database index. To drop and rebuild the database index: On the Platform Management page, in the Status tab, click Drop and rebuild the index . A global data rebuild successfully message appears.","title":"Drop and Rebuild the Index"},{"location":"thehive/administration/platform-management/status/export-status-report/","text":"Export Status Report # In this section you will find information about exporting platform status report. To export platform status report: On the Platform Management page, in the Status tab, click the Export status report button. The platform status report JSON Source file is exported.","title":"Export Status Report"},{"location":"thehive/administration/platform-management/status/export-status-report/#export-status-report","text":"In this section you will find information about exporting platform status report. To export platform status report: On the Platform Management page, in the Status tab, click the Export status report button. The platform status report JSON Source file is exported.","title":"Export Status Report"},{"location":"thehive/administration/platform-management/status/reindex-the-data/","text":"Reindex the Data # In this section you will find information about reindexing your database. To reindex data: On the Platform Management page, in the Status tab, click Reindex the data . A global data reindex confirmation dialog box appears. Click OK .","title":"Reindex the Data"},{"location":"thehive/administration/platform-management/status/reindex-the-data/#reindex-the-data","text":"In this section you will find information about reindexing your database. To reindex data: On the Platform Management page, in the Status tab, click Reindex the data . A global data reindex confirmation dialog box appears. Click OK .","title":"Reindex the Data"},{"location":"thehive/administration/platform-management/status/trigger-alert-case-check/","text":"Trigger Alert/Case Health Check # In this section you will find information about database integrity check. You can trigger an alert health check or a case health check. To trigger an alert data: On the Platform Management page, in the Status tab, click Trigger corresponding to the alert. A trigger alert health check confirmation dialog box appears. Click OK . A trigger alert health check started message appears. Trigger Case Check # To trigger a case data: On the Platform Management page, in the status tab, click Trigger corresponding to the case. A trigger case health check confirmation dialog box appears. Click OK . A trigger case health check started message appears.","title":"Trigger Alert/Case Health Check"},{"location":"thehive/administration/platform-management/status/trigger-alert-case-check/#trigger-alertcase-health-check","text":"In this section you will find information about database integrity check. You can trigger an alert health check or a case health check. To trigger an alert data: On the Platform Management page, in the Status tab, click Trigger corresponding to the alert. A trigger alert health check confirmation dialog box appears. Click OK . A trigger alert health check started message appears.","title":"Trigger Alert/Case Health Check"},{"location":"thehive/administration/platform-management/status/trigger-alert-case-check/#trigger-case-check","text":"To trigger a case data: On the Platform Management page, in the status tab, click Trigger corresponding to the case. A trigger case health check confirmation dialog box appears. Click OK . A trigger case health check started message appears.","title":"Trigger Case Check"},{"location":"thehive/administration/platform-management/status/view-status/","text":"View Status # After you log in to the application, you are on the Organization list page by default. To view the platform status: On the left side of the window, click the Platform Management tab. By default, the License page opens. Select the Status tab. On the status page, you can see database schema status, database index status, and database integrity check.","title":"View Status"},{"location":"thehive/administration/platform-management/status/view-status/#view-status","text":"After you log in to the application, you are on the Organization list page by default. To view the platform status: On the left side of the window, click the Platform Management tab. By default, the License page opens. Select the Status tab. On the status page, you can see database schema status, database index status, and database integrity check.","title":"View Status"},{"location":"thehive/administration/users/","text":"","title":"Index"},{"location":"thehive/administration/users/about-type-of-users/","text":"About Type of Users # In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201canalyst\u201d in \u201corganisationA\u201d \u201cadmin\u201d in \u201corganisationB\u201d \u201cread-only\u201d in \u201corganisationC\u201d NOTE : A single user can belong to many organizations and in each organization it has a dedicated user profile.","title":"About Type of Users"},{"location":"thehive/administration/users/about-type-of-users/#about-type-of-users","text":"In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201canalyst\u201d in \u201corganisationA\u201d \u201cadmin\u201d in \u201corganisationB\u201d \u201cread-only\u201d in \u201corganisationC\u201d NOTE : A single user can belong to many organizations and in each organization it has a dedicated user profile.","title":"About Type of Users"},{"location":"thehive/administration/users/add-edit-delete-user/","text":"Manage Users # In this section, you can find information about managing the users. You can add, delete, and edit the user details. Add New User # To add a new user: On the Users page, click the + button. Adding a User window opens. Enter a Login and Name for the new user. Choose the Organization for the user and the user profile from the list. Click the Confirm user creation button. Edit User # To edit a User: On the Users page, click the Preview button corresponding to the user you want to edit. Preview user window opens. Edit the required details. Click the Save button. Delete User # To delete a User: On the Users page, click the Preview button corresponding to the user you want to delete. Preview user window opens. Click the Delete user button. A delete confirmation dialog box appears. Click OK .","title":"Manage Users"},{"location":"thehive/administration/users/add-edit-delete-user/#manage-users","text":"In this section, you can find information about managing the users. You can add, delete, and edit the user details.","title":"Manage Users"},{"location":"thehive/administration/users/add-edit-delete-user/#add-new-user","text":"To add a new user: On the Users page, click the + button. Adding a User window opens. Enter a Login and Name for the new user. Choose the Organization for the user and the user profile from the list. Click the Confirm user creation button.","title":"Add New User"},{"location":"thehive/administration/users/add-edit-delete-user/#edit-user","text":"To edit a User: On the Users page, click the Preview button corresponding to the user you want to edit. Preview user window opens. Edit the required details. Click the Save button.","title":"Edit User"},{"location":"thehive/administration/users/add-edit-delete-user/#delete-user","text":"To delete a User: On the Users page, click the Preview button corresponding to the user you want to delete. Preview user window opens. Click the Delete user button. A delete confirmation dialog box appears. Click OK .","title":"Delete User"},{"location":"thehive/administration/users/filter-users-list/","text":"Filter Users # In this section, you can find information about applying filters to the users. On the users page, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Users"},{"location":"thehive/administration/users/filter-users-list/#filter-users","text":"In this section, you can find information about applying filters to the users. On the users page, switch on the Filters button. Click Add filters . Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filter Users"},{"location":"thehive/administration/users/lock-user/","text":"Lock User # In this section you can find information about how to lock a user. If a user is no longer working on the TheHive platform, you can lock the user. The user is noe deactivated and can no longer log in to the application. To lock the user: Go to the preview Users window. Switch on the Locked button.","title":"Lock User"},{"location":"thehive/administration/users/lock-user/#lock-user","text":"In this section you can find information about how to lock a user. If a user is no longer working on the TheHive platform, you can lock the user. The user is noe deactivated and can no longer log in to the application. To lock the user: Go to the preview Users window. Switch on the Locked button.","title":"Lock User"},{"location":"thehive/administration/users/manage-users-list-view/","text":"Manage Users List View # In this section you can find information about how to save and manage the current users list view. Save Current Users List View # To save the current organization list view: On the Users page, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message. Delete Users List View # To delete a view: On the Users page, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Manage Users List View"},{"location":"thehive/administration/users/manage-users-list-view/#manage-users-list-view","text":"In this section you can find information about how to save and manage the current users list view.","title":"Manage Users List View"},{"location":"thehive/administration/users/manage-users-list-view/#save-current-users-list-view","text":"To save the current organization list view: On the Users page, click the default button. A menu is displayed. Select the Save current view option. Save current view window opens. Enter the Name of view . Click the Save button. You can see the View saved successfully message.","title":"Save Current Users List View"},{"location":"thehive/administration/users/manage-users-list-view/#delete-users-list-view","text":"To delete a view: On the Users page, click the default button Menu is displayed. Select the Manage Views option. The Manage views window opens to manage all the views. Click the ellipsis (...) corresponding to the view you want to delete. Click Delete . A delete confirmation dialog box appears. Click OK . You can see the View deleted successfully message.","title":"Delete Users List View"},{"location":"thehive/administration/users/preview-user-details/","text":"Preview User Details # In this section, you can find information about previewing the user details. To preview the user details: On the Users page, click the Preview button corresponding to the username whose details you want to see. The user details window opens. You can see details like the username, avatar, login name, email id, API, and organizations.","title":"Preview User Details"},{"location":"thehive/administration/users/preview-user-details/#preview-user-details","text":"In this section, you can find information about previewing the user details. To preview the user details: On the Users page, click the Preview button corresponding to the username whose details you want to see. The user details window opens. You can see details like the username, avatar, login name, email id, API, and organizations.","title":"Preview User Details"},{"location":"thehive/administration/users/reset-password/","text":"Reset Password # In this section you can find information about how to reset your password if needed. To reset password: On the Users tab, click the Preview button corresponding to the username. The user details window opens. Click the Reset the password button. You can see a message that an email is sent to reset your password. You can change your password by clicking on the link sent in the email.","title":"Reset Password"},{"location":"thehive/administration/users/reset-password/#reset-password","text":"In this section you can find information about how to reset your password if needed. To reset password: On the Users tab, click the Preview button corresponding to the username. The user details window opens. Click the Reset the password button. You can see a message that an email is sent to reset your password. You can change your password by clicking on the link sent in the email.","title":"Reset Password"},{"location":"thehive/administration/users/view-users/","text":"View Users # After you log in to the application, you are on the Organization list page by default. To view the users: On the left side of the window, click the Users tab. The Users page opens. On the Users page, you can see the list of all users and their details like names, log-in ids, the organizations they belong to, created by, and the created/updated dates.","title":"View Users"},{"location":"thehive/administration/users/view-users/#view-users","text":"After you log in to the application, you are on the Organization list page by default. To view the users: On the left side of the window, click the Users tab. The Users page opens. On the Users page, you can see the list of all users and their details like names, log-in ids, the organizations they belong to, created by, and the created/updated dates.","title":"View Users"},{"location":"thehive/download/","text":"Download TheHive # TheHive is published and available as many binary packages formats: Debian / Ubuntu # /etc/apt/source.list.d/strangebee.list deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main Red Hat Enterprise Linux / Fedora # /etc/yum.repos.d/strangebee.repo [thehive] enabled=1 priority=1 name=StrangeBee RPM repository baseurl=https://rpm.strangebee.com/thehive-5.x/noarch gpgkey=https://archives.strangebee.com/keys/strangebee.gpg gpgcheck=1 ZIP archive # Download it at: https://archives.strangebee.com/zip/thehive-latest.zip Docker # Docker images are published on Dockerhub here: https://hub.docker.com/r/strangebee/thehive Archives # All binary packages and archives are also available on our archives websites: DEB packages # https://archives.strangebee.com/deb/ RPM packages # https://archives.strangebee.com/rpm/ ZIP # https://archives.strangebee.com/zip/","title":"Download"},{"location":"thehive/download/#download-thehive","text":"TheHive is published and available as many binary packages formats:","title":"Download TheHive"},{"location":"thehive/download/#debian-ubuntu","text":"/etc/apt/source.list.d/strangebee.list deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main","title":" Debian /  Ubuntu"},{"location":"thehive/download/#red-hat-enterprise-linux-fedora","text":"/etc/yum.repos.d/strangebee.repo [thehive] enabled=1 priority=1 name=StrangeBee RPM repository baseurl=https://rpm.strangebee.com/thehive-5.x/noarch gpgkey=https://archives.strangebee.com/keys/strangebee.gpg gpgcheck=1","title":" Red Hat Enterprise Linux /  Fedora"},{"location":"thehive/download/#zip-archive","text":"Download it at: https://archives.strangebee.com/zip/thehive-latest.zip","title":" ZIP archive"},{"location":"thehive/download/#docker","text":"Docker images are published on Dockerhub here: https://hub.docker.com/r/strangebee/thehive","title":" Docker"},{"location":"thehive/download/#archives","text":"All binary packages and archives are also available on our archives websites:","title":"Archives"},{"location":"thehive/download/#deb-packages","text":"https://archives.strangebee.com/deb/","title":"DEB packages"},{"location":"thehive/download/#rpm-packages","text":"https://archives.strangebee.com/rpm/","title":"RPM packages"},{"location":"thehive/download/#zip","text":"https://archives.strangebee.com/zip/","title":"ZIP"},{"location":"thehive/how-to/alert-management/","text":"Alert Management # Alert list # Alerts received by your organization can be viewed in TheHive: Every user inside the organization can view the alerts. But you will need the permission manageAlert to be able to edit alerts. A user can use predefined filter or custom filters to view only selected alerts: Alert details # From the alert list, an alert can be opened for more investigation. Details are filled, comments by analysts can be made on the alert too: You can use tags, comments, severity, tlp, pap, custom fields and custom statuses to help categorize your alerts. Observables from the alert can be further analyzed either by the analysts or by using Cortex analyzers: Finally, depending on the analyst investigation, an alert can be closed (marked as \"False Positive\", \"Duplicate\", \"Ignored\" or an other custom status) or a case can be created to pursue the investigation.","title":"Alert Management"},{"location":"thehive/how-to/alert-management/#alert-management","text":"","title":"Alert Management"},{"location":"thehive/how-to/alert-management/#alert-list","text":"Alerts received by your organization can be viewed in TheHive: Every user inside the organization can view the alerts. But you will need the permission manageAlert to be able to edit alerts. A user can use predefined filter or custom filters to view only selected alerts:","title":"Alert list"},{"location":"thehive/how-to/alert-management/#alert-details","text":"From the alert list, an alert can be opened for more investigation. Details are filled, comments by analysts can be made on the alert too: You can use tags, comments, severity, tlp, pap, custom fields and custom statuses to help categorize your alerts. Observables from the alert can be further analyzed either by the analysts or by using Cortex analyzers: Finally, depending on the analyst investigation, an alert can be closed (marked as \"False Positive\", \"Duplicate\", \"Ignored\" or an other custom status) or a case can be created to pursue the investigation.","title":"Alert details"},{"location":"thehive/how-to/authentication/","text":"Authentication # TheHive supports several authentication providers: local (credential are securely stored in TheHive database) directory (LDAP and Active Directory) OAuth2/OpenID-Connect SAML based on HTTP header to delegate authentication to reverse proxy Multi-factor authentication can be enabled to enforce security on user authentication. Several authentication providers can be enable. Each of them is check sequentially (order is important). Active Directory # LDAP # OAuth2 / OpenID-Connect # SAML # User synchronisation # The user can be provisionned and deprovisionned automatically based on the content of a directory. The user data are synchronised periodically. New users in LDAP are created in TheHive, removed users are disabled. The organisation membership and the profile of an user are set using LDAP groups. The configuration contain the mapping of LDAP groups with organisation/profile.","title":"Authentication"},{"location":"thehive/how-to/authentication/#authentication","text":"TheHive supports several authentication providers: local (credential are securely stored in TheHive database) directory (LDAP and Active Directory) OAuth2/OpenID-Connect SAML based on HTTP header to delegate authentication to reverse proxy Multi-factor authentication can be enabled to enforce security on user authentication. Several authentication providers can be enable. Each of them is check sequentially (order is important).","title":"Authentication"},{"location":"thehive/how-to/authentication/#active-directory","text":"","title":"Active Directory"},{"location":"thehive/how-to/authentication/#ldap","text":"","title":"LDAP"},{"location":"thehive/how-to/authentication/#oauth2-openid-connect","text":"","title":"OAuth2 / OpenID-Connect"},{"location":"thehive/how-to/authentication/#saml","text":"","title":"SAML"},{"location":"thehive/how-to/authentication/#user-synchronisation","text":"The user can be provisionned and deprovisionned automatically based on the content of a directory. The user data are synchronised periodically. New users in LDAP are created in TheHive, removed users are disabled. The organisation membership and the profile of an user are set using LDAP groups. The configuration contain the mapping of LDAP groups with organisation/profile.","title":"User synchronisation"},{"location":"thehive/how-to/case-management/","text":"Case Management # Case Management is the main purpose of TheHive. Handling incidents with predefined tasks or manually added tasks, assiging a case owner, adding observables and enrich them, looking for correlations with existing cases and alert, prioritising incidents and classifying them... those are few of the case management capabilities in TheHive. Creating case # Cases can be created in various ways: Manually from scratch Manually using a case template Importing a TheHive archive generated from another TheHive instance Converting one or many alerts into a incident Creating a case from a case template # Case templates are models of cases, including predefined and documented tasks as well as custom fields Applying case template on ongoing investigations # Case templates can also be used to enrich a case with additional tasks, tags and custom field during open investigations: Anatomy of a case # A case in TheHive is defined by: A set of predefined properties: Title, tags, assignee, TLP, PAP, severity, description, status A set of custom fields (optional or mandatory) A set of tasks, defined by a title, assignee, status, description and a set of task logs and attachements A set of observables, of predefined or custom data types, defined by a value, IoC and Sighted flags, sighting date, tags and a description A set of TTPs related to MITRE ATT&CK A set of attachments A set of pages as a wiki A set of comments Case tasks # Case observables # Case TTPs # Case timeline # Case correlations # Case correlations with existing cases and alert are based on the common observables Case export # Cases can be exported as password protected archives or as a MISP event","title":"Case Management"},{"location":"thehive/how-to/case-management/#case-management","text":"Case Management is the main purpose of TheHive. Handling incidents with predefined tasks or manually added tasks, assiging a case owner, adding observables and enrich them, looking for correlations with existing cases and alert, prioritising incidents and classifying them... those are few of the case management capabilities in TheHive.","title":"Case Management"},{"location":"thehive/how-to/case-management/#creating-case","text":"Cases can be created in various ways: Manually from scratch Manually using a case template Importing a TheHive archive generated from another TheHive instance Converting one or many alerts into a incident","title":"Creating case"},{"location":"thehive/how-to/case-management/#creating-a-case-from-a-case-template","text":"Case templates are models of cases, including predefined and documented tasks as well as custom fields","title":"Creating a case from a case template"},{"location":"thehive/how-to/case-management/#applying-case-template-on-ongoing-investigations","text":"Case templates can also be used to enrich a case with additional tasks, tags and custom field during open investigations:","title":"Applying case template on ongoing investigations"},{"location":"thehive/how-to/case-management/#anatomy-of-a-case","text":"A case in TheHive is defined by: A set of predefined properties: Title, tags, assignee, TLP, PAP, severity, description, status A set of custom fields (optional or mandatory) A set of tasks, defined by a title, assignee, status, description and a set of task logs and attachements A set of observables, of predefined or custom data types, defined by a value, IoC and Sighted flags, sighting date, tags and a description A set of TTPs related to MITRE ATT&CK A set of attachments A set of pages as a wiki A set of comments","title":"Anatomy of a case"},{"location":"thehive/how-to/case-management/#case-tasks","text":"","title":"Case tasks"},{"location":"thehive/how-to/case-management/#case-observables","text":"","title":"Case observables"},{"location":"thehive/how-to/case-management/#case-ttps","text":"","title":"Case TTPs"},{"location":"thehive/how-to/case-management/#case-timeline","text":"","title":"Case timeline"},{"location":"thehive/how-to/case-management/#case-correlations","text":"Case correlations with existing cases and alert are based on the common observables","title":"Case correlations"},{"location":"thehive/how-to/case-management/#case-export","text":"Cases can be exported as password protected archives or as a MISP event","title":"Case export"},{"location":"thehive/how-to/dashboards/","text":"Dashboards and Reporting # TheHive comes with a reporting module that allows designing shared and private dashbords using various widgets for data visualisation. Reports can gather metrics from any data stored in TheHive like cases, alerts, tasks, observables... Dashboards # Every user has read access to the dashboards defined in the organisation (s)he belongs to. If the manageDashboard permission is part of the user's profile, the user can create dashboards. Create a dashboard # List dashboards # View dashboard # Configure widgets # Case timelines # Case timelines are a second part of TheHive's reporting capabilities. Case timelines display any event that happened during the lifecycle of a given case: Alert occurences Case creation Investigation start Task completion Flagged task logs IoC sightings Mitre Attack patterns Additional custom events","title":"Dashboards and Reporting"},{"location":"thehive/how-to/dashboards/#dashboards-and-reporting","text":"TheHive comes with a reporting module that allows designing shared and private dashbords using various widgets for data visualisation. Reports can gather metrics from any data stored in TheHive like cases, alerts, tasks, observables...","title":"Dashboards and Reporting"},{"location":"thehive/how-to/dashboards/#dashboards","text":"Every user has read access to the dashboards defined in the organisation (s)he belongs to. If the manageDashboard permission is part of the user's profile, the user can create dashboards.","title":"Dashboards"},{"location":"thehive/how-to/dashboards/#create-a-dashboard","text":"","title":"Create a dashboard"},{"location":"thehive/how-to/dashboards/#list-dashboards","text":"","title":"List dashboards"},{"location":"thehive/how-to/dashboards/#view-dashboard","text":"","title":"View dashboard"},{"location":"thehive/how-to/dashboards/#configure-widgets","text":"","title":"Configure widgets"},{"location":"thehive/how-to/dashboards/#case-timelines","text":"Case timelines are a second part of TheHive's reporting capabilities. Case timelines display any event that happened during the lifecycle of a given case: Alert occurences Case creation Investigation start Task completion Flagged task logs IoC sightings Mitre Attack patterns Additional custom events","title":"Case timelines"},{"location":"thehive/how-to/functions/","text":"Functions # Functions enable you to integrate external applications directly into TheHive processing. A Function is a piece of custom Javascript code that runs inside TheHive. The function can receive inputs from the outside, treat it and call TheHive APIs directly. This can be used for instance to create alerts inside TheHive without a python glue service that transforms the data. Create a function # Let's imagine that when an event occurs in your system, you want to create an alert in TheHive. Your external system has its own schema for the events, something like: { \"eventId\" : \"d9ec98b1-410f-40eb-8634-cfe189749da6\" , \"date\" : \"2021-06-05T12:45:36.698Z\" , \"title\" : \"An intrusion was detected\" , \"details\" : \"An intrusion was detected on the server 10.10.43.2\" , \"data\" : [ { \"kind\" : \"ip\" , \"value\" : \"10.10.43.2\" , \"name\" : \"server-ip\" }, { \"kind\" : \"name\" , \"value\" : \"root\" , \"name\" : \"login\" }, { \"kind\" : \"ip\" , \"value\" : \"92.43.123.1\" , \"name\" : \"origin\" } ] } This format is not the same as TheHive, so you need to transform the data to match TheHive alert format. As an org-admin , you can create new functions for your organization that can take this input, transform it into TheHive format and create an alert from it. The code of the function would be something like this: function handle ( input , context ) { const theHiveAlert = { \"type\" : \"event\" , \"source\" : \"my-system\" , \"sourceRef\" : input . eventId , \"title\" : input . title , \"description\" : input . details , \"date\" : ( new Date ( input . date )). getTime (), \"observables\" : input . data . map ( data => { // map event data kind to TheHive Observable type const dataType = data . kind === \"ip\" ? \"ip\" : \"other\" ; return { \"dataType\" : dataType , \"data\" : data . value , \"tags\" : [ `name: ${ data . name } ` ] // use a tag for the data name } }) }; // call TheHive APIs, here alert creation return context . alert . create ( theHiveAlert ); } A function can be in one of three modes: Enabled : The function will be executed when called Disabled : The function will not be executed when called Dry-Run : The function will be executed but no entity will be created or modified in TheHive. Entity creations will return null instead. This can be useful to test your integration before setting it live. The creation page allows you to test your function and see what it would return once executed. In dry-run mode, the function will be executed but no resource creation or modification will be executed. Call a function # Once saved, the function can then be called with an http call from your system: curl -X POST -H 'Authorization: Bearer $API_KEY' https://<thehive_url>/api/v1/function/<function_name> -H 'Content-Type: application/json' --data ' { \"eventId\": \"d9ec98b1-410f-40eb-8634-cfe189749da6\", \"date\": \"2021-06-05T12:45:36.698Z\", \"title\": \"An intrusion was detected\", \"details\": \"An intrusion was detected on the server 10.10.43.2\", \"data\": [ {\"kind\": \"ip\", \"value\": \"10.10.43.2\", \"name\": \"server-ip\" }, {\"kind\": \"name\", \"value\": \"root\", \"name\": \"login\" }, {\"kind\": \"ip\", \"value\": \"92.43.123.1\", \"name\": \"origin\" } ] } ' TheHive will take your input (the body of the http call), the definition of your function and execute the function with the input. It will the respond to the http call with the data returned by the function. Example: Create an alert from a Splunk alert # When creating a Splunk alert, you can define a webhook as an action . So when the alert is triggered the webhook is called with a payload. But the payload is defined by splunk and can't be changed. It should look a bit like: { \"sid\" : \"rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1\" , \"search_name\" : \"My Alert\" , \"app\" : \"search\" , \"owner\" : \"admin\" , \"results_link\" : \"http://8afeb4633464:8000/app/search/search?q=%7Cloadjob%20rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1%20%7C%20head%201%20%7C%20tail%201&earliest=0&latest=now\" , \"result\" : { \"_time\" : \"1659705859.827088\" , \"host\" : \"8afeb4633464\" , \"source\" : \"audittrail\" , \"sourcetype\" : \"audittrail\" , \"action\" : \"edit_search_schedule_priority\" , \"info\" : \"granted\" , \"user\" : \"admin\" , \"is_searches\" : \"0\" , \"is_not_searches\" : \"1\" , \"is_modify\" : \"0\" , \"is_not_modify\" : \"1\" , \"_confstr\" : \"source::audittrail|host::8afeb4633464|audittrail\" , \"_indextime\" : \"1659705859\" , \"_kv\" : \"1\" , \"_raw\" : \"Audit:[timestamp=08-05-2022 13:24:19.827, user=admin, action=edit_search_schedule_priority, info=granted ]\" , \"_serial\" : \"1\" , \"_si\" : [ \"8afeb4633464\" , \"_audit\" ], \"_sourcetype\" : \"audittrail\" , \"_subsecond\" : \".827088\" } } To transform this splunk alert into a TheHive alert, a function like this can be used: function handle ( input , context ) { const theHiveAlert = { \"type\" : \"splunk\" , \"source\" : input . search_name , \"sourceRef\" : input . result . _serial , \"title\" : `Splunk Alert triggered: ${ input . search_name } by ${ input . result . sourcetype } ` , \"description\" : `Alert created by splunk search ' ${ input . search_name } :\\n ${ input . result . _raw } '` , \"date\" : ( new Date ( parseFloat ( input . result . _time ) * 1000 )). getTime (), \"observables\" : [ { \"dataType\" : \"hostname\" , \"data\" : input . result . host }, { \"dataType\" : \"other\" , \"data\" : input . result . action , \"message\" : \"action\" }, { \"dataType\" : \"other\" , \"data\" : input . result . _raw_ , \"message\" : \"raw\" } ] }; return context . alert . create ( theHiveAlert ); } In splunk, you will need to set the webhook url to TheHive function url.","title":"Functions"},{"location":"thehive/how-to/functions/#functions","text":"Functions enable you to integrate external applications directly into TheHive processing. A Function is a piece of custom Javascript code that runs inside TheHive. The function can receive inputs from the outside, treat it and call TheHive APIs directly. This can be used for instance to create alerts inside TheHive without a python glue service that transforms the data.","title":"Functions"},{"location":"thehive/how-to/functions/#create-a-function","text":"Let's imagine that when an event occurs in your system, you want to create an alert in TheHive. Your external system has its own schema for the events, something like: { \"eventId\" : \"d9ec98b1-410f-40eb-8634-cfe189749da6\" , \"date\" : \"2021-06-05T12:45:36.698Z\" , \"title\" : \"An intrusion was detected\" , \"details\" : \"An intrusion was detected on the server 10.10.43.2\" , \"data\" : [ { \"kind\" : \"ip\" , \"value\" : \"10.10.43.2\" , \"name\" : \"server-ip\" }, { \"kind\" : \"name\" , \"value\" : \"root\" , \"name\" : \"login\" }, { \"kind\" : \"ip\" , \"value\" : \"92.43.123.1\" , \"name\" : \"origin\" } ] } This format is not the same as TheHive, so you need to transform the data to match TheHive alert format. As an org-admin , you can create new functions for your organization that can take this input, transform it into TheHive format and create an alert from it. The code of the function would be something like this: function handle ( input , context ) { const theHiveAlert = { \"type\" : \"event\" , \"source\" : \"my-system\" , \"sourceRef\" : input . eventId , \"title\" : input . title , \"description\" : input . details , \"date\" : ( new Date ( input . date )). getTime (), \"observables\" : input . data . map ( data => { // map event data kind to TheHive Observable type const dataType = data . kind === \"ip\" ? \"ip\" : \"other\" ; return { \"dataType\" : dataType , \"data\" : data . value , \"tags\" : [ `name: ${ data . name } ` ] // use a tag for the data name } }) }; // call TheHive APIs, here alert creation return context . alert . create ( theHiveAlert ); } A function can be in one of three modes: Enabled : The function will be executed when called Disabled : The function will not be executed when called Dry-Run : The function will be executed but no entity will be created or modified in TheHive. Entity creations will return null instead. This can be useful to test your integration before setting it live. The creation page allows you to test your function and see what it would return once executed. In dry-run mode, the function will be executed but no resource creation or modification will be executed.","title":"Create a function"},{"location":"thehive/how-to/functions/#call-a-function","text":"Once saved, the function can then be called with an http call from your system: curl -X POST -H 'Authorization: Bearer $API_KEY' https://<thehive_url>/api/v1/function/<function_name> -H 'Content-Type: application/json' --data ' { \"eventId\": \"d9ec98b1-410f-40eb-8634-cfe189749da6\", \"date\": \"2021-06-05T12:45:36.698Z\", \"title\": \"An intrusion was detected\", \"details\": \"An intrusion was detected on the server 10.10.43.2\", \"data\": [ {\"kind\": \"ip\", \"value\": \"10.10.43.2\", \"name\": \"server-ip\" }, {\"kind\": \"name\", \"value\": \"root\", \"name\": \"login\" }, {\"kind\": \"ip\", \"value\": \"92.43.123.1\", \"name\": \"origin\" } ] } ' TheHive will take your input (the body of the http call), the definition of your function and execute the function with the input. It will the respond to the http call with the data returned by the function.","title":"Call a function"},{"location":"thehive/how-to/functions/#example-create-an-alert-from-a-splunk-alert","text":"When creating a Splunk alert, you can define a webhook as an action . So when the alert is triggered the webhook is called with a payload. But the payload is defined by splunk and can't be changed. It should look a bit like: { \"sid\" : \"rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1\" , \"search_name\" : \"My Alert\" , \"app\" : \"search\" , \"owner\" : \"admin\" , \"results_link\" : \"http://8afeb4633464:8000/app/search/search?q=%7Cloadjob%20rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1%20%7C%20head%201%20%7C%20tail%201&earliest=0&latest=now\" , \"result\" : { \"_time\" : \"1659705859.827088\" , \"host\" : \"8afeb4633464\" , \"source\" : \"audittrail\" , \"sourcetype\" : \"audittrail\" , \"action\" : \"edit_search_schedule_priority\" , \"info\" : \"granted\" , \"user\" : \"admin\" , \"is_searches\" : \"0\" , \"is_not_searches\" : \"1\" , \"is_modify\" : \"0\" , \"is_not_modify\" : \"1\" , \"_confstr\" : \"source::audittrail|host::8afeb4633464|audittrail\" , \"_indextime\" : \"1659705859\" , \"_kv\" : \"1\" , \"_raw\" : \"Audit:[timestamp=08-05-2022 13:24:19.827, user=admin, action=edit_search_schedule_priority, info=granted ]\" , \"_serial\" : \"1\" , \"_si\" : [ \"8afeb4633464\" , \"_audit\" ], \"_sourcetype\" : \"audittrail\" , \"_subsecond\" : \".827088\" } } To transform this splunk alert into a TheHive alert, a function like this can be used: function handle ( input , context ) { const theHiveAlert = { \"type\" : \"splunk\" , \"source\" : input . search_name , \"sourceRef\" : input . result . _serial , \"title\" : `Splunk Alert triggered: ${ input . search_name } by ${ input . result . sourcetype } ` , \"description\" : `Alert created by splunk search ' ${ input . search_name } :\\n ${ input . result . _raw } '` , \"date\" : ( new Date ( parseFloat ( input . result . _time ) * 1000 )). getTime (), \"observables\" : [ { \"dataType\" : \"hostname\" , \"data\" : input . result . host }, { \"dataType\" : \"other\" , \"data\" : input . result . action , \"message\" : \"action\" }, { \"dataType\" : \"other\" , \"data\" : input . result . _raw_ , \"message\" : \"raw\" } ] }; return context . alert . create ( theHiveAlert ); } In splunk, you will need to set the webhook url to TheHive function url.","title":"Example: Create an alert from a Splunk alert"},{"location":"thehive/how-to/knowledge-base/","text":"Knwoledge Base # TheHive has a knowledge base module that allow writing Markdown pages at two levels: Organisation level Case level Organisation wiki # Every organisation is able to define a set of Markdown pages accessible to all the users. Adding pages requires a manageKnowledgeBase user permission. Case pages # Within every case, users with managePage permissions, can create and write Markdown pages. This feature ca, be used for: meeting notes reports pasties any other content","title":"Knwoledge Base"},{"location":"thehive/how-to/knowledge-base/#knwoledge-base","text":"TheHive has a knowledge base module that allow writing Markdown pages at two levels: Organisation level Case level","title":"Knwoledge Base"},{"location":"thehive/how-to/knowledge-base/#organisation-wiki","text":"Every organisation is able to define a set of Markdown pages accessible to all the users. Adding pages requires a manageKnowledgeBase user permission.","title":"Organisation wiki"},{"location":"thehive/how-to/knowledge-base/#case-pages","text":"Within every case, users with managePage permissions, can create and write Markdown pages. This feature ca, be used for: meeting notes reports pasties any other content","title":"Case pages"},{"location":"thehive/how-to/misp-integration/","text":"MISP Integration # TheHive in strongly integrated with MISP (Malware Information Sharing Platform). Using it's connector, TheHive has the capabilities to: Receive MISP events and ingest them as alerts Send TheHive Cases to MISP as events This integration is highly configurable and allows TheHive to synchronize with one or multiple MISP servers. Configuration # To add or configure a MISP server, open the Admin Organisation page (1), go to the Platform Management menu (2) and navigate to the MISP tab (3). Click the \"+\" button to add a new MISP server (4). General settings # This configuration is common to all MISP servers connected to TheHive. Interval: define the time interval between each events polling from TheHive to MISP Servers General settings # While clicking on add or edit a MISP server, a drawer will appear where you can define the following settings: Server name: MISP server name to display within TheHive Server URL: URL of the MISP server API Key: secret with sufficient permission to get & create MISP events Purpose: Chose the synchronization way; Import: only import events from MISP to TheHive. Export: only exports cases from TheHive to MISP. Import and Export allow both ways synchronization Server Proxy Settings # Proxy settings should be set only if a proxy is required to reach the MISP server from TheHive. Type of protocol: Define on which protocol (HTTP/HTTPS) the proxy is listening Address: Define the proxy address Authentication: If the proxy require authentication, check this box. Username and password to authenticate must be provided when this box is checked. Do not check certificate authority: Do not verify the certificate authority when communicating with the proxy (not recommended, for HTTPS connection only) Disable hostname verification: Do not verify the hostname match with the certificate hostname. Server Advanced Settings # Chose the filter on TheHive organizations: For each server, you can define which TheHive organisation(s) to include or exclude of the synchronization (excluded or not included organisations will not receive the MISP events as Alerts ) Tags: Append one or several tags to each MISP event ingested as Alert Export case tags: If enabled, the export will include the Case tags. Export observables tags: If enabled, the exported Observables will include the Observables tags. Server Filter Settings # This section allows to define filters for MISP events import. Maximum age: define the maximum age (based on creation date) for an event to be imported in TheHive. Organizations to include: Import only events created by the MISP organisation(s) defined in this field. Organizations to exclude: Import only events NOT created by the MISP organisation(s) defined in this field. Maximum number of attributes: Define a maximum number of MISP attributes (observables) per event to import. List of allowed tags: Import only events that contains the tags defined in this field Prohibited tags list: Import only events that DON'T contains the tags defined in this field","title":"MISP Integration"},{"location":"thehive/how-to/misp-integration/#misp-integration","text":"TheHive in strongly integrated with MISP (Malware Information Sharing Platform). Using it's connector, TheHive has the capabilities to: Receive MISP events and ingest them as alerts Send TheHive Cases to MISP as events This integration is highly configurable and allows TheHive to synchronize with one or multiple MISP servers.","title":"MISP Integration"},{"location":"thehive/how-to/misp-integration/#configuration","text":"To add or configure a MISP server, open the Admin Organisation page (1), go to the Platform Management menu (2) and navigate to the MISP tab (3). Click the \"+\" button to add a new MISP server (4).","title":"Configuration"},{"location":"thehive/how-to/misp-integration/#general-settings","text":"This configuration is common to all MISP servers connected to TheHive. Interval: define the time interval between each events polling from TheHive to MISP","title":"General settings"},{"location":"thehive/how-to/misp-integration/#servers-general-settings","text":"While clicking on add or edit a MISP server, a drawer will appear where you can define the following settings: Server name: MISP server name to display within TheHive Server URL: URL of the MISP server API Key: secret with sufficient permission to get & create MISP events Purpose: Chose the synchronization way; Import: only import events from MISP to TheHive. Export: only exports cases from TheHive to MISP. Import and Export allow both ways synchronization","title":"Servers General settings"},{"location":"thehive/how-to/misp-integration/#server-proxy-settings","text":"Proxy settings should be set only if a proxy is required to reach the MISP server from TheHive. Type of protocol: Define on which protocol (HTTP/HTTPS) the proxy is listening Address: Define the proxy address Authentication: If the proxy require authentication, check this box. Username and password to authenticate must be provided when this box is checked. Do not check certificate authority: Do not verify the certificate authority when communicating with the proxy (not recommended, for HTTPS connection only) Disable hostname verification: Do not verify the hostname match with the certificate hostname.","title":"Server Proxy Settings"},{"location":"thehive/how-to/misp-integration/#server-advanced-settings","text":"Chose the filter on TheHive organizations: For each server, you can define which TheHive organisation(s) to include or exclude of the synchronization (excluded or not included organisations will not receive the MISP events as Alerts ) Tags: Append one or several tags to each MISP event ingested as Alert Export case tags: If enabled, the export will include the Case tags. Export observables tags: If enabled, the exported Observables will include the Observables tags.","title":"Server Advanced Settings"},{"location":"thehive/how-to/misp-integration/#server-filter-settings","text":"This section allows to define filters for MISP events import. Maximum age: define the maximum age (based on creation date) for an event to be imported in TheHive. Organizations to include: Import only events created by the MISP organisation(s) defined in this field. Organizations to exclude: Import only events NOT created by the MISP organisation(s) defined in this field. Maximum number of attributes: Define a maximum number of MISP attributes (observables) per event to import. List of allowed tags: Import only events that contains the tags defined in this field Prohibited tags list: Import only events that DON'T contains the tags defined in this field","title":"Server Filter Settings"},{"location":"thehive/how-to/notifications/","text":"Notifications # TheHive Notifications allow you to automatically react on specific events occurring in TheHive and send notification to defined Endpoints that can be: Cortex Webhook listener Http listener Slack Mattermost Endpoints need to be configured prior to use them in Notifications. You can also send an Email as notification. Notifications management # Notifications are unique to each organisation. With an org admin account open the Organization menu (1), and navigate to the Notifications tab (2). To create a notification, clic on the \"+\" button (3) Configure a Notification # While clicking on add or edit a notifier, a drawer will appear where you can define the following settings: Name: Notification name to display within TheHive Send notification to every user in the organisation: Check this box to notify by email every users of the organization this Notifier has triggered Trigger: Chose in a list of triggers on which event you want to react. You can also select \"FilteredEvent\" to create your own event filter. Enable notification: Check this box to enable the notifier. Uncheck the box to disable the notifier. Finally, select which endpoint will receive the notification. Pre-defined triggers & FilteredEvent # While configuring the Trigger setting, you can pick a pre-defined trigger from a list, or chose to create your own filters. Current pre-defined filters list: AnyEvent Case Created Case Closed Case Shared Alert Created Alert Imported Job Finished Alert Observable Created Case Observable Created Observable Created Log in my task Task Assigned Task Closed Task Mandatory But you can also chose to use a custom filter to react on specific events. Custom filters are JSON format written and can use common operators. Example with a filter for cases which Severity is updated to High or Critical: Use variables in notifications # You can include variables in your Email & HTTP notification. Use the \"add variable\" bouton to see the list of available variables. Example with an email notification: See our Leveraging TheHive 5 notifications capabilities blog articles to know more about Notifications","title":"Notifications"},{"location":"thehive/how-to/notifications/#notifications","text":"TheHive Notifications allow you to automatically react on specific events occurring in TheHive and send notification to defined Endpoints that can be: Cortex Webhook listener Http listener Slack Mattermost Endpoints need to be configured prior to use them in Notifications. You can also send an Email as notification.","title":"Notifications"},{"location":"thehive/how-to/notifications/#notifications-management","text":"Notifications are unique to each organisation. With an org admin account open the Organization menu (1), and navigate to the Notifications tab (2). To create a notification, clic on the \"+\" button (3)","title":"Notifications management"},{"location":"thehive/how-to/notifications/#configure-a-notification","text":"While clicking on add or edit a notifier, a drawer will appear where you can define the following settings: Name: Notification name to display within TheHive Send notification to every user in the organisation: Check this box to notify by email every users of the organization this Notifier has triggered Trigger: Chose in a list of triggers on which event you want to react. You can also select \"FilteredEvent\" to create your own event filter. Enable notification: Check this box to enable the notifier. Uncheck the box to disable the notifier. Finally, select which endpoint will receive the notification.","title":"Configure a Notification"},{"location":"thehive/how-to/notifications/#pre-defined-triggers-filteredevent","text":"While configuring the Trigger setting, you can pick a pre-defined trigger from a list, or chose to create your own filters. Current pre-defined filters list: AnyEvent Case Created Case Closed Case Shared Alert Created Alert Imported Job Finished Alert Observable Created Case Observable Created Observable Created Log in my task Task Assigned Task Closed Task Mandatory But you can also chose to use a custom filter to react on specific events. Custom filters are JSON format written and can use common operators. Example with a filter for cases which Severity is updated to High or Critical:","title":"Pre-defined triggers &amp; FilteredEvent"},{"location":"thehive/how-to/notifications/#use-variables-in-notifications","text":"You can include variables in your Email & HTTP notification. Use the \"add variable\" bouton to see the list of available variables. Example with an email notification: See our Leveraging TheHive 5 notifications capabilities blog articles to know more about Notifications","title":"Use variables in notifications"},{"location":"thehive/how-to/user-management/","text":"User Management # In TheHive users can be created once an added to different organistions. User lists are available to admin and org-admin users: When adding a user in an organisation, a user profile can be choosen for every organisation: Users can be created by administrators or organisation administrators or any user having the manageUser permission. This permission is included by default in the admin and org-admin user profiles. In TheHive, there are two types of users: Users with GUI access Service account aka. API users Once created, users can be assigned a password and an API key","title":"User Management"},{"location":"thehive/how-to/user-management/#user-management","text":"In TheHive users can be created once an added to different organistions. User lists are available to admin and org-admin users: When adding a user in an organisation, a user profile can be choosen for every organisation: Users can be created by administrators or organisation administrators or any user having the manageUser permission. This permission is included by default in the admin and org-admin user profiles. In TheHive, there are two types of users: Users with GUI access Service account aka. API users Once created, users can be assigned a password and an API key","title":"User Management"},{"location":"thehive/organization/Filter-and-sort/","text":"Filters # In this section, you can find information about applying filters. To apply filter: On the list of incidents page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters. Sorting # Sorting can be performed on any field values. To Sort: On the list of incidents page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Filters"},{"location":"thehive/organization/Filter-and-sort/#filters","text":"In this section, you can find information about applying filters. To apply filter: On the list of incidents page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/Filter-and-sort/#sorting","text":"Sorting can be performed on any field values. To Sort: On the list of incidents page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Sorting"},{"location":"thehive/organization/authentication/","text":"","title":"Authentication"},{"location":"thehive/organization/contacting-support/","text":"Contact Support #","title":"Contact Support"},{"location":"thehive/organization/contacting-support/#contact-support","text":"","title":"Contact Support"},{"location":"thehive/organization/getting-started/","text":"Getting Started # TheHive supports different roles for users. Whether you are an administrator of the platform, an organization, or an analyst, you can have access and run different actions on the platform. This user guide aims at describing all major howtos when you sign in as a Org-Admin.","title":"Getting Started"},{"location":"thehive/organization/getting-started/#getting-started","text":"TheHive supports different roles for users. Whether you are an administrator of the platform, an organization, or an analyst, you can have access and run different actions on the platform. This user guide aims at describing all major howtos when you sign in as a Org-Admin.","title":"Getting Started"},{"location":"thehive/organization/introduction/","text":"Introduction # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly.","title":"Introduction"},{"location":"thehive/organization/introduction/#introduction","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly.","title":"Introduction"},{"location":"thehive/organization/list-of-incidents/","text":"List of Incidents # After you login to TheHive application, you can see a list of incidents displayed on the screen by default. Main Menu # In the left pane, you can see the main Menu. Case List Alerts Tasks Dashboards Search Organization Clicking on the arrow will switch you back to the image view. Top Menu # On the top of the page are the following options: Defaults Quick Filters Auto Refresh Stats Filter Defaults # It displays the default view. Click on Default . A list is displayed. To Save a view: Click Save view As . A new window opens. Click Confirm . To Manage views: Click the default button. Click Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view. Select the action. e.g. Delete Quick Filters # To apply Quick filter: Click the quick filter option. The list displays options to select. Auto refresh # The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the list of incidents page, switch on the Auto refresh toggle button. Stats # To view Stats: On the cases list page, switch on the Stats toggle button. The stats are displayed. Filters # To apply filter: On the cases list page, switch on the Filters button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"List of Incidents"},{"location":"thehive/organization/list-of-incidents/#list-of-incidents","text":"After you login to TheHive application, you can see a list of incidents displayed on the screen by default.","title":"List of Incidents"},{"location":"thehive/organization/list-of-incidents/#main-menu","text":"In the left pane, you can see the main Menu. Case List Alerts Tasks Dashboards Search Organization Clicking on the arrow will switch you back to the image view.","title":"Main Menu"},{"location":"thehive/organization/list-of-incidents/#top-menu","text":"On the top of the page are the following options: Defaults Quick Filters Auto Refresh Stats Filter","title":"Top Menu"},{"location":"thehive/organization/list-of-incidents/#defaults","text":"It displays the default view. Click on Default . A list is displayed. To Save a view: Click Save view As . A new window opens. Click Confirm . To Manage views: Click the default button. Click Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view. Select the action. e.g. Delete","title":"Defaults"},{"location":"thehive/organization/list-of-incidents/#quick-filters","text":"To apply Quick filter: Click the quick filter option. The list displays options to select.","title":"Quick Filters"},{"location":"thehive/organization/list-of-incidents/#auto-refresh","text":"The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the list of incidents page, switch on the Auto refresh toggle button.","title":"Auto refresh"},{"location":"thehive/organization/list-of-incidents/#stats","text":"To view Stats: On the cases list page, switch on the Stats toggle button. The stats are displayed.","title":"Stats"},{"location":"thehive/organization/list-of-incidents/#filters","text":"To apply filter: On the cases list page, switch on the Filters button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/organizations/","text":"About Organization # Organizations are the customers or tenants who are using the TheHive application. TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other and can't share data. On the home page, you can see the organization List.","title":"About Organization"},{"location":"thehive/organization/organizations/#about-organization","text":"Organizations are the customers or tenants who are using the TheHive application. TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other and can't share data. On the home page, you can see the organization List.","title":"About Organization"},{"location":"thehive/organization/sign-in-as-an-admin/","text":"Sign In as Org-Admin # Open the TheHive application in your browser. Enter user credentials, Login name and password . Click the Let me In button.","title":"Sign In as Org-Admin"},{"location":"thehive/organization/sign-in-as-an-admin/#sign-in-as-org-admin","text":"Open the TheHive application in your browser. Enter user credentials, Login name and password . Click the Let me In button.","title":"Sign In as Org-Admin"},{"location":"thehive/organization/alerts/about-alerts/","text":"Alerts # In this section you can find information about Alerts. Alerts provide timely information about current security issues, vulnerabilities, and exploits. View alert details # To view alert details: You can click on any of the alerts in the list to view more details. The Alerts page displays various tabs that have more details about alerts such general tab, observables, TTPs, similar cases, similar alerts, responders tab.","title":"Alerts"},{"location":"thehive/organization/alerts/about-alerts/#alerts","text":"In this section you can find information about Alerts. Alerts provide timely information about current security issues, vulnerabilities, and exploits.","title":"Alerts"},{"location":"thehive/organization/alerts/about-alerts/#view-alert-details","text":"To view alert details: You can click on any of the alerts in the list to view more details. The Alerts page displays various tabs that have more details about alerts such general tab, observables, TTPs, similar cases, similar alerts, responders tab.","title":"View alert details"},{"location":"thehive/organization/alerts/actions/","text":"Actions # You can make use of any of the available actions. Start # Click the Start option to begin an alert. Close # Click the Close option to remove a task. A new window opens. Select Status from the list. Change the Summary Click the Close tasks and case button. Track/Ignore New Updates # Click the Track New Updates option to track an alert. A success message is displayed. Click the Ignore New Updates option to ignore an alert. A success message is displayed. Unlink # Click the Unlink option to unlink an alert. Click the OK button.","title":"Actions"},{"location":"thehive/organization/alerts/actions/#actions","text":"You can make use of any of the available actions.","title":"Actions"},{"location":"thehive/organization/alerts/actions/#start","text":"Click the Start option to begin an alert.","title":"Start"},{"location":"thehive/organization/alerts/actions/#close","text":"Click the Close option to remove a task. A new window opens. Select Status from the list. Change the Summary Click the Close tasks and case button.","title":"Close"},{"location":"thehive/organization/alerts/actions/#trackignore-new-updates","text":"Click the Track New Updates option to track an alert. A success message is displayed. Click the Ignore New Updates option to ignore an alert. A success message is displayed.","title":"Track/Ignore New Updates"},{"location":"thehive/organization/alerts/actions/#unlink","text":"Click the Unlink option to unlink an alert. Click the OK button.","title":"Unlink"},{"location":"thehive/organization/alerts/general/","text":"General # The information on the general page comes from templates and is auto-populated. In the left pane of the window, you can see details such as created by, created date, TLP, PAP, severity details, status of the alert, start date, and task completion details. In the left pane of the window you can configure the PAP,TLP and Severity. Refer to Configure Alert Details for more details. In the right pane of the window, enter Comments if any. Click the Comment button. Add Tags. (Refer to Add tags ). Enter the Description . Add Custom fields . (Refer to Add custom fields ) Click Add to enter the respective business unit and location details.","title":"General"},{"location":"thehive/organization/alerts/general/#general","text":"The information on the general page comes from templates and is auto-populated. In the left pane of the window, you can see details such as created by, created date, TLP, PAP, severity details, status of the alert, start date, and task completion details. In the left pane of the window you can configure the PAP,TLP and Severity. Refer to Configure Alert Details for more details. In the right pane of the window, enter Comments if any. Click the Comment button. Add Tags. (Refer to Add tags ). Enter the Description . Add Custom fields . (Refer to Add custom fields ) Click Add to enter the respective business unit and location details.","title":"General"},{"location":"thehive/organization/alerts/manage-views/","text":"Manage views # In this section, you can find information about managing views. To manage views: Click the default button. Click on Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete . Manage Alerts # There are various option available for to apply on the alerts. Quick Filters # To apply Quick filter: Click the Quick Filter option. The list displays options to select from. Auto refresh # The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the alerts page, switch on the Auto refresh button. Stats # To view stats: On the alerts page, switch on the Stats toggle button, the stats will be displayed. Filters # To apply filter: On the alerts page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters. Sorting # Sorting can be performed on any field values. To Sort: On the alerts page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Manage views"},{"location":"thehive/organization/alerts/manage-views/#manage-views","text":"In this section, you can find information about managing views. To manage views: Click the default button. Click on Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete .","title":"Manage views"},{"location":"thehive/organization/alerts/manage-views/#manage-alerts","text":"There are various option available for to apply on the alerts.","title":"Manage Alerts"},{"location":"thehive/organization/alerts/manage-views/#quick-filters","text":"To apply Quick filter: Click the Quick Filter option. The list displays options to select from.","title":"Quick Filters"},{"location":"thehive/organization/alerts/manage-views/#auto-refresh","text":"The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the alerts page, switch on the Auto refresh button.","title":"Auto refresh"},{"location":"thehive/organization/alerts/manage-views/#stats","text":"To view stats: On the alerts page, switch on the Stats toggle button, the stats will be displayed.","title":"Stats"},{"location":"thehive/organization/alerts/manage-views/#filters","text":"To apply filter: On the alerts page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/alerts/manage-views/#sorting","text":"Sorting can be performed on any field values. To Sort: On the alerts page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Sorting"},{"location":"thehive/organization/alerts/merge-alerts/","text":"Merge alerts # In this section you can find information about merging alerts. On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. Merge alerts / merge selection into case option is available only for New alerts from the list. To merge alerts: Go to alert details page. Select the alert to merge data into Click merge alerts . NOTE: Merging two cases, removes the originating cases, and creates a new one with all the merged data.","title":"Merge alerts"},{"location":"thehive/organization/alerts/merge-alerts/#merge-alerts","text":"In this section you can find information about merging alerts. On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. Merge alerts / merge selection into case option is available only for New alerts from the list. To merge alerts: Go to alert details page. Select the alert to merge data into Click merge alerts . NOTE: Merging two cases, removes the originating cases, and creates a new one with all the merged data.","title":"Merge alerts"},{"location":"thehive/organization/alerts/new-case-from-selection/","text":"New case from selection # In this section you can find information about creating a new case from selection. On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. New case from selection option is available only for new alerts from the list. To add a new case from selection: Go to alert details page. Select the alert for which you want to add new case. Click New Case from selection option. A new window opens.","title":"New case from selection"},{"location":"thehive/organization/alerts/new-case-from-selection/#new-case-from-selection","text":"In this section you can find information about creating a new case from selection. On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. New case from selection option is available only for new alerts from the list. To add a new case from selection: Go to alert details page. Select the alert for which you want to add new case. Click New Case from selection option. A new window opens.","title":"New case from selection"},{"location":"thehive/organization/alerts/preview-alerts/","text":"Preview alerts # In this section you can find information about previewing alerts and associated details. To preview the alert details: On the list of alerts page, there is a Preview button corresponding to the specific alert name. Click the Preview option. The alert details preview window opens. You can see details like the id, created by, created at date, last reviewed by, last reviewed date, import date, TLP, PAP and severity details, title, tags, description, status and summary of the alert. Add Custom fields (Refer to Add custom fields ), business unit , and location details. Click Add to enter business unit and location details. Actions # You can click the Actions Button to start, close, track/ignore new updates, unlink the alerts or to Run Responders . Alert Details # Click the Go to details button to view more details of the alert. Alert details menu # Click the Go to details button to view more details of the alert. On the top of the page, there are many task options available such as start, close, track/ignore new updates, unlink the alerts and run responders.","title":"Preview alerts"},{"location":"thehive/organization/alerts/preview-alerts/#preview-alerts","text":"In this section you can find information about previewing alerts and associated details. To preview the alert details: On the list of alerts page, there is a Preview button corresponding to the specific alert name. Click the Preview option. The alert details preview window opens. You can see details like the id, created by, created at date, last reviewed by, last reviewed date, import date, TLP, PAP and severity details, title, tags, description, status and summary of the alert. Add Custom fields (Refer to Add custom fields ), business unit , and location details. Click Add to enter business unit and location details.","title":"Preview alerts"},{"location":"thehive/organization/alerts/preview-alerts/#actions","text":"You can click the Actions Button to start, close, track/ignore new updates, unlink the alerts or to Run Responders .","title":"Actions"},{"location":"thehive/organization/alerts/preview-alerts/#alert-details","text":"Click the Go to details button to view more details of the alert.","title":"Alert Details"},{"location":"thehive/organization/alerts/preview-alerts/#alert-details-menu","text":"Click the Go to details button to view more details of the alert. On the top of the page, there are many task options available such as start, close, track/ignore new updates, unlink the alerts and run responders.","title":"Alert details menu"},{"location":"thehive/organization/alerts/run-responders/","text":"Run responders # Responders # Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Run responders"},{"location":"thehive/organization/alerts/run-responders/#run-responders","text":"","title":"Run responders"},{"location":"thehive/organization/alerts/run-responders/#responders","text":"Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Responders"},{"location":"thehive/organization/alerts/view-observables/","text":"View observables # In this section you can find information about viewing observables. When you install the TheHive application, it comes with a set of pre-defined observables such as IP and email addresses, URLs, domain names, files or hashes. You can define your own observable type. You can see the list of all Observable Types.","title":"View observables"},{"location":"thehive/organization/alerts/view-observables/#view-observables","text":"In this section you can find information about viewing observables. When you install the TheHive application, it comes with a set of pre-defined observables such as IP and email addresses, URLs, domain names, files or hashes. You can define your own observable type. You can see the list of all Observable Types.","title":"View observables"},{"location":"thehive/organization/alerts/view-responders/","text":"View responders # Responder is a tool that can be used in security penetration tests on the infrastructure of the networks.","title":"View responders"},{"location":"thehive/organization/alerts/view-responders/#view-responders","text":"Responder is a tool that can be used in security penetration tests on the infrastructure of the networks.","title":"View responders"},{"location":"thehive/organization/alerts/view-similar-alerts/","text":"View similar alerts # In this section, you can find information about all the similar alerts listed down.","title":"View similar alerts"},{"location":"thehive/organization/alerts/view-similar-alerts/#view-similar-alerts","text":"In this section, you can find information about all the similar alerts listed down.","title":"View similar alerts"},{"location":"thehive/organization/alerts/view-similar-cases/","text":"View similar cases # In this section, you can find information about all the similar cases listed down.","title":"View similar cases"},{"location":"thehive/organization/alerts/view-similar-cases/#view-similar-cases","text":"In this section, you can find information about all the similar cases listed down.","title":"View similar cases"},{"location":"thehive/organization/alerts/view-ttps/","text":"View TTPS # In this section, you can find information about all the TTPS listed down. Tactics, techniques, and procedures (TTPs) are the patterns of activities or methods associated with a specific threat actor or group of threat actors.","title":"View TTPS"},{"location":"thehive/organization/alerts/view-ttps/#view-ttps","text":"In this section, you can find information about all the TTPS listed down. Tactics, techniques, and procedures (TTPs) are the patterns of activities or methods associated with a specific threat actor or group of threat actors.","title":"View TTPS"},{"location":"thehive/organization/cases/about-a-case/","text":"About a case # In this section you can find information about cases. A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users. Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats.","title":"About a case"},{"location":"thehive/organization/cases/about-a-case/#about-a-case","text":"In this section you can find information about cases. A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users. Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats.","title":"About a case"},{"location":"thehive/organization/cases/adding_to_a_case/","text":"Adding to a case (Tags/Tasks/Custom field values) # Add tags # Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box. Click the Add selected tags button. Add tasks # The task Group is default. Enter the task Title . Enter the task description in the Description . Switch the toggle button to Flag this task? . Select the Due date . Click Save and add another , to add another task. Click Confirm . Add custom field values # Select custom field value from the given list. (location/business-unit/detection-source/test). Click Confirm custom field value creation .","title":"Adding to a case (Tags/Tasks/Custom field values)"},{"location":"thehive/organization/cases/adding_to_a_case/#adding-to-a-case-tagstaskscustom-field-values","text":"","title":"Adding to a case (Tags/Tasks/Custom field values)"},{"location":"thehive/organization/cases/adding_to_a_case/#add-tags","text":"Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box. Click the Add selected tags button.","title":"Add tags"},{"location":"thehive/organization/cases/adding_to_a_case/#add-tasks","text":"The task Group is default. Enter the task Title . Enter the task description in the Description . Switch the toggle button to Flag this task? . Select the Due date . Click Save and add another , to add another task. Click Confirm .","title":"Add tasks"},{"location":"thehive/organization/cases/adding_to_a_case/#add-custom-field-values","text":"Select custom field value from the given list. (location/business-unit/detection-source/test). Click Confirm custom field value creation .","title":"Add custom field values"},{"location":"thehive/organization/cases/create-a-new-case/","text":"Create new cases using templates # A User can create new cases using templates. Click Create Case + on the header. A new screen opens. A user can create cases by selecting any one of the following options: Click the below links to create each type of new case. Empty Case EDR / Phishing Template Archive MISP","title":"Create new cases using templates"},{"location":"thehive/organization/cases/create-a-new-case/#create-new-cases-using-templates","text":"A User can create new cases using templates. Click Create Case + on the header. A new screen opens. A user can create cases by selecting any one of the following options: Click the below links to create each type of new case. Empty Case EDR / Phishing Template Archive MISP","title":"Create new cases using templates"},{"location":"thehive/organization/cases/create-case-from-archive/","text":"From archive (.thar) # Create a new case from archive. Upload File as Attachment. Enter the Password . Click the Confirm case creation button.","title":"From archive (.thar)"},{"location":"thehive/organization/cases/create-case-from-archive/#from-archive-thar","text":"Create a new case from archive. Upload File as Attachment. Enter the Password . Click the Confirm case creation button.","title":"From archive (.thar)"},{"location":"thehive/organization/cases/create-case-from-misp/","text":"From MISP(.json) # Create a new case from MISP. Upload File as Attachment.(import from MISP). Add Tasks. (Refer to Add tasks ). Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case creation button.","title":"From MISP(.json)"},{"location":"thehive/organization/cases/create-case-from-misp/#from-mispjson","text":"Create a new case from MISP. Upload File as Attachment.(import from MISP). Add Tasks. (Refer to Add tasks ). Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case creation button.","title":"From MISP(.json)"},{"location":"thehive/organization/cases/create-case-from-template/","text":"From template # Create a new case from templates 1. Create a new case from EDR template. # Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags .) Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks . / Edit tasks . / Delete tasks .) Add Custom Fields. (Refer to Add custom field values . / Edit custom field values . / Delete custom field values .) Click the Confirm case creation button. 2. Create a new case from Phishing template. # Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags .) Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks . / Edit tasks . / Delete tasks .) Add Custom Fields. (Refer to Add custom field values . / Edit custom field values . / Delete custom field values .) Click the Confirm case creation button. Add tags # Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box Click the Add selected tags button. Add tasks # The task Group is default. Enter the task Title . Enter the task description in the Description . Select the Due date . Click Save . Click Save and add another , to add another task. Add custom field values # Select custom field value from the given list. (location/business-unit/detection-source/test). Click Confirm custom field value creation . Edit tasks # Click the edit link. A new window opens. Edit the required values Click the Confirm edition button. Edit custom field values # Click the edit link. A new window opens. Edit the required custom field values Click the Confirm custom field value edition button. Delete tasks # Click the delete link beside the value that has to be deleted. Delete custom field values # Click the delete link beside the custom field value that has to be deleted.","title":"From template"},{"location":"thehive/organization/cases/create-case-from-template/#from-template","text":"Create a new case from templates","title":"From template"},{"location":"thehive/organization/cases/create-case-from-template/#1-create-a-new-case-from-edr-template","text":"Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags .) Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks . / Edit tasks . / Delete tasks .) Add Custom Fields. (Refer to Add custom field values . / Edit custom field values . / Delete custom field values .) Click the Confirm case creation button.","title":"1. Create a new case from EDR template."},{"location":"thehive/organization/cases/create-case-from-template/#2-create-a-new-case-from-phishing-template","text":"Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags .) Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks . / Edit tasks . / Delete tasks .) Add Custom Fields. (Refer to Add custom field values . / Edit custom field values . / Delete custom field values .) Click the Confirm case creation button.","title":"2. Create a new case from Phishing template."},{"location":"thehive/organization/cases/create-case-from-template/#add-tags","text":"Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box Click the Add selected tags button.","title":"Add tags"},{"location":"thehive/organization/cases/create-case-from-template/#add-tasks","text":"The task Group is default. Enter the task Title . Enter the task description in the Description . Select the Due date . Click Save . Click Save and add another , to add another task.","title":"Add tasks"},{"location":"thehive/organization/cases/create-case-from-template/#add-custom-field-values","text":"Select custom field value from the given list. (location/business-unit/detection-source/test). Click Confirm custom field value creation .","title":"Add custom field values"},{"location":"thehive/organization/cases/create-case-from-template/#edit-tasks","text":"Click the edit link. A new window opens. Edit the required values Click the Confirm edition button.","title":"Edit tasks"},{"location":"thehive/organization/cases/create-case-from-template/#edit-custom-field-values","text":"Click the edit link. A new window opens. Edit the required custom field values Click the Confirm custom field value edition button.","title":"Edit custom field values"},{"location":"thehive/organization/cases/create-case-from-template/#delete-tasks","text":"Click the delete link beside the value that has to be deleted.","title":"Delete tasks"},{"location":"thehive/organization/cases/create-case-from-template/#delete-custom-field-values","text":"Click the delete link beside the custom field value that has to be deleted.","title":"Delete custom field values"},{"location":"thehive/organization/cases/create-empty-case/","text":"From an empty case # Create a new case from an empty case. Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags ). Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks ). Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case creation button.","title":"From an empty case"},{"location":"thehive/organization/cases/create-empty-case/#from-an-empty-case","text":"Create a new case from an empty case. Enter the case title in the Title . Select the date from the Date . Select Severity , (Low/Medium/High/Critical). Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Click + to add Tags . (Refer to Add tags ). Enter the case description in the Description . Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all). Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all). Add Tasks. (Refer to Add tasks ). Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case creation button.","title":"From an empty case"},{"location":"thehive/organization/cases-list/about-a-case-list/","text":"Cases # In this section you can find information about cases. A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users. Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats. View case details # To view case details: Click on any of the cases displayed in the list to view more details. The case list page displays various tabs that have more details about each case such general tab, tasks, observables, TTPs, attachments, timeline, pages tab.","title":"Cases"},{"location":"thehive/organization/cases-list/about-a-case-list/#cases","text":"In this section you can find information about cases. A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users. Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats.","title":"Cases"},{"location":"thehive/organization/cases-list/about-a-case-list/#view-case-details","text":"To view case details: Click on any of the cases displayed in the list to view more details. The case list page displays various tabs that have more details about each case such general tab, tasks, observables, TTPs, attachments, timeline, pages tab.","title":"View case details"},{"location":"thehive/organization/cases-list/actions/","text":"Actions # You can make use of any of the available actions. Flag/Unflag # Click the Flag/Unflag option to either flag or unflag a case. A pop-up message appears Close # Click the Close option to remove a case A new window opens. Select Status from the list. Change the Summary Click the Close tasks and case button.","title":"Actions"},{"location":"thehive/organization/cases-list/actions/#actions","text":"You can make use of any of the available actions.","title":"Actions"},{"location":"thehive/organization/cases-list/actions/#flagunflag","text":"Click the Flag/Unflag option to either flag or unflag a case. A pop-up message appears","title":"Flag/Unflag"},{"location":"thehive/organization/cases-list/actions/#close","text":"Click the Close option to remove a case A new window opens. Select Status from the list. Change the Summary Click the Close tasks and case button.","title":"Close"},{"location":"thehive/organization/cases-list/add-custom-event/","text":"Add custom event # In this section you can find information about adding cutom events. To add custom event: After clicking the + option. A new window opens. Add the Title . Enter the Description . Select Date . Select End date . Click the Add button.","title":"Add custom event"},{"location":"thehive/organization/cases-list/add-custom-event/#add-custom-event","text":"In this section you can find information about adding cutom events. To add custom event: After clicking the + option. A new window opens. Add the Title . Enter the Description . Select Date . Select End date . Click the Add button.","title":"Add custom event"},{"location":"thehive/organization/cases-list/add-custom-fields/","text":"Add Custom Fields # In this section you can find information about adding custom fields. To add custom fields: After clicking the Add option beside the custom fields. A new window opens. Add the custom fields from the list. Click the Confirm button.","title":"Add Custom Fields"},{"location":"thehive/organization/cases-list/add-custom-fields/#add-custom-fields","text":"In this section you can find information about adding custom fields. To add custom fields: After clicking the Add option beside the custom fields. A new window opens. Add the custom fields from the list. Click the Confirm button.","title":"Add Custom Fields"},{"location":"thehive/organization/cases-list/attachments/","text":"View attachments # In this section you can find information about Attachments . Add Attachment # To add a new attachment: On the cases list page, on the attachments tab, on the cases sub-tab, Click the + to add a new attachment. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: A user can add one or more files simultaneously. Copy URL # On the cases list page, on the attachments tab, on the Tasks sub-tab, click the ellipsis (...) corresponding to the url to be copied. Click copy URL . Download # On the cases list page, on the attachments tab, on the Tasks sub-tab, Click the ellipsis (...) corresponding to the download option. Click Download .","title":"View attachments"},{"location":"thehive/organization/cases-list/attachments/#view-attachments","text":"In this section you can find information about Attachments .","title":"View attachments"},{"location":"thehive/organization/cases-list/attachments/#add-attachment","text":"To add a new attachment: On the cases list page, on the attachments tab, on the cases sub-tab, Click the + to add a new attachment. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: A user can add one or more files simultaneously.","title":"Add Attachment"},{"location":"thehive/organization/cases-list/attachments/#copy-url","text":"On the cases list page, on the attachments tab, on the Tasks sub-tab, click the ellipsis (...) corresponding to the url to be copied. Click copy URL .","title":"Copy URL"},{"location":"thehive/organization/cases-list/attachments/#download","text":"On the cases list page, on the attachments tab, on the Tasks sub-tab, Click the ellipsis (...) corresponding to the download option. Click Download .","title":"Download"},{"location":"thehive/organization/cases-list/configure-pap-tlp-severity/","text":"Configure Case details # In this section you can find information about configuring case details. Every case has three important elements the TLP, PAP and Severity. TLP defines the confidentiality of information. PAP is the level of exposure of information to the outsde world and Severity implies the severity of information. Configure TLP (Confidentiality of information) # Select TLP , (White/Green/Amber/Red) from the list. Configure PAP (Level of exposure of information) # Select PAP , (White/Green/Amber/Red) from the list. Configure Severity (Severity of information) # Select Severity , (Low/Medium/High/Critical) from the list.","title":"Configure Case details"},{"location":"thehive/organization/cases-list/configure-pap-tlp-severity/#configure-case-details","text":"In this section you can find information about configuring case details. Every case has three important elements the TLP, PAP and Severity. TLP defines the confidentiality of information. PAP is the level of exposure of information to the outsde world and Severity implies the severity of information.","title":"Configure Case details"},{"location":"thehive/organization/cases-list/configure-pap-tlp-severity/#configure-tlp-confidentiality-of-information","text":"Select TLP , (White/Green/Amber/Red) from the list.","title":"Configure TLP (Confidentiality of information)"},{"location":"thehive/organization/cases-list/configure-pap-tlp-severity/#configure-pap-level-of-exposure-of-information","text":"Select PAP , (White/Green/Amber/Red) from the list.","title":"Configure PAP (Level of exposure of information)"},{"location":"thehive/organization/cases-list/configure-pap-tlp-severity/#configure-severity-severity-of-information","text":"Select Severity , (Low/Medium/High/Critical) from the list.","title":"Configure Severity (Severity of information)"},{"location":"thehive/organization/cases-list/general/","text":"General # The information on this page comes from templates and is populated. In the left pane of the window, details are displayed, such as created by, created date, TLP, PAP, and severity details. The status of the alert, start date and task completion details can be seen. In the left pane of the window you can configure the PAP,TLP and Severity of the case. Refer to Configure Case Details for more details. In the right pane, at bottom of the window, type Comments if any for the team. Enter the Title . Add Tags. (Refer to Add tags ). Enter the Description . Add Custom fields . (Refer to Add custom fields ). Enter the Company name . Add business unit, detection source and location details. Click the Confirm button. Click the Comment button.","title":"General"},{"location":"thehive/organization/cases-list/general/#general","text":"The information on this page comes from templates and is populated. In the left pane of the window, details are displayed, such as created by, created date, TLP, PAP, and severity details. The status of the alert, start date and task completion details can be seen. In the left pane of the window you can configure the PAP,TLP and Severity of the case. Refer to Configure Case Details for more details. In the right pane, at bottom of the window, type Comments if any for the team. Enter the Title . Add Tags. (Refer to Add tags ). Enter the Description . Add Custom fields . (Refer to Add custom fields ). Enter the Company name . Add business unit, detection source and location details. Click the Confirm button. Click the Comment button.","title":"General"},{"location":"thehive/organization/cases-list/manage-views/","text":"Manage views # In this section, you can find information about managing views. To manage views: Click the default button. Click on Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete. Manage Cases # There are various option available to apply on the cases. Quick Filters # To apply Quick filter: Click the Quick Filter option. The list displays options to select from. Auto refresh # The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the cases list page, switch on the Auto refresh button. Stats # To view Stats: On the cases list page, switch on the Stats toggle button, the stats will be displayed. Filters # To apply filter: On the page, in the tab, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters. Sorting # Sorting can be performed on any field values. To Sort: On the cases list page, click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Manage views"},{"location":"thehive/organization/cases-list/manage-views/#manage-views","text":"In this section, you can find information about managing views. To manage views: Click the default button. Click on Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage views"},{"location":"thehive/organization/cases-list/manage-views/#manage-cases","text":"There are various option available to apply on the cases.","title":"Manage Cases"},{"location":"thehive/organization/cases-list/manage-views/#quick-filters","text":"To apply Quick filter: Click the Quick Filter option. The list displays options to select from.","title":"Quick Filters"},{"location":"thehive/organization/cases-list/manage-views/#auto-refresh","text":"The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the cases list page, switch on the Auto refresh button.","title":"Auto refresh"},{"location":"thehive/organization/cases-list/manage-views/#stats","text":"To view Stats: On the cases list page, switch on the Stats toggle button, the stats will be displayed.","title":"Stats"},{"location":"thehive/organization/cases-list/manage-views/#filters","text":"To apply filter: On the page, in the tab, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/cases-list/manage-views/#sorting","text":"Sorting can be performed on any field values. To Sort: On the cases list page, click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Sorting"},{"location":"thehive/organization/cases-list/observables/","text":"View observables # In this section you can find information about observables. Observables represent stateful properties (such as the MD5 hash of a file or the value of a registry key) or measurable events (such as the creation of a registry key or the deletion of a file) that are pertinent to the operation of computers and networks. Add observables # Click the + to add an observable. Type the Type . Type the Value . Select TLP , (White/Green/Amber/Red) from the options. Select PAP , (White/Green/Amber/Red) from the options. Switch the on button for Is IOC . (IoC repository contains objects, and each of the objects contain a specific piece of information.) Switch on the button for Has Been Sighted . Switch on the button for Ignore Similarity . Add Tags. (Refer to Add tags ). Type the Description . Click the Save and add another button. Click the Confirm button. Observables Actions # You can make use of any of the available actions. Delete # Click the Delete option to remove an observable. A message pops-up Click the OK button. Run Analyzers # Click the Run Analyzers option. A new window opens. Select one or more Analyzers from the list. Click the Run Analyzers button. Responders # Click the Responders option. Pin/Unpin # Click the Pin/Unpin option to pin or unpin observables. Export # To Export an observable details file: Click the Export option. A file is downloaded, that can be exported/sent. Copy Data # Click the Copy data option .","title":"View observables"},{"location":"thehive/organization/cases-list/observables/#view-observables","text":"In this section you can find information about observables. Observables represent stateful properties (such as the MD5 hash of a file or the value of a registry key) or measurable events (such as the creation of a registry key or the deletion of a file) that are pertinent to the operation of computers and networks.","title":"View observables"},{"location":"thehive/organization/cases-list/observables/#add-observables","text":"Click the + to add an observable. Type the Type . Type the Value . Select TLP , (White/Green/Amber/Red) from the options. Select PAP , (White/Green/Amber/Red) from the options. Switch the on button for Is IOC . (IoC repository contains objects, and each of the objects contain a specific piece of information.) Switch on the button for Has Been Sighted . Switch on the button for Ignore Similarity . Add Tags. (Refer to Add tags ). Type the Description . Click the Save and add another button. Click the Confirm button.","title":"Add observables"},{"location":"thehive/organization/cases-list/observables/#observables-actions","text":"You can make use of any of the available actions.","title":"Observables Actions"},{"location":"thehive/organization/cases-list/observables/#delete","text":"Click the Delete option to remove an observable. A message pops-up Click the OK button.","title":"Delete"},{"location":"thehive/organization/cases-list/observables/#run-analyzers","text":"Click the Run Analyzers option. A new window opens. Select one or more Analyzers from the list. Click the Run Analyzers button.","title":"Run Analyzers"},{"location":"thehive/organization/cases-list/observables/#responders","text":"Click the Responders option.","title":"Responders"},{"location":"thehive/organization/cases-list/observables/#pinunpin","text":"Click the Pin/Unpin option to pin or unpin observables.","title":"Pin/Unpin"},{"location":"thehive/organization/cases-list/observables/#export","text":"To Export an observable details file: Click the Export option. A file is downloaded, that can be exported/sent.","title":"Export"},{"location":"thehive/organization/cases-list/observables/#copy-data","text":"Click the Copy data option .","title":"Copy Data"},{"location":"thehive/organization/cases-list/pages/","text":"View pages # In this section you will find information regarding the lessons learnt. Add/Edit Pages # A user can add a new page. In the right pane of the window, Click + A new window opens. Add a Title . Add a Category . Click the Confirm button. A user can edit pages by clicking on the pencil icon. Make changes and click the save icon. Search a Page by Title # A user can search a page by typing the title in the search box.","title":"View pages"},{"location":"thehive/organization/cases-list/pages/#view-pages","text":"In this section you will find information regarding the lessons learnt.","title":"View pages"},{"location":"thehive/organization/cases-list/pages/#addedit-pages","text":"A user can add a new page. In the right pane of the window, Click + A new window opens. Add a Title . Add a Category . Click the Confirm button. A user can edit pages by clicking on the pencil icon. Make changes and click the save icon.","title":"Add/Edit Pages"},{"location":"thehive/organization/cases-list/pages/#search-a-page-by-title","text":"A user can search a page by typing the title in the search box.","title":"Search a Page by Title"},{"location":"thehive/organization/cases-list/preview-cases/","text":"Preview Cases # In this section you can find information about previewing cases and associated details. To preview the cases details: On the list of case details page, there is a Preview button corresponding to the specific case name. Click the Preview option. The case details preview window opens. You can see details like the id, created by, created at date, updated date, TLP, PAP and severity details, title, status, tags, and description details. In the right pane of the window, there is an option to Add the following: Add Task . Add Observable . Add TTP . At the bottom of the window, there is an option to Add the following: Add custom fields . Add business unit , and location details. Click Add to type business unit, detection-source and location details. Actions # You can click the Actions Button to Flag/unflag, close, or to Run Responders . and to `Run Analyzer . Case Details # Click on the Go to details button. You can view more details of the case. Case details menu # You can view more details of the case by going to the menu. On the top of the page, there are many case options available such as flag/unflag, merge, export, close delete and run responders.","title":"Preview Cases"},{"location":"thehive/organization/cases-list/preview-cases/#preview-cases","text":"In this section you can find information about previewing cases and associated details. To preview the cases details: On the list of case details page, there is a Preview button corresponding to the specific case name. Click the Preview option. The case details preview window opens. You can see details like the id, created by, created at date, updated date, TLP, PAP and severity details, title, status, tags, and description details. In the right pane of the window, there is an option to Add the following: Add Task . Add Observable . Add TTP . At the bottom of the window, there is an option to Add the following: Add custom fields . Add business unit , and location details. Click Add to type business unit, detection-source and location details.","title":"Preview Cases"},{"location":"thehive/organization/cases-list/preview-cases/#actions","text":"You can click the Actions Button to Flag/unflag, close, or to Run Responders . and to `Run Analyzer .","title":"Actions"},{"location":"thehive/organization/cases-list/preview-cases/#case-details","text":"Click on the Go to details button. You can view more details of the case.","title":"Case Details"},{"location":"thehive/organization/cases-list/preview-cases/#case-details-menu","text":"You can view more details of the case by going to the menu. On the top of the page, there are many case options available such as flag/unflag, merge, export, close delete and run responders.","title":"Case details menu"},{"location":"thehive/organization/cases-list/run-analyzer/","text":"Run Analyzers #","title":"Run Analyzers"},{"location":"thehive/organization/cases-list/run-analyzer/#run-analyzers","text":"","title":"Run Analyzers"},{"location":"thehive/organization/cases-list/run-responders/","text":"Run responders # Responders # Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Run responders"},{"location":"thehive/organization/cases-list/run-responders/#run-responders","text":"","title":"Run responders"},{"location":"thehive/organization/cases-list/run-responders/#responders","text":"Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Responders"},{"location":"thehive/organization/cases-list/tasks/","text":"View tasks # Refer to the full section on Tasks for more details. 'About tasks' 'Preview task details' 'Manage Views'","title":"View tasks"},{"location":"thehive/organization/cases-list/tasks/#view-tasks","text":"Refer to the full section on Tasks for more details. 'About tasks' 'Preview task details' 'Manage Views'","title":"View tasks"},{"location":"thehive/organization/cases-list/timeline/","text":"View timeline # In this section you can find information about timelines. A time line is a where a user can view the case details at a glance. The timeline has details about custom events, TTPs, logs, tasks, alerts, case events shown on a timeline. Configure timeline # Add a custom event (refer to - Add a Custom Event ), Export to JSON, Zoom out, zoom in, centre timeline, graph view, list view. There is an option to view the list of custom events. (under the custom events i.e. the last icon, - you can select the custom event to include in the timeline or not).","title":"View timeline"},{"location":"thehive/organization/cases-list/timeline/#view-timeline","text":"In this section you can find information about timelines. A time line is a where a user can view the case details at a glance. The timeline has details about custom events, TTPs, logs, tasks, alerts, case events shown on a timeline.","title":"View timeline"},{"location":"thehive/organization/cases-list/timeline/#configure-timeline","text":"Add a custom event (refer to - Add a Custom Event ), Export to JSON, Zoom out, zoom in, centre timeline, graph view, list view. There is an option to view the list of custom events. (under the custom events i.e. the last icon, - you can select the custom event to include in the timeline or not).","title":"Configure timeline"},{"location":"thehive/organization/cases-list/ttps/","text":"TTPs # In this section you will find information about TTPs. Add TTP # To add TTP: After clicking the + option. A new window opens. Select the Catalogue from the list. Select the Occur date . Click the Add toggle button on to add Procedure . Click the Save and add another or the Confirm button. Delete TTP # Click the ellipsis (...) corresponding to the TTP to be deleted. Click the delete option.","title":"TTPs"},{"location":"thehive/organization/cases-list/ttps/#ttps","text":"In this section you will find information about TTPs.","title":"TTPs"},{"location":"thehive/organization/cases-list/ttps/#add-ttp","text":"To add TTP: After clicking the + option. A new window opens. Select the Catalogue from the list. Select the Occur date . Click the Add toggle button on to add Procedure . Click the Save and add another or the Confirm button.","title":"Add TTP"},{"location":"thehive/organization/cases-list/ttps/#delete-ttp","text":"Click the ellipsis (...) corresponding to the TTP to be deleted. Click the delete option.","title":"Delete TTP"},{"location":"thehive/organization/configure-organization/manage-case-templates/","text":"Manage case templates # About Case templates # A case template auto-populates fields when a new case is being created. A user can create templates to simplify the process of creating tasks and cases by populating fields automatically. A user can add, edit, delete, import and export case templates in addition to managing views and filters. The case templates tab, shows a list of cases and corresponding case details such as display name, name, details, created/updated dates. Each case entry on the list can be edited, exported and deleted. A user can add new case templates and can import case templates. Add a case template # To Add a case template: Click the + , to Add a case template. A new window opens. Enter the case title prefix in the Prefix . Enter the case template Name . Enter the case template Display Name . Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Select Severity , (Low/Medium/High/Critical). Click + to add Tags . (Refer to Add tags ). Enter the case description in the Description . Click + to Add Tasks. (Refer to Add tasks ). Click + to Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case template creation button. Edit a case template # To Edit a case template: Click the Edit option from the list. A new window opens. Edit the required fields. Click the Confirm case template edition button. Delete a case template # To Delete a case template: Click the Delete option from the list. A new message pops-up. Click OK to delete the case template from the list. Import a case template # To Import a case template: Click the Import Case Template option. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: The file must be a valid JSON file. You can use the exported case template directly from theHive platform. Export a case template # To Export a case template: Click the Export Case Template option. A file is downloaded, that can be exported/sent.","title":"Manage case templates"},{"location":"thehive/organization/configure-organization/manage-case-templates/#manage-case-templates","text":"","title":"Manage case templates"},{"location":"thehive/organization/configure-organization/manage-case-templates/#about-case-templates","text":"A case template auto-populates fields when a new case is being created. A user can create templates to simplify the process of creating tasks and cases by populating fields automatically. A user can add, edit, delete, import and export case templates in addition to managing views and filters. The case templates tab, shows a list of cases and corresponding case details such as display name, name, details, created/updated dates. Each case entry on the list can be edited, exported and deleted. A user can add new case templates and can import case templates.","title":"About Case templates"},{"location":"thehive/organization/configure-organization/manage-case-templates/#add-a-case-template","text":"To Add a case template: Click the + , to Add a case template. A new window opens. Enter the case title prefix in the Prefix . Enter the case template Name . Enter the case template Display Name . Select TLP , (White/Green/Amber/Red). Select PAP , (White/Green/Amber/Red). Select Severity , (Low/Medium/High/Critical). Click + to add Tags . (Refer to Add tags ). Enter the case description in the Description . Click + to Add Tasks. (Refer to Add tasks ). Click + to Add Custom Fields. (Refer to Add custom field values ). Click the Confirm case template creation button.","title":"Add a case template"},{"location":"thehive/organization/configure-organization/manage-case-templates/#edit-a-case-template","text":"To Edit a case template: Click the Edit option from the list. A new window opens. Edit the required fields. Click the Confirm case template edition button.","title":"Edit a case template"},{"location":"thehive/organization/configure-organization/manage-case-templates/#delete-a-case-template","text":"To Delete a case template: Click the Delete option from the list. A new message pops-up. Click OK to delete the case template from the list.","title":"Delete a case template"},{"location":"thehive/organization/configure-organization/manage-case-templates/#import-a-case-template","text":"To Import a case template: Click the Import Case Template option. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: The file must be a valid JSON file. You can use the exported case template directly from theHive platform.","title":"Import a case template"},{"location":"thehive/organization/configure-organization/manage-case-templates/#export-a-case-template","text":"To Export a case template: Click the Export Case Template option. A file is downloaded, that can be exported/sent.","title":"Export a case template"},{"location":"thehive/organization/configure-organization/manage-attachments/about-attachments/","text":"About Attachments # In this section you can find information about Attachments . To add a new attachment: On the attachments tab, Click the + to add a new attachment. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: A user can add one or more files simultaneously.","title":"About Attachments"},{"location":"thehive/organization/configure-organization/manage-attachments/about-attachments/#about-attachments","text":"In this section you can find information about Attachments . To add a new attachment: On the attachments tab, Click the + to add a new attachment. A new window opens. Click the Drop file or Click option in attachment. Click the Confirm button. NOTE: A user can add one or more files simultaneously.","title":"About Attachments"},{"location":"thehive/organization/configure-organization/manage-attachments/update-attachments/","text":"Update Attachments # In this section you can find more information about updating Attachments . To copy an attachment: On the attachments tab, Click the ellipsis (...) corresponding to the attachment to view more options. Copy # The copy URL option lets you copy the url of the attachment. Download # The download option lets you download the attachment. Remove # The Remove option lets you remove/delete the attachment.","title":"Update Attachments"},{"location":"thehive/organization/configure-organization/manage-attachments/update-attachments/#update-attachments","text":"In this section you can find more information about updating Attachments . To copy an attachment: On the attachments tab, Click the ellipsis (...) corresponding to the attachment to view more options.","title":"Update Attachments"},{"location":"thehive/organization/configure-organization/manage-attachments/update-attachments/#copy","text":"The copy URL option lets you copy the url of the attachment.","title":"Copy"},{"location":"thehive/organization/configure-organization/manage-attachments/update-attachments/#download","text":"The download option lets you download the attachment.","title":"Download"},{"location":"thehive/organization/configure-organization/manage-attachments/update-attachments/#remove","text":"The Remove option lets you remove/delete the attachment.","title":"Remove"},{"location":"thehive/organization/configure-organization/manage-custom-tags/about_custom_tags/","text":"Custom Tags # In this section you can find information about custom tags. Custom tags or free tags are free text tags associated with TheHive objects. Custom tags are not shared across organisations. Users can define sensitive data in tags without worrying about any data leakage issue. A user can add, edit, delete, and change the colour of a custom tags. A user can use the toggle filter button to apply filters, add filters and clear the filters. By default, the custom tags page displays a list of 30 items that can be navigated using the previous and next buttons. A user can manage views. A user can view all the details about the number of cases, alerts, observables, templates, and details of created By, dates of creation and dates of update.","title":"Custom Tags"},{"location":"thehive/organization/configure-organization/manage-custom-tags/about_custom_tags/#custom-tags","text":"In this section you can find information about custom tags. Custom tags or free tags are free text tags associated with TheHive objects. Custom tags are not shared across organisations. Users can define sensitive data in tags without worrying about any data leakage issue. A user can add, edit, delete, and change the colour of a custom tags. A user can use the toggle filter button to apply filters, add filters and clear the filters. By default, the custom tags page displays a list of 30 items that can be navigated using the previous and next buttons. A user can manage views. A user can view all the details about the number of cases, alerts, observables, templates, and details of created By, dates of creation and dates of update.","title":"Custom Tags"},{"location":"thehive/organization/configure-organization/manage-custom-tags/manage_views/","text":"Manage Views # A user can manage views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage Views"},{"location":"thehive/organization/configure-organization/manage-custom-tags/manage_views/#manage-views","text":"A user can manage views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage Views"},{"location":"thehive/organization/configure-organization/manage-custom-tags/update/","text":"Update Custom Tags # To update custom tags: Edit the Name of the tag. Click the tick mark to save changes. Click the cross mark to discard changes. To update colour of the tag: Edit the Colour of the tag. Select the colour from the pallette. Click the tick mark to save changes. Click the cross mark to discard changes.","title":"Update Custom Tags"},{"location":"thehive/organization/configure-organization/manage-custom-tags/update/#update-custom-tags","text":"To update custom tags: Edit the Name of the tag. Click the tick mark to save changes. Click the cross mark to discard changes. To update colour of the tag: Edit the Colour of the tag. Select the colour from the pallette. Click the tick mark to save changes. Click the cross mark to discard changes.","title":"Update Custom Tags"},{"location":"thehive/organization/configure-organization/manage-endpoints/about-endpoints/","text":"About endpoints # In this section you can find information regarding endpoints. An endpoint is the point of entry in a communication channel when two systems interact with each other. A user can create an endpoint. Supported connectors # Webhook. Mattermost. Slack. Http.","title":"About endpoints"},{"location":"thehive/organization/configure-organization/manage-endpoints/about-endpoints/#about-endpoints","text":"In this section you can find information regarding endpoints. An endpoint is the point of entry in a communication channel when two systems interact with each other. A user can create an endpoint.","title":"About endpoints"},{"location":"thehive/organization/configure-organization/manage-endpoints/about-endpoints/#supported-connectors","text":"Webhook. Mattermost. Slack. Http.","title":"Supported connectors"},{"location":"thehive/organization/configure-organization/manage-endpoints/add_endpoints/","text":"Add endpoints # A user can create an endpoint. To Add an Endpoint: Click the + button or Click the Add a new endpoint link. A new page opens. Select the connector. Click the Save button.","title":"Add endpoints"},{"location":"thehive/organization/configure-organization/manage-endpoints/add_endpoints/#add-endpoints","text":"A user can create an endpoint. To Add an Endpoint: Click the + button or Click the Add a new endpoint link. A new page opens. Select the connector. Click the Save button.","title":"Add endpoints"},{"location":"thehive/organization/configure-organization/manage-notifications/about-notifications/","text":"Notification # In this section you can find information about notifications. A notification is a message, or an alert, or status that is available in the UI. You can create a notification.","title":"Notification"},{"location":"thehive/organization/configure-organization/manage-notifications/about-notifications/#notification","text":"In this section you can find information about notifications. A notification is a message, or an alert, or status that is available in the UI. You can create a notification.","title":"Notification"},{"location":"thehive/organization/configure-organization/manage-notifications/manage_notification/","text":"Manage Notification # Add notification # A user can create a notification. To Add a notification: Enter notfification Name . Send notification to every user in the organization. Select Trigger from the list. (Refer to Trigger events ) Enable notification. Select Notifiers . (Refer to Selecting supported notifiers ) Click the Save button.","title":"Manage Notification"},{"location":"thehive/organization/configure-organization/manage-notifications/manage_notification/#manage-notification","text":"","title":"Manage Notification"},{"location":"thehive/organization/configure-organization/manage-notifications/manage_notification/#add-notification","text":"A user can create a notification. To Add a notification: Enter notfification Name . Send notification to every user in the organization. Select Trigger from the list. (Refer to Trigger events ) Enable notification. Select Notifiers . (Refer to Selecting supported notifiers ) Click the Save button.","title":"Add notification"},{"location":"thehive/organization/configure-organization/manage-notifications/supported-notifiers/","text":"Supported notifiers # Click EmailerToAddr. Enter Subject . Enter the email address of the sender in From . Enter the email address of the receiver in To . Click Add variable to add in the Template field. Click the Save button. Click HttpRequest. Select Endpoint . (Refer to Add endpoints ) Select the HTTP Method . Either enter the URL . Or select the option Use endpoint url as prefix . Click Add variable to add in the Template field. Select option to Log errors . Click the + to add headers. enter a valid header key . enter a valid header value . Under Authentication select the Type . Select the option Proxy . Select the option Do not check certificate Authority . (Not recommended) Select the option Disable hostname verification . Click the Save button. Click MatterMost. Select Endpoint .(Refer to Add endpoints ) Enter the Username . Enter the Channel . Click Add variable to add in the Template field. Click the Save button. Click Slack. Select Endpoint . (Refer to Add endpoints ) Click Add variable to add in the Text Template field. Enter the Username . Enter the Channel . Check the box for Advanced Settings If you need help filling the Advance Settings fields, check the Slack documentation link Enter a Blocks template . Enter an Attachment template . Select As User . Enter an icon emoji . Enter icon URL . Select Link names . Select Markdown . Enter a valid Parse string . Select Unfirl Links . Select Unfirl Media . Click the Save button. Click Kafka. Enter a Topic . Enter type the list of servers in Bootstrap servers .","title":"Supported notifiers"},{"location":"thehive/organization/configure-organization/manage-notifications/supported-notifiers/#supported-notifiers","text":"Click EmailerToAddr. Enter Subject . Enter the email address of the sender in From . Enter the email address of the receiver in To . Click Add variable to add in the Template field. Click the Save button. Click HttpRequest. Select Endpoint . (Refer to Add endpoints ) Select the HTTP Method . Either enter the URL . Or select the option Use endpoint url as prefix . Click Add variable to add in the Template field. Select option to Log errors . Click the + to add headers. enter a valid header key . enter a valid header value . Under Authentication select the Type . Select the option Proxy . Select the option Do not check certificate Authority . (Not recommended) Select the option Disable hostname verification . Click the Save button. Click MatterMost. Select Endpoint .(Refer to Add endpoints ) Enter the Username . Enter the Channel . Click Add variable to add in the Template field. Click the Save button. Click Slack. Select Endpoint . (Refer to Add endpoints ) Click Add variable to add in the Text Template field. Enter the Username . Enter the Channel . Check the box for Advanced Settings If you need help filling the Advance Settings fields, check the Slack documentation link Enter a Blocks template . Enter an Attachment template . Select As User . Enter an icon emoji . Enter icon URL . Select Link names . Select Markdown . Enter a valid Parse string . Select Unfirl Links . Select Unfirl Media . Click the Save button. Click Kafka. Enter a Topic . Enter type the list of servers in Bootstrap servers .","title":"Supported notifiers"},{"location":"thehive/organization/configure-organization/manage-notifications/triggering-events/","text":"Trigger events # Default (AnyCase). Case. CaseClosed CaseShared CaseCreated Alert. AlertCreated AlertImported Observable. ObservableCreated Job. JobFinished Task. TaskAssigned TaskClosed","title":"Trigger events"},{"location":"thehive/organization/configure-organization/manage-notifications/triggering-events/#trigger-events","text":"Default (AnyCase). Case. CaseClosed CaseShared CaseCreated Alert. AlertCreated AlertImported Observable. ObservableCreated Job. JobFinished Task. TaskAssigned TaskClosed","title":"Trigger events"},{"location":"thehive/organization/configure-organization/manage-ui-configuration/about-ui-configuration/","text":"About UI Configuration # In this section you can find information about UI Configuration . To configure the UI: Select the option Hide Empty Case Button to disallow creating empty cases. Select the option Merge alerts into closed cases to disallow merging alerts into closed cases. Select the option Disallow refresh option in dashboards to disallow the refresh option in dashboards Select the default filter of alert case similarity panel from the list. Define the default date format used to display dates. e.g. YYYY-MM-DD HH:mm Click the Save button. NOTE: Default date format can be configured at the Organisation level, by defining the preferred format in the UI Configuration view. This requires a user with org-admin profile or any profile with manageConfig permission.","title":"About UI Configuration"},{"location":"thehive/organization/configure-organization/manage-ui-configuration/about-ui-configuration/#about-ui-configuration","text":"In this section you can find information about UI Configuration . To configure the UI: Select the option Hide Empty Case Button to disallow creating empty cases. Select the option Merge alerts into closed cases to disallow merging alerts into closed cases. Select the option Disallow refresh option in dashboards to disallow the refresh option in dashboards Select the default filter of alert case similarity panel from the list. Define the default date format used to display dates. e.g. YYYY-MM-DD HH:mm Click the Save button. NOTE: Default date format can be configured at the Organisation level, by defining the preferred format in the UI Configuration view. This requires a user with org-admin profile or any profile with manageConfig permission.","title":"About UI Configuration"},{"location":"thehive/organization/configure-organization/manage-users/add-delete-user/","text":"Users (Add/Delete) # After you login to TheHive application, you can see the organization icon on the page. Click the Organization icon. On the first tab, you can find information about Users. You can add and delete user details. Add User # To Add the user details: Click the + to Add user. A new window opens. Enter Login credentials. Enter the Name . Select a Profile from the list. Click the Save and add another button to add more users. Click the Save button to add a user. Delete User # If a user has left the organization in such cases a user profile may be deleted. To Delete the user details: Click the ellipsis (...) corresponding to the required user. Click the Delete option. The following message pops up: Click the OK button.","title":"Users (Add/Delete)"},{"location":"thehive/organization/configure-organization/manage-users/add-delete-user/#users-adddelete","text":"After you login to TheHive application, you can see the organization icon on the page. Click the Organization icon. On the first tab, you can find information about Users. You can add and delete user details.","title":"Users (Add/Delete)"},{"location":"thehive/organization/configure-organization/manage-users/add-delete-user/#add-user","text":"To Add the user details: Click the + to Add user. A new window opens. Enter Login credentials. Enter the Name . Select a Profile from the list. Click the Save and add another button to add more users. Click the Save button to add a user.","title":"Add User"},{"location":"thehive/organization/configure-organization/manage-users/add-delete-user/#delete-user","text":"If a user has left the organization in such cases a user profile may be deleted. To Delete the user details: Click the ellipsis (...) corresponding to the required user. Click the Delete option. The following message pops up: Click the OK button.","title":"Delete User"},{"location":"thehive/organization/configure-organization/manage-users/lock/","text":"Lock User # If a user is on contract in the organization and the contract ends, in such cases a user profile may be locked. To lock the user details: Click the ellipsis (...) corresponding to the user details that you want to lock. Click the Lock option. A new pop up message opens. Click the OK button.","title":"Lock User"},{"location":"thehive/organization/configure-organization/manage-users/lock/#lock-user","text":"If a user is on contract in the organization and the contract ends, in such cases a user profile may be locked. To lock the user details: Click the ellipsis (...) corresponding to the user details that you want to lock. Click the Lock option. A new pop up message opens. Click the OK button.","title":"Lock User"},{"location":"thehive/organization/configure-organization/manage-users/manage-views/","text":"Manage Views # In this section, you can find information about managing views. To manage views: Click the default button. Click Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view. Select the action. e.g. Delete","title":"Manage Views"},{"location":"thehive/organization/configure-organization/manage-users/manage-views/#manage-views","text":"In this section, you can find information about managing views. To manage views: Click the default button. Click Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view. Select the action. e.g. Delete","title":"Manage Views"},{"location":"thehive/organization/configure-organization/manage-users/preview/","text":"Preview # You can manage previews to see the details of a specific user. To preview the user details: On the list of users page, there is a Preview button corresponding to the specific user name. Click the Preview option. The user details preview window opens. You can see details like the id, created by, created at date, updated date, name, login id, email, logo, MFA, API Key, and locked status of the user.","title":"Preview"},{"location":"thehive/organization/configure-organization/manage-users/preview/#preview","text":"You can manage previews to see the details of a specific user. To preview the user details: On the list of users page, there is a Preview button corresponding to the specific user name. Click the Preview option. The user details preview window opens. You can see details like the id, created by, created at date, updated date, name, login id, email, logo, MFA, API Key, and locked status of the user.","title":"Preview"},{"location":"thehive/organization/configure-organization/manage-users/sort/","text":"Sort # If a user wants to re-arranged any field, it can be done using the sort option. To sort on a field: Click the small arrows beside the field to be sorted.","title":"Sort"},{"location":"thehive/organization/configure-organization/manage-users/sort/#sort","text":"If a user wants to re-arranged any field, it can be done using the sort option. To sort on a field: Click the small arrows beside the field to be sorted.","title":"Sort"},{"location":"thehive/organization/dashboard/about-dashboard/","text":"Dashboards # In this section you can find information about dashboards. A dashboard is a visual display of data to provide information at-a-glance. The dashboard is configurable, allowing you to choose which data you want to see and whether you want to include charts or graphs to visualize the numbers. The dashboard lists all details of cases such as, status, name, version number, widget, created by, dates of creation and date on which data was updated. A user can apply filters on the dashboard, sort based on fields, and manage views.","title":"Dashboards"},{"location":"thehive/organization/dashboard/about-dashboard/#dashboards","text":"In this section you can find information about dashboards. A dashboard is a visual display of data to provide information at-a-glance. The dashboard is configurable, allowing you to choose which data you want to see and whether you want to include charts or graphs to visualize the numbers. The dashboard lists all details of cases such as, status, name, version number, widget, created by, dates of creation and date on which data was updated. A user can apply filters on the dashboard, sort based on fields, and manage views.","title":"Dashboards"},{"location":"thehive/organization/dashboard/filter-sort/","text":"Filters and Sort # In this section, you can find information about applying filters. Filters # To apply filter: On the dashboard page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters. Sorting # Sorting can be performed on any field values. To Sort: On the dashboard page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Filters and Sort"},{"location":"thehive/organization/dashboard/filter-sort/#filters-and-sort","text":"In this section, you can find information about applying filters.","title":"Filters and Sort"},{"location":"thehive/organization/dashboard/filter-sort/#filters","text":"To apply filter: On the dashboard page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/dashboard/filter-sort/#sorting","text":"Sorting can be performed on any field values. To Sort: On the dashboard page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Sorting"},{"location":"thehive/organization/dashboard/manage-dashboard/","text":"Manage Dashboard # In this section you can find information about managing dashboards. Add a dashboard # To Add a dashboard: Click the + , to Add a dashboard. A new window opens. Enter the Title . Enter the Description. Select the Visibility option. (Private or Shared) Click the confirm button. Edit a dashboard # To Edit a dashboard: Click the Edit option from the list. A new window opens. Edit the required fields. Click the Confirm button. Delete a dashboard # To Delete a dashboard: Click the Delete option from the list. A new message pops-up. Click OK to delete the dashboard from the list. Import a dashboard # To Import a dashboard: Click the Import dashboard option. A new window opens. Click the Drop file or Click option in attachment. NOTE: The file must be a valid JSON file. You can use the exported dashboard directly from theHive platform. Export a dashboard # To Export a dashboard: Click the Export option. A file is downloaded, that can be exported/sent.","title":"Manage Dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#manage-dashboard","text":"In this section you can find information about managing dashboards.","title":"Manage Dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#add-a-dashboard","text":"To Add a dashboard: Click the + , to Add a dashboard. A new window opens. Enter the Title . Enter the Description. Select the Visibility option. (Private or Shared) Click the confirm button.","title":"Add a dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#edit-a-dashboard","text":"To Edit a dashboard: Click the Edit option from the list. A new window opens. Edit the required fields. Click the Confirm button.","title":"Edit a dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#delete-a-dashboard","text":"To Delete a dashboard: Click the Delete option from the list. A new message pops-up. Click OK to delete the dashboard from the list.","title":"Delete a dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#import-a-dashboard","text":"To Import a dashboard: Click the Import dashboard option. A new window opens. Click the Drop file or Click option in attachment. NOTE: The file must be a valid JSON file. You can use the exported dashboard directly from theHive platform.","title":"Import a dashboard"},{"location":"thehive/organization/dashboard/manage-dashboard/#export-a-dashboard","text":"To Export a dashboard: Click the Export option. A file is downloaded, that can be exported/sent.","title":"Export a dashboard"},{"location":"thehive/organization/dashboard/manage-views/","text":"Manage Views # In this section, you can find information about managing dashboard views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage Views"},{"location":"thehive/organization/dashboard/manage-views/#manage-views","text":"In this section, you can find information about managing dashboard views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage Views"},{"location":"thehive/organization/search/about-search/","text":"Search # In this section you can find information about the Search function. The Search function allows you to refine search by selecting a particular search criteria. Often, the search criteria are broad, resulting in large number of results. The search function allow users to trim down their list of search results by applying filters on one or more search results. You can search based on the following search criteria: Cases. Tasks. Task Logs. Observables. Alerts. Jobs. Audit Logs. Based on your search criteria, a set of search results appear in the right pane of the page. A maximum of 300 results can be displayed on the page that can be navigated using the Previous and Next buttons at the bottom of the page.","title":"Search"},{"location":"thehive/organization/search/about-search/#search","text":"In this section you can find information about the Search function. The Search function allows you to refine search by selecting a particular search criteria. Often, the search criteria are broad, resulting in large number of results. The search function allow users to trim down their list of search results by applying filters on one or more search results. You can search based on the following search criteria: Cases. Tasks. Task Logs. Observables. Alerts. Jobs. Audit Logs. Based on your search criteria, a set of search results appear in the right pane of the page. A maximum of 300 results can be displayed on the page that can be navigated using the Previous and Next buttons at the bottom of the page.","title":"Search"},{"location":"thehive/organization/search/search-scope-by/alerts/","text":"Alerts # To search by Alerts: if a user wants to search based on Alerts In the left pane, Click on Alerts . A user can specify search criterias, by using Add new filters . A user can user clear all to remove any of the applied filters. Click on Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Alerts"},{"location":"thehive/organization/search/search-scope-by/alerts/#alerts","text":"To search by Alerts: if a user wants to search based on Alerts In the left pane, Click on Alerts . A user can specify search criterias, by using Add new filters . A user can user clear all to remove any of the applied filters. Click on Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Alerts"},{"location":"thehive/organization/search/search-scope-by/auditlogs/","text":"Audit logs # To search by Audit Logs: if you want to search based on Audit Logs In the left pane, Click on Audit Logs . You can specify search criterias, by using Add new filters . A user can user clear all to remove any of the applied filters. Click on Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Audit logs"},{"location":"thehive/organization/search/search-scope-by/auditlogs/#audit-logs","text":"To search by Audit Logs: if you want to search based on Audit Logs In the left pane, Click on Audit Logs . You can specify search criterias, by using Add new filters . A user can user clear all to remove any of the applied filters. Click on Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Audit logs"},{"location":"thehive/organization/search/search-scope-by/cases/","text":"Cases # To search by Cases: if you want to search based on cases In the left pane, Click on Cases . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. (e.g. _createdBy, is not empty etc.) Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Cases"},{"location":"thehive/organization/search/search-scope-by/cases/#cases","text":"To search by Cases: if you want to search based on cases In the left pane, Click on Cases . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. (e.g. _createdBy, is not empty etc.) Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Cases"},{"location":"thehive/organization/search/search-scope-by/logs/","text":"Logs # To search by Logs: if you want to search based on Logs In the left pane, Click on Logs . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Logs"},{"location":"thehive/organization/search/search-scope-by/logs/#logs","text":"To search by Logs: if you want to search based on Logs In the left pane, Click on Logs . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Logs"},{"location":"thehive/organization/search/search-scope-by/observables/","text":"Observables # To search by Observables: if you want to search based on Observables In the left pane, Click on Observables . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Observables"},{"location":"thehive/organization/search/search-scope-by/observables/#observables","text":"To search by Observables: if you want to search based on Observables In the left pane, Click on Observables . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Observables"},{"location":"thehive/organization/search/search-scope-by/tasklogs/","text":"Tasks logs # To search by Task logs: if you want to search based on Tasks Logs In the left pane, Click on Tasks Logs . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Tasks logs"},{"location":"thehive/organization/search/search-scope-by/tasklogs/#tasks-logs","text":"To search by Task logs: if you want to search based on Tasks Logs In the left pane, Click on Tasks Logs . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Tasks logs"},{"location":"thehive/organization/search/search-scope-by/tasks/","text":"Tasks # To search by Tasks: if you want to search based on Tasks In the left pane, Click on Tasks . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Tasks"},{"location":"thehive/organization/search/search-scope-by/tasks/#tasks","text":"To search by Tasks: if you want to search based on Tasks In the left pane, Click on Tasks . You can specify search criterias, by using Add new filters . You can use clear all to remove any of the applied filters. Click Add new filters . Select the required filters from the list. Click the Search button at the bottom. A set of search results appear in the right pane of the page.","title":"Tasks"},{"location":"thehive/organization/task/","text":"Preview task details","title":"Index"},{"location":"thehive/organization/task/about-tasks/","text":"Tasks # Task details, require action from its users. Task details page displays a list of tasks. When navigating through the Task list, you can easily see and determine which Task needs an action. To view task details # You can click on any of the tasks in the list to view more details. The details are displayed In the left pane of the window you can configure the PAP, TLP and Severity. Refer to 'Configure Alert Details' for more details.","title":"Tasks"},{"location":"thehive/organization/task/about-tasks/#tasks","text":"Task details, require action from its users. Task details page displays a list of tasks. When navigating through the Task list, you can easily see and determine which Task needs an action.","title":"Tasks"},{"location":"thehive/organization/task/about-tasks/#to-view-task-details","text":"You can click on any of the tasks in the list to view more details. The details are displayed In the left pane of the window you can configure the PAP, TLP and Severity. Refer to 'Configure Alert Details' for more details.","title":"To view task details"},{"location":"thehive/organization/task/manage-views/","text":"Manage Views # In this section, you can find information about managing views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete. Manage Tasks # There are various option available to apply on the tasks. Quick Filters # To apply Quick filter: Click the Quick Filter option. The list displays options to select from. Auto refresh # The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the tasks list page, switch on the Auto refresh button. Show tasks as groups # To view tasks as groups: On the tasks list page, switch on the tasks as groups button, the task groups will be displayed. Filters # To apply filter: On the tasks list page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters. Sorting # Sorting can be performed on any field values. To Sort: On the tasks list page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Manage Views"},{"location":"thehive/organization/task/manage-views/#manage-views","text":"In this section, you can find information about managing views. To manage views: Click the default button. Click the Manage Views from the list. A new page opens. It has the Name of the view and the corresponding Actions . Click the ellipsis (...) corresponding to the name of the view that you want to delete. Click Delete.","title":"Manage Views"},{"location":"thehive/organization/task/manage-views/#manage-tasks","text":"There are various option available to apply on the tasks.","title":"Manage Tasks"},{"location":"thehive/organization/task/manage-views/#quick-filters","text":"To apply Quick filter: Click the Quick Filter option. The list displays options to select from.","title":"Quick Filters"},{"location":"thehive/organization/task/manage-views/#auto-refresh","text":"The auto-refresh option allows you to automatically refresh a page. To perform Auto refresh: On the tasks list page, switch on the Auto refresh button.","title":"Auto refresh"},{"location":"thehive/organization/task/manage-views/#show-tasks-as-groups","text":"To view tasks as groups: On the tasks list page, switch on the tasks as groups button, the task groups will be displayed.","title":"Show tasks as groups"},{"location":"thehive/organization/task/manage-views/#filters","text":"To apply filter: On the tasks list page, switch on the Filters toggle button. Click Add filters . Apply Filter to the required field. Select the filters from the list. Click Apply filters . (Optional) Click Clear filters to clear all applied filters.","title":"Filters"},{"location":"thehive/organization/task/manage-views/#sorting","text":"Sorting can be performed on any field values. To Sort: On the tasks list page, Click the small arrow that points upwards/downwards to sort on a particular filed name.","title":"Sorting"},{"location":"thehive/organization/task/preview-task-details/Preview-tasks/","text":"Preview Tasks # In this section you can find information about previewing tasks. To preview the task details: On the list of tasks page, there is a Preview button corresponding to the specific task name. Click the Preview option. The task details preview window opens. You can see details like the id, created by, created at date, updated date, title, flag, status, group, assignee, start date, due date, description, activity, responder reports of the task. You can click the Actions Button to start, delete, pin/unpin, flag/unflag the tasks or to Run Responders . You can add activities/task logs by clicking on the + . Refer to Create a task log . Task Details # Click the Go to details button to view more details of the task. Task details menu # Click the Go to details button to view more details of the task. On the top of the page, there are many task options available such as flag, merge, export, close, delete, responders.","title":"Preview Tasks"},{"location":"thehive/organization/task/preview-task-details/Preview-tasks/#preview-tasks","text":"In this section you can find information about previewing tasks. To preview the task details: On the list of tasks page, there is a Preview button corresponding to the specific task name. Click the Preview option. The task details preview window opens. You can see details like the id, created by, created at date, updated date, title, flag, status, group, assignee, start date, due date, description, activity, responder reports of the task. You can click the Actions Button to start, delete, pin/unpin, flag/unflag the tasks or to Run Responders . You can add activities/task logs by clicking on the + . Refer to Create a task log .","title":"Preview Tasks"},{"location":"thehive/organization/task/preview-task-details/Preview-tasks/#task-details","text":"Click the Go to details button to view more details of the task.","title":"Task Details"},{"location":"thehive/organization/task/preview-task-details/Preview-tasks/#task-details-menu","text":"Click the Go to details button to view more details of the task. On the top of the page, there are many task options available such as flag, merge, export, close, delete, responders.","title":"Task details menu"},{"location":"thehive/organization/task/preview-task-details/actions/","text":"Actions # You can make use of any of the available actions. Start # Click the Start option to begin a task. Delete # Click the Delete option to remove a task. A message pops-up Click the OK button. Unflag # Click the Flag/Unflag option to flag or unflag a task. Unpin # Click the Pin/Unpin option to pin or unpin a task.","title":"Actions"},{"location":"thehive/organization/task/preview-task-details/actions/#actions","text":"You can make use of any of the available actions.","title":"Actions"},{"location":"thehive/organization/task/preview-task-details/actions/#start","text":"Click the Start option to begin a task.","title":"Start"},{"location":"thehive/organization/task/preview-task-details/actions/#delete","text":"Click the Delete option to remove a task. A message pops-up Click the OK button.","title":"Delete"},{"location":"thehive/organization/task/preview-task-details/actions/#unflag","text":"Click the Flag/Unflag option to flag or unflag a task.","title":"Unflag"},{"location":"thehive/organization/task/preview-task-details/actions/#unpin","text":"Click the Pin/Unpin option to pin or unpin a task.","title":"Unpin"},{"location":"thehive/organization/task/preview-task-details/create-a-task-log/","text":"Create a task log # In this section you can find information about creating a task log. To create a task log: Enter the log message in Description area of the given box. Switch on the Toggle the button to include in timeline . (If you opt for a timeline, you have to provide the date for the timeline). In Attachment Drop file or click. You can upload an attachment. Click the Create a Task Log button.","title":"Create a task log"},{"location":"thehive/organization/task/preview-task-details/create-a-task-log/#create-a-task-log","text":"In this section you can find information about creating a task log. To create a task log: Enter the log message in Description area of the given box. Switch on the Toggle the button to include in timeline . (If you opt for a timeline, you have to provide the date for the timeline). In Attachment Drop file or click. You can upload an attachment. Click the Create a Task Log button.","title":"Create a task log"},{"location":"thehive/organization/task/preview-task-details/run-responders/","text":"Run Responders # Responders # Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Run Responders"},{"location":"thehive/organization/task/preview-task-details/run-responders/#run-responders","text":"","title":"Run Responders"},{"location":"thehive/organization/task/preview-task-details/run-responders/#responders","text":"Click on responders option. A new window appears. Search for a specific responder in the search box.","title":"Responders"},{"location":"thehive/release-notes/release-notes-5.0/","text":"Release Notes of 5.0 series # 5.0.12 - 16th August 2022 # Fixes # UI: Fix issue where case assignee was not displayed (because user was in an other organisation) Fix org switch not working when using http context (bug introduced in 5.0.11) API: Don't fail integrityChecks if field _createdAt is missing Sync cortex jobs with status Deleted Don't fail queries if a phamtom vertex is encountered Migration: Fix issue where users could be duplicated by the migration 3->5 process Improvements # UI: Add field to set the expiration token duration for reset password links In Proxy forms, add input field to set the port of the proxy API: Alert can be created even if they contain duplicated observables of type files. Before it triggered an \"AlreadyExits\" error. Remove orphan cortex jobs (when observable was deleted) 5.0.11 - 5th August 2022 # Fixes # UI: Fix import of .thar files When switching of organisation, the user is redirected to her homepage. Filters on number types showed -1 in the preview In endpoints configuration, the setting \"Do not check Certificate Authority\" was not sent correctly Fix filters when using between dates with an hour or minute precision API: Cases created from an alert with a case template could have duplicated custom fields Cases created from an alert with a case template had duplicated tasks and prefix (from 5.0.10) Prevent failures during migration from v4 to v5: TheHive will automatically reindex its data when a change in the index is detected (change from lucene to elasticsearch) TheHive will no longer try to run migrations when the setting db.janusgraph.index.search.elasticsearch.bulk-refresh = false is present Fix issue where uploaded files like observable attachments were deleted from the server before being processed, resulting in \"File Not Found\" errors Improvements # UI: Improve the search experience when merging a case Ctrl+click on a case title in the list view will open the case in a new tab API: TheHive will try to fix ghost vertices when encountering one (a vertex that is present in the index but not in the database) a new field extendedStatus is passed to Cortex responders which represents the status of the Case which can be customized. The field status is still a fixed enumeration. Add integrity checks for Share s and Role s entities Some cortex jobs could be stuck in \"Waiting\" status: now when TheHive starts it will try to fix those jobs. 5.0.10 - 21st July 2022 # Fixes # UI: Dashboard: fix style issue when loading the knowledge base at the same time update api used by bar widget to fix issues with custom fields query Markdown preview in full screen mode now uses the full height of the screen Fix a filter when using enumeration (severity, pap) MISP: add settings in the UI to be able to export tags on Case and Observable from TheHive to MISP Cortex: UI is notified when an analyzer has finished adding all the observables to a report Fix report view for Spamhaus analyzer API: Fix issue that caused the user system@thehive.local to be deleted by the user integrity check: this could cause issues with MISP or Cortex synchronization (Cortex jobs triggered by notifiers were left in \"Waiting\" state). If the user was missing from your instance, it will be recreated after an update of TheHive. Increase the timeout for count requests to 10s and add a config to increase this value ( db.limitedCountTimeout ). If a count takes longer that this duration, a truncated result is returned. Dashboard queries: Filter on custom fields was not applied on the name of the custom field (only on the value) Fix an issue that prevented TheHive from starting when misp or cortex were not configured correctly (missing http scheme in url for instance) Docker: Fix issue with argument --no-cql-wait that caused the next argument to be ignored Improvements # UI: Responder jobs list are now sorted by start date Users in Assignee field are now sorted by alphabetical order When showing an observable, prevent from loading all the reports details when loading the job list. Report details are now loaded only when opening the report drawer. API: Add new parameters to the endpoint \"create a case from an alert\". When creating a case from an alert, the events CaseCreated and AlertImported are triggered once all the alert observables are imported inside the case. When merging alerts inside a case, triggger the event AlertImported once all observables for an alert are imported inside the case. Emailer will now send html emails instead of text emails. This means that you use advanced formatting and styles in your emails. Cortex job reports are not included in the list anymore. An extra data needs to be used to retrieve them. Docker: Misp and Cortex modules are enabled by default when using the entrypoint. (They are not enabled if you use --no-config ) 5.0.9 - 1st July 2022 # Fixes # UI: Dashboard: Fix radar when aggregating on custom fields Radar widget now uses a more efficient query Clicks on donut and counter widgets correctly sets the filters on the search page Reset scroll when page is changed In alert observable list, remove the share column Fix french translation in quick views Fix links in search results for observables API: When merging an alert in a case, merge the tag reports for the observable Improvements # UI: In analyzer reports, add the tags on the observables Sort case template by name when creating a case Add support for cortex entities in charts and dashboard Related case view: sort cases by matching percentage API: Limit the duration of count queries not using the index to 5 seconds Add support for cortex entities for charts api Add TheHive version in logs 5.0.8 - 15 June 2022 # Security # Fix issue related to AD/LDAP module. If you are using the ad/ldap authentification, you should update to TheHive 5.0.8 or later Fixes # UI: Fixes issues around importing observables from an analyzer report Analyzer template list was not be exhaustive in certain cases Fix pagination and sorting when listing similar cases In related cases list, show several matching observables instead of only one API: Removed full text search on tags: that caused slow queries as the index cannot be used here Fixed regression from TH4: when merging alerts into a case, observables could be duplicated if they appeared in several alerts Add ability to filter and sort by case/alert status/stage on api v0 Docker: Fix entrypoint for s3 configuration Improvements # API: Log login success and failures: those logs are useful for auditing purpose, to detect password guessing attacks via large unsuccessful logon attempts Prevent duplication of custom fields values Improve queries when filtering on a custom field with a high number of matches 5.0.7 - 31 May 2022 # Fixes # UI: When creating a case from an alert, the correct case template is selected if the field was set in the alert Dashboard: fix an issue when converting a v4 dashboard Case count was not refreshed when adding a case Refresh comment section Misp configuration: Fix organisation selection Analyzer reports: in an alert, display the extractable observables can now import an observable of type file Custom Fields: Limit the size of custom fields in list views use the display name in list views When closing a case, custom fields are no longer deleted API: Fix breaking change in api V0: don't limit the size of observable data in json. This prevented the creation of files in observables. Note: with v1 the prefered way is to use a multipart request. Migration 3 to 5 Fix migration of custom fields of type number Improvements # UI: Add ability to manually refresh a list when auto-refresh is disabled Notifications: Add a json validator when creating a custom filter Prevent automatic scroll when an entity is updated Fix flickering of updated data fields when updating an entity Other UI improvements API: Check for duplicated files (by filename) when attaching a file to a case or to a log Add field stage to alert and case in api v0 Users can manage their own api key without the permission manageUsers Add new auth mecanism based on htpasswd file 5.0.6 - 17 May 2022 # Fixes # UI: List of analyzers did not refresh correctly for alert observables Fix error when trying to download a task log attachment with the char { in the name logout popin remained open after a reconnection API docs did not appear when setting an http context Fix an issue where breadcrumps were not displayed correctly Improvements # UI: Faster render of big markdown section: use marked-react library instead of react-markdown Adjust fanged message Improve sorting of tasks when in group mode Update observable count when an observable is added or removed Improvements for \"Required Action\" on tasks Migration 4 to 5: Improve migration for custom fields where previous script could overload the application 5.0.5 - 5 May 2022 # Fixes # UI: Analyzer templates: optimize rendering time Fixed SSO login when using an http context API: Dashboard: increase number of retrieved values for aggregations (eg. chart on custom field values) Fix for permission manageConfig Improve support for AWS Keyspace: add retries on some failed queries Fix endpoint for deletion of catalog of ttp 5.0.4 - 3 May 2022 # Fixes # UI: Tasks are now displayed by their order Changed color of field for search by case id Fixed an issue where custom fields were deleted when editing a case template Fix download link of attachments when http context is set Update of vulnerable libraries When using bulk edit, \"Add tags\" and \"Remove tags\" now work API: Fixed bug introduced in 5.0.3 where TTPs were not linked to their tactics and reported <unknown> It's now possible to delete the title prefix in a case template 5.0.3 - 15 April 2022 # New Features # API: Add support to procedures (TTPs) when creating alerts in TheHive Fixes # UI: When importing a case from misp: additional parameters (custom fields, shares) are correctly sent When converting an alert to case, custom fields are no longer lost during the process Update moment.js library Migration tool: Fix typo in migration tool configuration 5.0.2 - 8 April 2022 # We found a bug that prevents a user from using the reset pasword flow (present in 5.0.0). We recommend all 5.0.x users to update to this version. Fixes: Fix 404 page during the reset password flow Dashboard: include end date in time interval Fix detached live feed when using an http context 5.0.1 - 7 April 2022 # TheHive 5.0.1 is the first patch release in the 5.0 series. It contains fixes and improvements for the UI and some small changes for the API compared to 5.0.0. We recommend all 5.0.0 users to update to this version. Notables changes # UI: fix description display on search page display org admin tabs only with required permissions fix permissions checks in the application improve case sharing user experience notifications - fix urls of http endpoints notifications - improve editor when using template variables be able to download a file attachment for an alert forms and drawers don't lose user data when the entity is refreshed by the feed improve live feed for cortex jobs on observables API: add ability to create alert with observables (of type string and files), see. API docs for more information rename field user to assignee in case creation rename field customFieldValues to customFields in alert creation Backend: fix tag edition fix permissions check for observables in case of sharing fix user deletion (user could be left without org) fix license reload on cluster nodes add more check to ensure uniqueness of data increase quotas for service users (set to unlimited) and cluster nodes (unlimited for platinum plan) update of dependencies Docs: fix url to website add documentation for MISP endpoints","title":"Release Notes for TheHive 5.0"},{"location":"thehive/release-notes/release-notes-5.0/#release-notes-of-50-series","text":"","title":"Release Notes of 5.0 series"},{"location":"thehive/release-notes/release-notes-5.0/#5012-16th-august-2022","text":"","title":"5.0.12 - 16th August 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes","text":"UI: Fix issue where case assignee was not displayed (because user was in an other organisation) Fix org switch not working when using http context (bug introduced in 5.0.11) API: Don't fail integrityChecks if field _createdAt is missing Sync cortex jobs with status Deleted Don't fail queries if a phamtom vertex is encountered Migration: Fix issue where users could be duplicated by the migration 3->5 process","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements","text":"UI: Add field to set the expiration token duration for reset password links In Proxy forms, add input field to set the port of the proxy API: Alert can be created even if they contain duplicated observables of type files. Before it triggered an \"AlreadyExits\" error. Remove orphan cortex jobs (when observable was deleted)","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#5011-5th-august-2022","text":"","title":"5.0.11 - 5th August 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_1","text":"UI: Fix import of .thar files When switching of organisation, the user is redirected to her homepage. Filters on number types showed -1 in the preview In endpoints configuration, the setting \"Do not check Certificate Authority\" was not sent correctly Fix filters when using between dates with an hour or minute precision API: Cases created from an alert with a case template could have duplicated custom fields Cases created from an alert with a case template had duplicated tasks and prefix (from 5.0.10) Prevent failures during migration from v4 to v5: TheHive will automatically reindex its data when a change in the index is detected (change from lucene to elasticsearch) TheHive will no longer try to run migrations when the setting db.janusgraph.index.search.elasticsearch.bulk-refresh = false is present Fix issue where uploaded files like observable attachments were deleted from the server before being processed, resulting in \"File Not Found\" errors","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_1","text":"UI: Improve the search experience when merging a case Ctrl+click on a case title in the list view will open the case in a new tab API: TheHive will try to fix ghost vertices when encountering one (a vertex that is present in the index but not in the database) a new field extendedStatus is passed to Cortex responders which represents the status of the Case which can be customized. The field status is still a fixed enumeration. Add integrity checks for Share s and Role s entities Some cortex jobs could be stuck in \"Waiting\" status: now when TheHive starts it will try to fix those jobs.","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#5010-21st-july-2022","text":"","title":"5.0.10 - 21st July 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_2","text":"UI: Dashboard: fix style issue when loading the knowledge base at the same time update api used by bar widget to fix issues with custom fields query Markdown preview in full screen mode now uses the full height of the screen Fix a filter when using enumeration (severity, pap) MISP: add settings in the UI to be able to export tags on Case and Observable from TheHive to MISP Cortex: UI is notified when an analyzer has finished adding all the observables to a report Fix report view for Spamhaus analyzer API: Fix issue that caused the user system@thehive.local to be deleted by the user integrity check: this could cause issues with MISP or Cortex synchronization (Cortex jobs triggered by notifiers were left in \"Waiting\" state). If the user was missing from your instance, it will be recreated after an update of TheHive. Increase the timeout for count requests to 10s and add a config to increase this value ( db.limitedCountTimeout ). If a count takes longer that this duration, a truncated result is returned. Dashboard queries: Filter on custom fields was not applied on the name of the custom field (only on the value) Fix an issue that prevented TheHive from starting when misp or cortex were not configured correctly (missing http scheme in url for instance) Docker: Fix issue with argument --no-cql-wait that caused the next argument to be ignored","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_2","text":"UI: Responder jobs list are now sorted by start date Users in Assignee field are now sorted by alphabetical order When showing an observable, prevent from loading all the reports details when loading the job list. Report details are now loaded only when opening the report drawer. API: Add new parameters to the endpoint \"create a case from an alert\". When creating a case from an alert, the events CaseCreated and AlertImported are triggered once all the alert observables are imported inside the case. When merging alerts inside a case, triggger the event AlertImported once all observables for an alert are imported inside the case. Emailer will now send html emails instead of text emails. This means that you use advanced formatting and styles in your emails. Cortex job reports are not included in the list anymore. An extra data needs to be used to retrieve them. Docker: Misp and Cortex modules are enabled by default when using the entrypoint. (They are not enabled if you use --no-config )","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#509-1st-july-2022","text":"","title":"5.0.9 - 1st July 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_3","text":"UI: Dashboard: Fix radar when aggregating on custom fields Radar widget now uses a more efficient query Clicks on donut and counter widgets correctly sets the filters on the search page Reset scroll when page is changed In alert observable list, remove the share column Fix french translation in quick views Fix links in search results for observables API: When merging an alert in a case, merge the tag reports for the observable","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_3","text":"UI: In analyzer reports, add the tags on the observables Sort case template by name when creating a case Add support for cortex entities in charts and dashboard Related case view: sort cases by matching percentage API: Limit the duration of count queries not using the index to 5 seconds Add support for cortex entities for charts api Add TheHive version in logs","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#508-15-june-2022","text":"","title":"5.0.8 - 15 June 2022"},{"location":"thehive/release-notes/release-notes-5.0/#security","text":"Fix issue related to AD/LDAP module. If you are using the ad/ldap authentification, you should update to TheHive 5.0.8 or later","title":"Security"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_4","text":"UI: Fixes issues around importing observables from an analyzer report Analyzer template list was not be exhaustive in certain cases Fix pagination and sorting when listing similar cases In related cases list, show several matching observables instead of only one API: Removed full text search on tags: that caused slow queries as the index cannot be used here Fixed regression from TH4: when merging alerts into a case, observables could be duplicated if they appeared in several alerts Add ability to filter and sort by case/alert status/stage on api v0 Docker: Fix entrypoint for s3 configuration","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_4","text":"API: Log login success and failures: those logs are useful for auditing purpose, to detect password guessing attacks via large unsuccessful logon attempts Prevent duplication of custom fields values Improve queries when filtering on a custom field with a high number of matches","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#507-31-may-2022","text":"","title":"5.0.7 - 31 May 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_5","text":"UI: When creating a case from an alert, the correct case template is selected if the field was set in the alert Dashboard: fix an issue when converting a v4 dashboard Case count was not refreshed when adding a case Refresh comment section Misp configuration: Fix organisation selection Analyzer reports: in an alert, display the extractable observables can now import an observable of type file Custom Fields: Limit the size of custom fields in list views use the display name in list views When closing a case, custom fields are no longer deleted API: Fix breaking change in api V0: don't limit the size of observable data in json. This prevented the creation of files in observables. Note: with v1 the prefered way is to use a multipart request. Migration 3 to 5 Fix migration of custom fields of type number","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_5","text":"UI: Add ability to manually refresh a list when auto-refresh is disabled Notifications: Add a json validator when creating a custom filter Prevent automatic scroll when an entity is updated Fix flickering of updated data fields when updating an entity Other UI improvements API: Check for duplicated files (by filename) when attaching a file to a case or to a log Add field stage to alert and case in api v0 Users can manage their own api key without the permission manageUsers Add new auth mecanism based on htpasswd file","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#506-17-may-2022","text":"","title":"5.0.6 - 17 May 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_6","text":"UI: List of analyzers did not refresh correctly for alert observables Fix error when trying to download a task log attachment with the char { in the name logout popin remained open after a reconnection API docs did not appear when setting an http context Fix an issue where breadcrumps were not displayed correctly","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_6","text":"UI: Faster render of big markdown section: use marked-react library instead of react-markdown Adjust fanged message Improve sorting of tasks when in group mode Update observable count when an observable is added or removed Improvements for \"Required Action\" on tasks Migration 4 to 5: Improve migration for custom fields where previous script could overload the application","title":"Improvements"},{"location":"thehive/release-notes/release-notes-5.0/#505-5-may-2022","text":"","title":"5.0.5 - 5 May 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_7","text":"UI: Analyzer templates: optimize rendering time Fixed SSO login when using an http context API: Dashboard: increase number of retrieved values for aggregations (eg. chart on custom field values) Fix for permission manageConfig Improve support for AWS Keyspace: add retries on some failed queries Fix endpoint for deletion of catalog of ttp","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#504-3-may-2022","text":"","title":"5.0.4 - 3 May 2022"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_8","text":"UI: Tasks are now displayed by their order Changed color of field for search by case id Fixed an issue where custom fields were deleted when editing a case template Fix download link of attachments when http context is set Update of vulnerable libraries When using bulk edit, \"Add tags\" and \"Remove tags\" now work API: Fixed bug introduced in 5.0.3 where TTPs were not linked to their tactics and reported <unknown> It's now possible to delete the title prefix in a case template","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#503-15-april-2022","text":"","title":"5.0.3 - 15 April 2022"},{"location":"thehive/release-notes/release-notes-5.0/#new-features","text":"API: Add support to procedures (TTPs) when creating alerts in TheHive","title":"New Features"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_9","text":"UI: When importing a case from misp: additional parameters (custom fields, shares) are correctly sent When converting an alert to case, custom fields are no longer lost during the process Update moment.js library Migration tool: Fix typo in migration tool configuration","title":"Fixes"},{"location":"thehive/release-notes/release-notes-5.0/#502-8-april-2022","text":"We found a bug that prevents a user from using the reset pasword flow (present in 5.0.0). We recommend all 5.0.x users to update to this version. Fixes: Fix 404 page during the reset password flow Dashboard: include end date in time interval Fix detached live feed when using an http context","title":"5.0.2 - 8 April 2022"},{"location":"thehive/release-notes/release-notes-5.0/#501-7-april-2022","text":"TheHive 5.0.1 is the first patch release in the 5.0 series. It contains fixes and improvements for the UI and some small changes for the API compared to 5.0.0. We recommend all 5.0.0 users to update to this version.","title":"5.0.1 - 7 April 2022"},{"location":"thehive/release-notes/release-notes-5.0/#notables-changes","text":"UI: fix description display on search page display org admin tabs only with required permissions fix permissions checks in the application improve case sharing user experience notifications - fix urls of http endpoints notifications - improve editor when using template variables be able to download a file attachment for an alert forms and drawers don't lose user data when the entity is refreshed by the feed improve live feed for cortex jobs on observables API: add ability to create alert with observables (of type string and files), see. API docs for more information rename field user to assignee in case creation rename field customFieldValues to customFields in alert creation Backend: fix tag edition fix permissions check for observables in case of sharing fix user deletion (user could be left without org) fix license reload on cluster nodes add more check to ensure uniqueness of data increase quotas for service users (set to unlimited) and cluster nodes (unlimited for platinum plan) update of dependencies Docs: fix url to website add documentation for MISP endpoints","title":"Notables changes"},{"location":"thehive/setup/","text":"Installation & configuration guides # Overview # TheHive can be deployed on a standalone server or as a cluster. The application relies on: Apache Cassandra to store data (Supported version: 4.x). Elasticsearch as indexing engine (Supported version: 7.x). A file storage solution is also required ; the local filesystem of the server hosting the application is adequate in the standalone server scenario ; S3 MINIO otherwise. Using Lucene Starting with TheHive 5.x we strongly recommend using Elasticsearch for production servers. TheHive 4.1.x embbeded Lucene to handle the data index ; this is still the case with the latest version with which we suggest to use it only for testing purpose. Requirements # Hardware requirements depends on the number of concurrent users (including integrations) and how they use the system. The following table diplays safe thresholds when hosting all services on the same machine: Number of users TheHive Cassandra ElasticSearch < 10 2 / 2 GB 2 / 2 GB 2 / 2 GB < 20 2-4 / 4 GB 2-4 / 4 GB 2-4 / 4 GB < 50 4-6 / 8 GB 4-6 / 8 GB 4-6 / 8 GB Tip If you are installing everything on the same server, we recommend at least 4 cores and 16 GB of RAM. And don't forget to set up jvm.options at least for Elasticsearch. Operating systems # TheHive has been tested and is supported on the following operating systems: Ubuntu 20.04 LTS Debian 11 RHEL 8 Fedora 35 StrangeBee also provides an official Docker image . Installation guides # Too much in a hurry to read ? If you are using one of the supported operating systems, use our all-in-one installation script : Using Ubuntu or Debian sudo -v ; wget -q -O /tmp/install-thehive.sh https://archives.strangebee.com/scripts/install-deb.sh ; bash /tmp/install-thehive.sh Using RHEL or Fedora sudo -v ; wget -q -O /tmp/install-thehive.sh https://archives.strangebee.com/scripts/install-rpm.sh ; bash /tmp/install-thehive.sh For each release, DEB, RPM and ZIP binary packages are built and provided. Discover how to install TheHive quickly by following our installation guides: Use a dedicated server # TheHive can be used on virtual or physical servers. Our step-by-step guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. Use Docker # An Official Docker image publicly available. Follow our installation guide for Docker to use it in production. Use Kubernetes # TheHive is now compatible with Kubernetes - follow the related guide here . Configuration Guides # The configuration files are stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by DEB or RPM packages. This file contains a secret that should be used by one instance. The configuration should only contain the necessary information to start the application: database and indexing File storage Connectors enabled Other service parameters All other settings are available in the application WebUI. Advanced uses cases # Upgrade from TheHive 4.x (standalone server) # F.A.Q Can I upgrade from TheHive 4.0.x ? # Yes, all TheHive 4.x can be updated to TheHive 5; the documentation is coming soon! I use TheHive 3.x, can I upgrade my data to TheHive 5 ? # TheHive 3 is out of support since 31 December 2021. Please contact StrangeBee for further assistance. __ TheHive as a cluster # Install a cluster with 3 nodes # If you are looking to install a cluster (fault tolerant, H.A., ...), the following guide details all the installation and configuration steps to make a cluster with 3 nodes working. In this example, the cluster is composed of: 3 TheHive nodes 3 Cassandra nodes 3 Elasticsearch nodes 3 Min.IO nodes Upgrade a cluster # Upgrade a cluster Update from TheHive 3.x # TheHive 3.x is not supported any more since 31st of December, 2021. Contact StrangeBee for further assistance at contact@strangebee.com .","title":"Overview"},{"location":"thehive/setup/#installation-configuration-guides","text":"","title":"Installation &amp; configuration guides"},{"location":"thehive/setup/#overview","text":"TheHive can be deployed on a standalone server or as a cluster. The application relies on: Apache Cassandra to store data (Supported version: 4.x). Elasticsearch as indexing engine (Supported version: 7.x). A file storage solution is also required ; the local filesystem of the server hosting the application is adequate in the standalone server scenario ; S3 MINIO otherwise. Using Lucene Starting with TheHive 5.x we strongly recommend using Elasticsearch for production servers. TheHive 4.1.x embbeded Lucene to handle the data index ; this is still the case with the latest version with which we suggest to use it only for testing purpose.","title":"Overview"},{"location":"thehive/setup/#requirements","text":"Hardware requirements depends on the number of concurrent users (including integrations) and how they use the system. The following table diplays safe thresholds when hosting all services on the same machine: Number of users TheHive Cassandra ElasticSearch < 10 2 / 2 GB 2 / 2 GB 2 / 2 GB < 20 2-4 / 4 GB 2-4 / 4 GB 2-4 / 4 GB < 50 4-6 / 8 GB 4-6 / 8 GB 4-6 / 8 GB Tip If you are installing everything on the same server, we recommend at least 4 cores and 16 GB of RAM. And don't forget to set up jvm.options at least for Elasticsearch.","title":"Requirements"},{"location":"thehive/setup/#operating-systems","text":"TheHive has been tested and is supported on the following operating systems: Ubuntu 20.04 LTS Debian 11 RHEL 8 Fedora 35 StrangeBee also provides an official Docker image .","title":"Operating systems"},{"location":"thehive/setup/#installation-guides","text":"Too much in a hurry to read ? If you are using one of the supported operating systems, use our all-in-one installation script : Using Ubuntu or Debian sudo -v ; wget -q -O /tmp/install-thehive.sh https://archives.strangebee.com/scripts/install-deb.sh ; bash /tmp/install-thehive.sh Using RHEL or Fedora sudo -v ; wget -q -O /tmp/install-thehive.sh https://archives.strangebee.com/scripts/install-rpm.sh ; bash /tmp/install-thehive.sh For each release, DEB, RPM and ZIP binary packages are built and provided. Discover how to install TheHive quickly by following our installation guides:","title":"Installation guides"},{"location":"thehive/setup/#use-a-dedicated-server","text":"TheHive can be used on virtual or physical servers. Our step-by-step guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages.","title":"Use a dedicated server"},{"location":"thehive/setup/#use-docker","text":"An Official Docker image publicly available. Follow our installation guide for Docker to use it in production.","title":"Use Docker "},{"location":"thehive/setup/#use-kubernetes","text":"TheHive is now compatible with Kubernetes - follow the related guide here .","title":"Use Kubernetes "},{"location":"thehive/setup/#configuration-guides","text":"The configuration files are stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by DEB or RPM packages. This file contains a secret that should be used by one instance. The configuration should only contain the necessary information to start the application: database and indexing File storage Connectors enabled Other service parameters All other settings are available in the application WebUI.","title":"Configuration Guides"},{"location":"thehive/setup/#advanced-uses-cases","text":"","title":"Advanced uses cases"},{"location":"thehive/setup/#upgrade-from-thehive-4x-standalone-server","text":"F.A.Q","title":"Upgrade from TheHive 4.x (standalone server)"},{"location":"thehive/setup/#can-i-upgrade-from-thehive-40x","text":"Yes, all TheHive 4.x can be updated to TheHive 5; the documentation is coming soon!","title":"Can I upgrade from TheHive 4.0.x ?"},{"location":"thehive/setup/#i-use-thehive-3x-can-i-upgrade-my-data-to-thehive-5","text":"TheHive 3 is out of support since 31 December 2021. Please contact StrangeBee for further assistance. __","title":"I use TheHive 3.x, can I upgrade my data to TheHive 5 ?"},{"location":"thehive/setup/#thehive-as-a-cluster","text":"","title":"TheHive as a cluster"},{"location":"thehive/setup/#install-a-cluster-with-3-nodes","text":"If you are looking to install a cluster (fault tolerant, H.A., ...), the following guide details all the installation and configuration steps to make a cluster with 3 nodes working. In this example, the cluster is composed of: 3 TheHive nodes 3 Cassandra nodes 3 Elasticsearch nodes 3 Min.IO nodes","title":"Install a cluster with 3 nodes"},{"location":"thehive/setup/#upgrade-a-cluster","text":"Upgrade a cluster","title":"Upgrade a cluster"},{"location":"thehive/setup/#update-from-thehive-3x","text":"TheHive 3.x is not supported any more since 31st of December, 2021. Contact StrangeBee for further assistance at contact@strangebee.com .","title":"Update from TheHive 3.x"},{"location":"thehive/setup/configuration/akka/","text":"Akka # Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. Basic configuration # A good cluster setup requires at least 3 nodes of TheHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Node 2 Node 3 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } SSL/TLS support # Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"Akka"},{"location":"thehive/setup/configuration/akka/#akka","text":"Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience.","title":"Akka"},{"location":"thehive/setup/configuration/akka/#basic-configuration","text":"A good cluster setup requires at least 3 nodes of TheHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Node 2 Node 3 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] }","title":"Basic configuration"},{"location":"thehive/setup/configuration/akka/#ssltls-support","text":"Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"SSL/TLS support"},{"location":"thehive/setup/configuration/authentication/","text":"Authentication # Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules: session # Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] } local # Create a session if the provided login and password, or API key is correct according to the local user database. key # Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key). basic # Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate header # Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login ad # Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] } ldap # Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] } oauth2 # Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak Okta Github Microsoft 365 Google ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://accounts.google.com/o/oauth2/v2/auth\" authorizationHeader : \"Bearer \" tokenUrl : \"https://oauth2.googleapis.com/token\" userUrl : \"https://openidconnect.googleapis.com/v1/userinfo\" scope : [ \"email\" , \"profile\" , \"openid\" ] userIdField : \"email\" # userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration User autocreation # To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert Multi-Factor Authentication # Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Authentication"},{"location":"thehive/setup/configuration/authentication/#authentication","text":"Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules:","title":"Authentication"},{"location":"thehive/setup/configuration/authentication/#session","text":"Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] }","title":"session"},{"location":"thehive/setup/configuration/authentication/#local","text":"Create a session if the provided login and password, or API key is correct according to the local user database.","title":"local"},{"location":"thehive/setup/configuration/authentication/#key","text":"Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key).","title":"key"},{"location":"thehive/setup/configuration/authentication/#basic","text":"Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate","title":"basic"},{"location":"thehive/setup/configuration/authentication/#header","text":"Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login","title":"header"},{"location":"thehive/setup/configuration/authentication/#ad","text":"Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] }","title":"ad"},{"location":"thehive/setup/configuration/authentication/#ldap","text":"Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] }","title":"ldap"},{"location":"thehive/setup/configuration/authentication/#oauth2","text":"Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak Okta Github Microsoft 365 Google ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://accounts.google.com/o/oauth2/v2/auth\" authorizationHeader : \"Bearer \" tokenUrl : \"https://oauth2.googleapis.com/token\" userUrl : \"https://openidconnect.googleapis.com/v1/userinfo\" scope : [ \"email\" , \"profile\" , \"openid\" ] userIdField : \"email\" # userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration","title":"oauth2"},{"location":"thehive/setup/configuration/authentication/#user-autocreation","text":"To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert","title":"User autocreation"},{"location":"thehive/setup/configuration/authentication/#multi-factor-authentication","text":"Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Multi-Factor Authentication"},{"location":"thehive/setup/configuration/connectors-misp/","text":"TheHive connector: MISP # Enable MISP connector # The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule Configuration # TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"TheHive connector: MISP"},{"location":"thehive/setup/configuration/connectors-misp/#thehive-connector-misp","text":"","title":"TheHive connector: MISP"},{"location":"thehive/setup/configuration/connectors-misp/#enable-misp-connector","text":"The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule","title":"Enable MISP connector"},{"location":"thehive/setup/configuration/connectors-misp/#configuration","text":"TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"Configuration"},{"location":"thehive/setup/configuration/connectors/","text":"TheHive connectors # Thehive has connectors to integrate with Cortex and MISP. By default, they are enabled in the /etc/thehive/application.conf configuration file. If you are not using one of them or both they can be disabled by commenting the relevant line: /etc/thehive/application.conf [ .. ] scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule # scalligraph.modules += org.thp.thehive.connector.misp.MispModule # (1) MISP connector is disabled Updating this configuration file requires TheHive service to be restarted","title":"TheHive connectors"},{"location":"thehive/setup/configuration/connectors/#thehive-connectors","text":"Thehive has connectors to integrate with Cortex and MISP. By default, they are enabled in the /etc/thehive/application.conf configuration file. If you are not using one of them or both they can be disabled by commenting the relevant line: /etc/thehive/application.conf [ .. ] scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule # scalligraph.modules += org.thp.thehive.connector.misp.MispModule # (1) MISP connector is disabled Updating this configuration file requires TheHive service to be restarted","title":"TheHive connectors"},{"location":"thehive/setup/configuration/database/","text":"Database and index configuration # TheHive use Cassandra and Elasticsearch databases to manage data and index. According to the setup, the instance can use: Basic Configuation # A typical database configuration for TheHive looks like this: ## Database configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"IP_ADDRESS\"] cql { cluster-name = thp keyspace = thehive } } ## Index configuration index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } } List of possible parameters # Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ . Use cases # Database and index engine can be different, depending on the use case and target setup: Example Standalone server with Cassandra & Elasticsearch Cluster with Cassandra & Elasticsearch Install a Cassandra server locally Install Elasticsearch Configure TheHive accordingly ## Database Configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"127.0.0.1\"] ## Cassandra authentication (if configured) username = \"thehive_account\" password = \"cassandra_password\" cql { cluster-name = thp keyspace = thehive } } ## Index configuration index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username = \"thehive_account\" password = \"cassandra_password\" cql { cluster-name = thp keyspace = thehive } } ## Index configuration index { search { backend = elasticsearch hostname = [\"10.1.2.5\"] index-name = thehive elasticsearch { http { auth { type = basic basic { username = httpuser password = httppassword } } } ssl { enabled = true truststore { location = /path/to/your/truststore.jks password = truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line = script.allowed_types : inline,stored","title":"Database & index"},{"location":"thehive/setup/configuration/database/#database-and-index-configuration","text":"TheHive use Cassandra and Elasticsearch databases to manage data and index. According to the setup, the instance can use:","title":"Database and index configuration"},{"location":"thehive/setup/configuration/database/#basic-configuation","text":"A typical database configuration for TheHive looks like this: ## Database configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"IP_ADDRESS\"] cql { cluster-name = thp keyspace = thehive } } ## Index configuration index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } }","title":"Basic Configuation"},{"location":"thehive/setup/configuration/database/#list-of-possible-parameters","text":"Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ .","title":"List of possible parameters"},{"location":"thehive/setup/configuration/database/#use-cases","text":"Database and index engine can be different, depending on the use case and target setup: Example Standalone server with Cassandra & Elasticsearch Cluster with Cassandra & Elasticsearch Install a Cassandra server locally Install Elasticsearch Configure TheHive accordingly ## Database Configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"127.0.0.1\"] ## Cassandra authentication (if configured) username = \"thehive_account\" password = \"cassandra_password\" cql { cluster-name = thp keyspace = thehive } } ## Index configuration index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider = janusgraph janusgraph { ## Storage configuration storage { backend = cql hostname = [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username = \"thehive_account\" password = \"cassandra_password\" cql { cluster-name = thp keyspace = thehive } } ## Index configuration index { search { backend = elasticsearch hostname = [\"10.1.2.5\"] index-name = thehive elasticsearch { http { auth { type = basic basic { username = httpuser password = httppassword } } } ssl { enabled = true truststore { location = /path/to/your/truststore.jks password = truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line = script.allowed_types : inline,stored","title":"Use cases"},{"location":"thehive/setup/configuration/file-storage/","text":"File storage configuration # TheHive can be configured to use local or distributed filesystems. Local or NFS Min.IO Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Install a Min.IO cluster Configure each node of TheHive accordingly: /etc/thehive/application.conf with TheHive 5.0.x ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" secretKey = \"password\" region = \"us-east-1\" } } alpakka.s3.access-style = path /etc/thehive/application.conf with TheHive 5.1.x storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" aws.credentials.provider = \"static\" aws.credentials.secret-access-key = \"password\" access-style = path aws.region.provider = \"static\" aws.region.default-region = \"us-east-1\" } } The configuration is backward compatible us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional.","title":"File storage"},{"location":"thehive/setup/configuration/file-storage/#file-storage-configuration","text":"TheHive can be configured to use local or distributed filesystems. Local or NFS Min.IO Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Install a Min.IO cluster Configure each node of TheHive accordingly: /etc/thehive/application.conf with TheHive 5.0.x ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" secretKey = \"password\" region = \"us-east-1\" } } alpakka.s3.access-style = path /etc/thehive/application.conf with TheHive 5.1.x storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" aws.credentials.provider = \"static\" aws.credentials.secret-access-key = \"password\" access-style = path aws.region.provider = \"static\" aws.region.default-region = \"us-east-1\" } } The configuration is backward compatible us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional.","title":"File storage configuration"},{"location":"thehive/setup/configuration/logs/","text":"Logs Configuration # TheHive uses logback to log information about the running process. The loggers are configured in the file /etc/thehive/logback.xml . Edit this file and reload the service to apply your changes. By default, logs are stored in /var/log/thehive/ . The last log file is called application.log and older filed are stored in a compressed format in application.%i.log.zip . How to increase/decrease the log level # Logback supports several log levels. To log more things you can update the root level to DEBUG (or TRACE ): logback.xml <!-- ... --> <root level= \"DEBUG\" > <!-- ... --> </root> To log less things you can use WARN , ERROR or OFF levels. The log level can also be updated individually by changing the level of a specific logger: logback.xml <logger name= \"org.thp\" level= \"DEBUG\" /> Logs in docker # In the docker container the logger is configured with the file /etc/thehive/logback.xml and the application by default will log to stdout and to /var/log/thehive/application.log . If you want to change the default configuration, you can mount your own logback file to /etc/thehive/logback.xml . Debug your logback configuration # If you have issues with it set the debug flag to true in logback.xml : logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"true\" > This will log your logback configuration in the console when the app starts How to create an access log # By changing the logback configuration, you can redirect certain logs from the application. Below is an example where access logs are redirected to the file access.log and uses a rolling file strategy. To apply this in your configuration, copy the appender s and logger s definitions. logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"false\" > <!-- ... other appenders and settings --> <appender name= \"ACCESSFILE\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> /var/log/thehive/access.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\" > <fileNamePattern> /var/log/thehive/access.%i.log.zip </fileNamePattern> <minIndex> 1 </minIndex> <maxIndex> 10 </maxIndex> </rollingPolicy> <triggeringPolicy class= \"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\" > <maxFileSize> 10MB </maxFileSize> </triggeringPolicy> <encoder> <pattern> %date [%level] from %logger [%traceID] %message%n%xException </pattern> </encoder> </appender> <appender name= \"ASYNCACCESSFILE\" class= \"ch.qos.logback.classic.AsyncAppender\" > <appender-ref ref= \"ACCESSFILE\" /> </appender> <logger name= \"org.thp.scalligraph.AccessLogFilter\" > <appender-ref ref= \"ASYNCACCESSFILE\" /> </logger> <logger name= \"org.thp.scalligraph.controllers.Entrypoint\" > <appender-ref ref= \"ASYNCACCESSFILE\" /> </logger> <root level= \"INFO\" > <!-- other appender-refs ... --> </root> </configuration> How to send logs to syslog # You will need to add a SyslogAppender . logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"false\" > <!-- ... other appenders and settings --> <appender name= \"SYSLOG\" class= \"ch.qos.logback.classic.net.SyslogAppender\" > <syslogHost> remote_host </syslogHost> <facility> AUTH </facility> <suffixPattern> [%thread] %logger %msg </suffixPattern> </appender> <root level= \"INFO\" > <appender-ref ref= \"SYSLOG\" /> <!-- other appender-refs ... --> </root> See the official docs for more information. Limitations: The official syslog appender can only send logs via UDP to a server. It cannot use TCP and TLS","title":"Logs Configuration"},{"location":"thehive/setup/configuration/logs/#logs-configuration","text":"TheHive uses logback to log information about the running process. The loggers are configured in the file /etc/thehive/logback.xml . Edit this file and reload the service to apply your changes. By default, logs are stored in /var/log/thehive/ . The last log file is called application.log and older filed are stored in a compressed format in application.%i.log.zip .","title":"Logs Configuration"},{"location":"thehive/setup/configuration/logs/#how-to-increasedecrease-the-log-level","text":"Logback supports several log levels. To log more things you can update the root level to DEBUG (or TRACE ): logback.xml <!-- ... --> <root level= \"DEBUG\" > <!-- ... --> </root> To log less things you can use WARN , ERROR or OFF levels. The log level can also be updated individually by changing the level of a specific logger: logback.xml <logger name= \"org.thp\" level= \"DEBUG\" />","title":"How to increase/decrease the log level"},{"location":"thehive/setup/configuration/logs/#logs-in-docker","text":"In the docker container the logger is configured with the file /etc/thehive/logback.xml and the application by default will log to stdout and to /var/log/thehive/application.log . If you want to change the default configuration, you can mount your own logback file to /etc/thehive/logback.xml .","title":"Logs in docker"},{"location":"thehive/setup/configuration/logs/#debug-your-logback-configuration","text":"If you have issues with it set the debug flag to true in logback.xml : logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"true\" > This will log your logback configuration in the console when the app starts","title":"Debug your logback configuration"},{"location":"thehive/setup/configuration/logs/#how-to-create-an-access-log","text":"By changing the logback configuration, you can redirect certain logs from the application. Below is an example where access logs are redirected to the file access.log and uses a rolling file strategy. To apply this in your configuration, copy the appender s and logger s definitions. logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"false\" > <!-- ... other appenders and settings --> <appender name= \"ACCESSFILE\" class= \"ch.qos.logback.core.rolling.RollingFileAppender\" > <file> /var/log/thehive/access.log </file> <rollingPolicy class= \"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\" > <fileNamePattern> /var/log/thehive/access.%i.log.zip </fileNamePattern> <minIndex> 1 </minIndex> <maxIndex> 10 </maxIndex> </rollingPolicy> <triggeringPolicy class= \"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\" > <maxFileSize> 10MB </maxFileSize> </triggeringPolicy> <encoder> <pattern> %date [%level] from %logger [%traceID] %message%n%xException </pattern> </encoder> </appender> <appender name= \"ASYNCACCESSFILE\" class= \"ch.qos.logback.classic.AsyncAppender\" > <appender-ref ref= \"ACCESSFILE\" /> </appender> <logger name= \"org.thp.scalligraph.AccessLogFilter\" > <appender-ref ref= \"ASYNCACCESSFILE\" /> </logger> <logger name= \"org.thp.scalligraph.controllers.Entrypoint\" > <appender-ref ref= \"ASYNCACCESSFILE\" /> </logger> <root level= \"INFO\" > <!-- other appender-refs ... --> </root> </configuration>","title":"How to create an access log"},{"location":"thehive/setup/configuration/logs/#how-to-send-logs-to-syslog","text":"You will need to add a SyslogAppender . logback.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration debug= \"false\" > <!-- ... other appenders and settings --> <appender name= \"SYSLOG\" class= \"ch.qos.logback.classic.net.SyslogAppender\" > <syslogHost> remote_host </syslogHost> <facility> AUTH </facility> <suffixPattern> [%thread] %logger %msg </suffixPattern> </appender> <root level= \"INFO\" > <appender-ref ref= \"SYSLOG\" /> <!-- other appender-refs ... --> </root> See the official docs for more information. Limitations: The official syslog appender can only send logs via UDP to a server. It cannot use TCP and TLS","title":"How to send logs to syslog"},{"location":"thehive/setup/configuration/proxy/","text":"Proxy settings # Proxy for global application # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy settings"},{"location":"thehive/setup/configuration/proxy/#proxy-settings","text":"","title":"Proxy settings"},{"location":"thehive/setup/configuration/proxy/#proxy-for-global-application","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy for global application"},{"location":"thehive/setup/configuration/secret/","text":"secret.conf file # This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"`secret.conf` file"},{"location":"thehive/setup/configuration/secret/#secretconf-file","text":"This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"secret.conf file"},{"location":"thehive/setup/configuration/service/","text":"Service # Listen address & port # By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000 Context # If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\" Specific configuration for streams # If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds Using Web proxy # if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Service"},{"location":"thehive/setup/configuration/service/#service","text":"","title":"Service"},{"location":"thehive/setup/configuration/service/#listen-address-port","text":"By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000","title":"Listen address &amp; port"},{"location":"thehive/setup/configuration/service/#context","text":"If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\"","title":"Context"},{"location":"thehive/setup/configuration/service/#specific-configuration-for-streams","text":"If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds","title":"Specific configuration for streams"},{"location":"thehive/setup/configuration/service/#using-web-proxy","text":"if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Using Web proxy"},{"location":"thehive/setup/configuration/ssl/","text":"SSL # Server configuration # We recommend using a reverse proxy to manage SSL layer. Connectors # Client configuration # SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false Certificate manager # Certificate manager is used to store client certificates and certificate authorities. Custom Certificate Authority # Global configuration # If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart Use dedicated trust stores # the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type = \"JKS\" // JKS or PEM path = \"keystore.jks\" password = \"password1\" } ] } Client certificates # keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type = \"pkcs12\" // JKS or PEM path = \"mycert.p12\" password = \"password1\" } ] } Protocols # If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so by providing a list explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] Debugging # To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"SSL"},{"location":"thehive/setup/configuration/ssl/#ssl","text":"","title":"SSL"},{"location":"thehive/setup/configuration/ssl/#server-configuration","text":"We recommend using a reverse proxy to manage SSL layer.","title":"Server configuration"},{"location":"thehive/setup/configuration/ssl/#connectors","text":"","title":"Connectors"},{"location":"thehive/setup/configuration/ssl/#client-configuration","text":"SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false","title":"Client configuration"},{"location":"thehive/setup/configuration/ssl/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities.","title":"Certificate manager"},{"location":"thehive/setup/configuration/ssl/#custom-certificate-authority","text":"","title":"Custom Certificate Authority"},{"location":"thehive/setup/configuration/ssl/#global-configuration","text":"If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart","title":"Global configuration"},{"location":"thehive/setup/configuration/ssl/#use-dedicated-trust-stores","text":"the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type = \"JKS\" // JKS or PEM path = \"keystore.jks\" password = \"password1\" } ] }","title":"Use dedicated trust stores"},{"location":"thehive/setup/configuration/ssl/#client-certificates","text":"keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type = \"pkcs12\" // JKS or PEM path = \"mycert.p12\" password = \"password1\" } ] }","title":"Client certificates"},{"location":"thehive/setup/configuration/ssl/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so by providing a list explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/setup/configuration/ssl/#ciphers","text":"Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/setup/configuration/ssl/#debugging","text":"To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/setup/installation/3-node-cluster/","text":"Use TheHive as a cluster # This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes. Prerequisite # 3 servers with TheHive and Cassandra installed. TheHive # In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like the following: /etc/thehive/application.conf akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Cassandra # We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this section, we consider that all nodes sit on the same network. Configuration # Nodes configuration # For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml /etc/cassandra/cassandra.yml cluster_name : 'thp' num_tokens : 256 authenticator : PasswordAuthenticator authorizer : CassandraAuthorizer role_manager : CassandraRoleManager data_file_directories : - /var/lib/cassandra/data commitlog_directory : /var/lib/cassandra/commitlog saved_caches_directory : /var/lib/cassandra/saved_caches seed_provider : - class_name : org.apache.cassandra.locator.SimpleSeedProvider parameters : - seeds : \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface : eth0 endpoint_snitch : SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties Start nodes # On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status = Up/Down | / State = Normal/Leaving/Joining/Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN <ip node 1 > 776 .53 KiB 256 100 .0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2 > 671 .72 KiB 256 100 .0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3 > 611 .54 KiB 256 100 .0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1 Initialise the database # On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ; TheHive associated configuration # Update the configuration of thehive accordingly in /etc/thehive/application.conf : /etc/thehive/application.conf ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend = cql hostname = [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username = \"thehive\" password = \"PASSWORD\" cql { cluster-name = thp keyspace = thehive } } Troubleshooting # Example of error message in /var/log/cassandra/ log files InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d.` set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE Fix it by running following CQL command: ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; and with the following command: nodetool repair -full MinIO # MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be applied on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3. Create a dedicated system account # Create a dedicated user and group for MinIO. adduser minio-user addgroup minio-user Create at least 2 data volumes on each server # Create 2 folders on each server: mkdir -p /srv/minio/ { 1 ,2 } chown -R minio-user:minio-user /srv/minio Setup hosts files # Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3 installation # Example for DEB packages wget https://dl.min.io/server/minio/release/linux-amd64/minio_20220607003341.0.0_amd64.deb wget https://dl.min.io/client/mc/release/linux-amd64/mcli_20220509040826.0.0_amd64.deb dpkg -i minio_20220607003341.0.0_amd64.deb dpkg -i mcli_20220509040826.0.0_amd64.deb Visit https://dl.min.io/ to find last version of required packages. Configuration # Create or edit file /etc/default/minio /etc/default/minio MINIO_OPTS=\"--address :9100 --console-address :9001\" MINIO_VOLUMES=\"http://minio{1...3}:9100/srv/minio/{1...2}\" MINIO_ROOT_USER=thehive MINIO_ROOT_PASSWORD=password MINIO_SITE_REGION=\"us-east-1\" Enable and start the service # systemctl daemon-reload systemctl enable minio systemctl start minio.service Prepare the service for TheHive # Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers. TheHive associated configuration # For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node: With TheHive 5.0.x With TheHive 5.1.0 or higher /etc/thehive/application.conf storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" secretKey = \"password\" region = \"us-east-1\" } } alpakka.s3.access-style = path /etc/thehive/application.conf storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" aws.credentials.provider = \"static\" aws.credentials.secret-access-key = \"password\" access-style = path aws.region.provider = \"static\" aws.region.default-region = \"us-east-1\" } } The configuration is backward compatible Each TheHive server can connect to one MinIO server.","title":"Use TheHive as a cluster"},{"location":"thehive/setup/installation/3-node-cluster/#use-thehive-as-a-cluster","text":"This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes.","title":"Use TheHive as a cluster"},{"location":"thehive/setup/installation/3-node-cluster/#prerequisite","text":"3 servers with TheHive and Cassandra installed.","title":"Prerequisite"},{"location":"thehive/setup/installation/3-node-cluster/#thehive","text":"In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like the following: /etc/thehive/application.conf akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] }","title":"TheHive"},{"location":"thehive/setup/installation/3-node-cluster/#cassandra","text":"We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this section, we consider that all nodes sit on the same network.","title":"Cassandra"},{"location":"thehive/setup/installation/3-node-cluster/#configuration","text":"","title":"Configuration"},{"location":"thehive/setup/installation/3-node-cluster/#nodes-configuration","text":"For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml /etc/cassandra/cassandra.yml cluster_name : 'thp' num_tokens : 256 authenticator : PasswordAuthenticator authorizer : CassandraAuthorizer role_manager : CassandraRoleManager data_file_directories : - /var/lib/cassandra/data commitlog_directory : /var/lib/cassandra/commitlog saved_caches_directory : /var/lib/cassandra/saved_caches seed_provider : - class_name : org.apache.cassandra.locator.SimpleSeedProvider parameters : - seeds : \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface : eth0 endpoint_snitch : SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties","title":"Nodes configuration"},{"location":"thehive/setup/installation/3-node-cluster/#start-nodes","text":"On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status = Up/Down | / State = Normal/Leaving/Joining/Moving -- Address Load Tokens Owns ( effective ) Host ID Rack UN <ip node 1 > 776 .53 KiB 256 100 .0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2 > 671 .72 KiB 256 100 .0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3 > 611 .54 KiB 256 100 .0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1","title":"Start nodes"},{"location":"thehive/setup/installation/3-node-cluster/#initialise-the-database","text":"On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ;","title":"Initialise the database"},{"location":"thehive/setup/installation/3-node-cluster/#thehive-associated-configuration","text":"Update the configuration of thehive accordingly in /etc/thehive/application.conf : /etc/thehive/application.conf ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend = cql hostname = [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username = \"thehive\" password = \"PASSWORD\" cql { cluster-name = thp keyspace = thehive } }","title":"TheHive associated configuration"},{"location":"thehive/setup/installation/3-node-cluster/#troubleshooting","text":"Example of error message in /var/log/cassandra/ log files InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d.` set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE Fix it by running following CQL command: ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; and with the following command: nodetool repair -full","title":"Troubleshooting"},{"location":"thehive/setup/installation/3-node-cluster/#minio","text":"MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be applied on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3.","title":"MinIO"},{"location":"thehive/setup/installation/3-node-cluster/#create-a-dedicated-system-account","text":"Create a dedicated user and group for MinIO. adduser minio-user addgroup minio-user","title":"Create a dedicated system account"},{"location":"thehive/setup/installation/3-node-cluster/#create-at-least-2-data-volumes-on-each-server","text":"Create 2 folders on each server: mkdir -p /srv/minio/ { 1 ,2 } chown -R minio-user:minio-user /srv/minio","title":"Create at least 2 data volumes on each server"},{"location":"thehive/setup/installation/3-node-cluster/#setup-hosts-files","text":"Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3","title":"Setup hosts files"},{"location":"thehive/setup/installation/3-node-cluster/#installation","text":"Example for DEB packages wget https://dl.min.io/server/minio/release/linux-amd64/minio_20220607003341.0.0_amd64.deb wget https://dl.min.io/client/mc/release/linux-amd64/mcli_20220509040826.0.0_amd64.deb dpkg -i minio_20220607003341.0.0_amd64.deb dpkg -i mcli_20220509040826.0.0_amd64.deb Visit https://dl.min.io/ to find last version of required packages.","title":"installation"},{"location":"thehive/setup/installation/3-node-cluster/#configuration_1","text":"Create or edit file /etc/default/minio /etc/default/minio MINIO_OPTS=\"--address :9100 --console-address :9001\" MINIO_VOLUMES=\"http://minio{1...3}:9100/srv/minio/{1...2}\" MINIO_ROOT_USER=thehive MINIO_ROOT_PASSWORD=password MINIO_SITE_REGION=\"us-east-1\"","title":"Configuration"},{"location":"thehive/setup/installation/3-node-cluster/#enable-and-start-the-service","text":"systemctl daemon-reload systemctl enable minio systemctl start minio.service","title":"Enable and start the service"},{"location":"thehive/setup/installation/3-node-cluster/#prepare-the-service-for-thehive","text":"Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers.","title":"Prepare the service for TheHive"},{"location":"thehive/setup/installation/3-node-cluster/#thehive-associated-configuration_1","text":"For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node: With TheHive 5.0.x With TheHive 5.1.0 or higher /etc/thehive/application.conf storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" secretKey = \"password\" region = \"us-east-1\" } } alpakka.s3.access-style = path /etc/thehive/application.conf storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"thehive\" aws.credentials.provider = \"static\" aws.credentials.secret-access-key = \"password\" access-style = path aws.region.provider = \"static\" aws.region.default-region = \"us-east-1\" } } The configuration is backward compatible Each TheHive server can connect to one MinIO server.","title":"TheHive associated configuration"},{"location":"thehive/setup/installation/docker/","text":"Running with docker # This page will guide you on how to use the docker image of TheHive Quick start # docker run --rm -p 9000 :9000 strangebee/thehive:<version> This will start an instance of thehive using a local database and index. Note that the data will be deleted when the container is deleted . So this should only be used for evaluation and tests. Connect on http://localhost:9000 to see the login page. Default credentials Default admin credentials are admin@thehive.local / secret We recommend to always set the version of your docker image in production scenarios and not to use latest . Using your own configuration file # The entry point arguments are used to create a application.conf file in the container. A custom configuration file can also be provided: docker run --rm -p 9000 :9000 -v <host_data_folder>:/data/files -v <host_conf_folder>:/data/conf <thehive-image> --no-config --config-file /data/conf/application.conf The folder <host_conf_folder> needs to contain a application.conf file. --no-config is used to tell the entrypoint to not generate any configuration. Otherwise the entry point will generate a default configuration that will be merged with your file. Using command line arguments # We recommend running TheHive with Cassandra and Elasticsearch for data storage and minio for file storage. You can pass the hostnames of your instances via the arguments: docker run --rm -p 9000 :9000 -v <host_data_folder>:/data/files strangebee/thehive:<version> \\ --secret <secret> --cql-hostnames <cqlhost1>,<cqlhost2>,... --cql-username <cqlusername> --cql-password <cqlusername> --index-backend elasticsearch --es-hostnames <eshost1>,<eshost2>,... --s3-endpoint <minio_endpoint> --s3-access-key <minio_access_key> --s3-secret-key <minio_secret_key> This will connect your docker container to external cassandra and elasticsearch nodes. The data files will be stored on minio. The container exposes TheHive on the port 9000 . All options # You can get a list of all options supported by the docker entry point with -h : docker run --rm strangebee/thehive:<version> -h Available options: --config-file <file> | configuration file path --no-config | do not try to configure TheHive (add secret and elasticsearch) --no-config-secret | do not add random secret to configuration --secret <secret> | secret to secure sessions --show-secret | show the generated secret --no-config-db | do not configure database automatically --cql-hostnames <host>,<host>,... | resolve these hostnames to find cassandra instances --cql-username <username> | username of cassandra database --cql-password <password> | password of cassandra database --no-cql-wait | don't wait for cassandra --bdb-directory <path> | location of local database, if cassandra is not used (default: /data/db) --index-backend | backend to use for index. One of 'lucene' or 'elasticsearch' (default: lucene) --es-hostnames | elasticsearch instances used for index --es-index | elasticsearch index name to used (default: thehive) --no-config-storage | do not configure storage automatically --storage-directory <path> | location of local storage, if s3 is not used (default: /data/files) --s3-endpoint <endpoint> | endpoint of s3 (or other object storage) if used, use 's3.amazonaws.com' for aws s3 --s3-region <region> | s3 region, optional for minio --s3-bucket <bucket> | name of the bucket to use (default: thehive), the bucket must already exists --s3-access-key <key> | s3 access key (required for s3) --s3-secret-key <key> | s3 secret key (required for s3) --s3-use-path-access-style | set this flag if you use minio or other non aws s3 provider, default to virtual host style --no-config-cortex | do not add Cortex configuration --cortex-proto <proto> | define protocol to connect to Cortex (default: http) --cortex-port <port> | define port to connect to Cortex (default: 9001) --cortex-hostnames <host>,<host>,... | resolve this hostname to find Cortex instances --cortex-keys <key>,<key>,... | define Cortex key --kubernetes | will use kubernetes api to join other nodes --kubernetes-pod-label-selector <selector> | selector to use to select other pods running the app (default app=thehive) --cluster-min-nodes-count <count> | minimum number of nodes to form a cluster (default to 1) migrate <param> <param> ... | run migration tool cloner <param> <param> ... | run cloner tool Using docker compose # Below is an example of a docker-compose file. It's composed of several services: cassandra, elasticsearch, minio and TheHive. version : \"3\" services : thehive : image : strangebee/thehive:<version> depends_on : - cassandra - elasticsearch - minio mem_limit : 1500m ports : - \"9000:9000\" environment : - JVM_OPTS=\"-Xms1024M -Xmx1024M\" command : - --secret - \"mySecretForTheHive\" - \"--cql-hostnames\" - \"cassandra\" - \"--index-backend\" - \"elasticsearch\" - \"--es-hostnames\" - \"elasticsearch\" - \"--s3-endpoint\" - \"http://minio:9000\" - \"--s3-access-key\" - \"minioadmin\" - \"--s3-secret-key\" - \"minioadmin\" - \"--s3-use-path-access-style\" - \"--no-config-cortex\" cassandra : image : 'cassandra:4' mem_limit : 1000m ports : - \"9042:9042\" environment : - CASSANDRA_CLUSTER_NAME=TheHive volumes : - cassandradata:/var/lib/cassandra elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.16.2 mem_limit : 512m ports : - \"9200:9200\" environment : - discovery.type=single-node - xpack.security.enabled=false volumes : - elasticsearchdata:/usr/share/elasticsearch/data minio : image : quay.io/minio/minio command : [ \"minio\" , \"server\" , \"/data\" , \"--console-address\" , \":9001\" ] environment : - MINIO_ROOT_USER=minioadmin - MINIO_ROOT_PASSWORD=minioadmin ports : - \"9001:9001\" volumes : - \"miniodata:/data\" volumes : miniodata : cassandradata : elasticsearchdata : It's highly recommended to change the default credentials and secrets. Usage in kubernetes # Refer to the dedicated page for instuctions on how to deploy on kubernetes.","title":"Running with Docker"},{"location":"thehive/setup/installation/docker/#running-with-docker","text":"This page will guide you on how to use the docker image of TheHive","title":" Running with docker"},{"location":"thehive/setup/installation/docker/#quick-start","text":"docker run --rm -p 9000 :9000 strangebee/thehive:<version> This will start an instance of thehive using a local database and index. Note that the data will be deleted when the container is deleted . So this should only be used for evaluation and tests. Connect on http://localhost:9000 to see the login page. Default credentials Default admin credentials are admin@thehive.local / secret We recommend to always set the version of your docker image in production scenarios and not to use latest .","title":"Quick start"},{"location":"thehive/setup/installation/docker/#using-your-own-configuration-file","text":"The entry point arguments are used to create a application.conf file in the container. A custom configuration file can also be provided: docker run --rm -p 9000 :9000 -v <host_data_folder>:/data/files -v <host_conf_folder>:/data/conf <thehive-image> --no-config --config-file /data/conf/application.conf The folder <host_conf_folder> needs to contain a application.conf file. --no-config is used to tell the entrypoint to not generate any configuration. Otherwise the entry point will generate a default configuration that will be merged with your file.","title":"Using your own configuration file"},{"location":"thehive/setup/installation/docker/#using-command-line-arguments","text":"We recommend running TheHive with Cassandra and Elasticsearch for data storage and minio for file storage. You can pass the hostnames of your instances via the arguments: docker run --rm -p 9000 :9000 -v <host_data_folder>:/data/files strangebee/thehive:<version> \\ --secret <secret> --cql-hostnames <cqlhost1>,<cqlhost2>,... --cql-username <cqlusername> --cql-password <cqlusername> --index-backend elasticsearch --es-hostnames <eshost1>,<eshost2>,... --s3-endpoint <minio_endpoint> --s3-access-key <minio_access_key> --s3-secret-key <minio_secret_key> This will connect your docker container to external cassandra and elasticsearch nodes. The data files will be stored on minio. The container exposes TheHive on the port 9000 .","title":"Using command line arguments"},{"location":"thehive/setup/installation/docker/#all-options","text":"You can get a list of all options supported by the docker entry point with -h : docker run --rm strangebee/thehive:<version> -h Available options: --config-file <file> | configuration file path --no-config | do not try to configure TheHive (add secret and elasticsearch) --no-config-secret | do not add random secret to configuration --secret <secret> | secret to secure sessions --show-secret | show the generated secret --no-config-db | do not configure database automatically --cql-hostnames <host>,<host>,... | resolve these hostnames to find cassandra instances --cql-username <username> | username of cassandra database --cql-password <password> | password of cassandra database --no-cql-wait | don't wait for cassandra --bdb-directory <path> | location of local database, if cassandra is not used (default: /data/db) --index-backend | backend to use for index. One of 'lucene' or 'elasticsearch' (default: lucene) --es-hostnames | elasticsearch instances used for index --es-index | elasticsearch index name to used (default: thehive) --no-config-storage | do not configure storage automatically --storage-directory <path> | location of local storage, if s3 is not used (default: /data/files) --s3-endpoint <endpoint> | endpoint of s3 (or other object storage) if used, use 's3.amazonaws.com' for aws s3 --s3-region <region> | s3 region, optional for minio --s3-bucket <bucket> | name of the bucket to use (default: thehive), the bucket must already exists --s3-access-key <key> | s3 access key (required for s3) --s3-secret-key <key> | s3 secret key (required for s3) --s3-use-path-access-style | set this flag if you use minio or other non aws s3 provider, default to virtual host style --no-config-cortex | do not add Cortex configuration --cortex-proto <proto> | define protocol to connect to Cortex (default: http) --cortex-port <port> | define port to connect to Cortex (default: 9001) --cortex-hostnames <host>,<host>,... | resolve this hostname to find Cortex instances --cortex-keys <key>,<key>,... | define Cortex key --kubernetes | will use kubernetes api to join other nodes --kubernetes-pod-label-selector <selector> | selector to use to select other pods running the app (default app=thehive) --cluster-min-nodes-count <count> | minimum number of nodes to form a cluster (default to 1) migrate <param> <param> ... | run migration tool cloner <param> <param> ... | run cloner tool","title":"All options"},{"location":"thehive/setup/installation/docker/#using-docker-compose","text":"Below is an example of a docker-compose file. It's composed of several services: cassandra, elasticsearch, minio and TheHive. version : \"3\" services : thehive : image : strangebee/thehive:<version> depends_on : - cassandra - elasticsearch - minio mem_limit : 1500m ports : - \"9000:9000\" environment : - JVM_OPTS=\"-Xms1024M -Xmx1024M\" command : - --secret - \"mySecretForTheHive\" - \"--cql-hostnames\" - \"cassandra\" - \"--index-backend\" - \"elasticsearch\" - \"--es-hostnames\" - \"elasticsearch\" - \"--s3-endpoint\" - \"http://minio:9000\" - \"--s3-access-key\" - \"minioadmin\" - \"--s3-secret-key\" - \"minioadmin\" - \"--s3-use-path-access-style\" - \"--no-config-cortex\" cassandra : image : 'cassandra:4' mem_limit : 1000m ports : - \"9042:9042\" environment : - CASSANDRA_CLUSTER_NAME=TheHive volumes : - cassandradata:/var/lib/cassandra elasticsearch : image : docker.elastic.co/elasticsearch/elasticsearch:7.16.2 mem_limit : 512m ports : - \"9200:9200\" environment : - discovery.type=single-node - xpack.security.enabled=false volumes : - elasticsearchdata:/usr/share/elasticsearch/data minio : image : quay.io/minio/minio command : [ \"minio\" , \"server\" , \"/data\" , \"--console-address\" , \":9001\" ] environment : - MINIO_ROOT_USER=minioadmin - MINIO_ROOT_PASSWORD=minioadmin ports : - \"9001:9001\" volumes : - \"miniodata:/data\" volumes : miniodata : cassandradata : elasticsearchdata : It's highly recommended to change the default credentials and secrets.","title":"Using docker compose"},{"location":"thehive/setup/installation/docker/#usage-in-kubernetes","text":"Refer to the dedicated page for instuctions on how to deploy on kubernetes.","title":"Usage in kubernetes"},{"location":"thehive/setup/installation/kubernetes/","text":"Deploy on Kubernetes # The deployment on kubernetes uses the docker image, so refer to the docker image documentation for more information about its usage. Sample # Download this file to find a kubernetes configuration that will deploy on kubernetes: 1 instance of TheHive 1 instance of Cassandra 1 instance of Elasticsearch 1 instance Minio This setup is good for a try out of TheHive but you should adapt the data stores to be more robust (setup clustering and storage volumes). We invite you to check their documentation on how to deploy on kubernetes for production use. Warning The volumes used here are emptyDir s, so the data will be lost when a pod is restarted. You should update the volume description if you want to persist your data. To deploy more than one node, you will need to update your license. Only one node is included in the Community License. Start with kubectl apply -f kubernetes.yml . This will create a namespace thehive and deploy the instances in it. Cleanup # Delete all the resources belonging to the thehive namespace: kubectl delete namespace thehive Kubernetes configuration # In kubernetes with several TheHive pods, the application needs to form a cluster between its nodes. For this, it will use the akka discovery method using the kubernetes API . To enable this you need: A service account that can connect to the kubernetes API Tell TheHive to use kubernetes API to discover the other nodes RBAC # Create a ServiceAccount named thehive that can get the running pods --- # # Create a role, `pod-reader`, that can list pods and # bind the default service account in the namespace # that the binding is deployed to to that role. # kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : v1 kind : ServiceAccount metadata : name : thehive --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : read-pods subjects : - kind : ServiceAccount name : thehive roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io Deployment # In your pod / deployment specification, you need to specify the created service account. Also make sure to add a label and a POD_IP environment variable. metadata : labels : app : thehive spec : serviceAccountName : thehive containers : - name : thehive image : ... env : # Make sure that the container can know its own IP - name : POD_IP valueFrom : fieldRef : fieldPath : status.podIP Configuration # Using docker entrypoint # If you use the docker entry point, add the flag --kubernetes . You can also use the following options: --kubernetes-pod-label-selector <selector> | selector to use to select other pods running the app (default app=thehive) --cluster-min-nodes-count <count> | minimum number of nodes to form a cluster (default to 1) Using custom application.conf # If you use your own application.conf file, add the following: akka.remote.artery.canonical.hostname = ${?POD_IP} singleInstance = false akka.management { cluster.bootstrap { contact-point-discovery { discovery-method = kubernetes-api # Set the minimum number of pods to form a cluster required-contact-point-nr = 1 } } } akka.extensions += \"akka.management.cluster.bootstrap.ClusterBootstrap\" akka.discovery { kubernetes-api { # Set here the pod selector to use for thehive pods pod-label-selector = \"thehive\" } } Pod probes # You can use the following probes to make sure the application is started and running correctly. The first startup can be a bit slow so you may enable those probes after validating the correct start of the application. Tip When applying a big migration, it's recommended to deactivate those probes as the HTTP server will not start until the migration is done. startupProbe : httpGet : path : /api/v1/status/public port : 9000 failureThreshold : 30 periodSeconds : 10 livenessProbe : httpGet : path : /api/v1/status/public port : 9000 periodSeconds : 10 Troubleshooting # Error on database init # If your logs contain the lines: [error] o.t.s.m.Database [|] *********************************************************************** [error] o.t.s.m.Database [|] * Database initialisation has failed. Restart application to retry it * [error] o.t.s.m.Database [|] *********************************************************************** This means that an error occured when trying to create the database schema. Below those lines, you should get more details on the cause of the error. Possible root causes: Cassandra / Elasticsearch is unavailable: check that both database are correctly started and that TheHive can connect to them. You may try to first start both database in the kubernetes cluster before starting TheHive: set TheHive deployment to replicas: 0 to do so. Cassandra / ES contains invalid data. Elasticsearch has a role of index for Cassandra and the data between the two may not be in sync, causing errors when accessing the data. If it is the first time you are setting up the cluster, delete both database volumes/data and restart the databases and TheHive","title":"Deploy on Kubernetes"},{"location":"thehive/setup/installation/kubernetes/#deploy-on-kubernetes","text":"The deployment on kubernetes uses the docker image, so refer to the docker image documentation for more information about its usage.","title":"Deploy on Kubernetes"},{"location":"thehive/setup/installation/kubernetes/#sample","text":"Download this file to find a kubernetes configuration that will deploy on kubernetes: 1 instance of TheHive 1 instance of Cassandra 1 instance of Elasticsearch 1 instance Minio This setup is good for a try out of TheHive but you should adapt the data stores to be more robust (setup clustering and storage volumes). We invite you to check their documentation on how to deploy on kubernetes for production use. Warning The volumes used here are emptyDir s, so the data will be lost when a pod is restarted. You should update the volume description if you want to persist your data. To deploy more than one node, you will need to update your license. Only one node is included in the Community License. Start with kubectl apply -f kubernetes.yml . This will create a namespace thehive and deploy the instances in it.","title":"Sample"},{"location":"thehive/setup/installation/kubernetes/#cleanup","text":"Delete all the resources belonging to the thehive namespace: kubectl delete namespace thehive","title":"Cleanup"},{"location":"thehive/setup/installation/kubernetes/#kubernetes-configuration","text":"In kubernetes with several TheHive pods, the application needs to form a cluster between its nodes. For this, it will use the akka discovery method using the kubernetes API . To enable this you need: A service account that can connect to the kubernetes API Tell TheHive to use kubernetes API to discover the other nodes","title":"Kubernetes configuration"},{"location":"thehive/setup/installation/kubernetes/#rbac","text":"Create a ServiceAccount named thehive that can get the running pods --- # # Create a role, `pod-reader`, that can list pods and # bind the default service account in the namespace # that the binding is deployed to to that role. # kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] --- apiVersion : v1 kind : ServiceAccount metadata : name : thehive --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : read-pods subjects : - kind : ServiceAccount name : thehive roleRef : kind : Role name : pod-reader apiGroup : rbac.authorization.k8s.io","title":"RBAC"},{"location":"thehive/setup/installation/kubernetes/#deployment","text":"In your pod / deployment specification, you need to specify the created service account. Also make sure to add a label and a POD_IP environment variable. metadata : labels : app : thehive spec : serviceAccountName : thehive containers : - name : thehive image : ... env : # Make sure that the container can know its own IP - name : POD_IP valueFrom : fieldRef : fieldPath : status.podIP","title":"Deployment"},{"location":"thehive/setup/installation/kubernetes/#configuration","text":"","title":"Configuration"},{"location":"thehive/setup/installation/kubernetes/#using-docker-entrypoint","text":"If you use the docker entry point, add the flag --kubernetes . You can also use the following options: --kubernetes-pod-label-selector <selector> | selector to use to select other pods running the app (default app=thehive) --cluster-min-nodes-count <count> | minimum number of nodes to form a cluster (default to 1)","title":"Using docker entrypoint"},{"location":"thehive/setup/installation/kubernetes/#using-custom-applicationconf","text":"If you use your own application.conf file, add the following: akka.remote.artery.canonical.hostname = ${?POD_IP} singleInstance = false akka.management { cluster.bootstrap { contact-point-discovery { discovery-method = kubernetes-api # Set the minimum number of pods to form a cluster required-contact-point-nr = 1 } } } akka.extensions += \"akka.management.cluster.bootstrap.ClusterBootstrap\" akka.discovery { kubernetes-api { # Set here the pod selector to use for thehive pods pod-label-selector = \"thehive\" } }","title":"Using custom application.conf"},{"location":"thehive/setup/installation/kubernetes/#pod-probes","text":"You can use the following probes to make sure the application is started and running correctly. The first startup can be a bit slow so you may enable those probes after validating the correct start of the application. Tip When applying a big migration, it's recommended to deactivate those probes as the HTTP server will not start until the migration is done. startupProbe : httpGet : path : /api/v1/status/public port : 9000 failureThreshold : 30 periodSeconds : 10 livenessProbe : httpGet : path : /api/v1/status/public port : 9000 periodSeconds : 10","title":"Pod probes"},{"location":"thehive/setup/installation/kubernetes/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"thehive/setup/installation/kubernetes/#error-on-database-init","text":"If your logs contain the lines: [error] o.t.s.m.Database [|] *********************************************************************** [error] o.t.s.m.Database [|] * Database initialisation has failed. Restart application to retry it * [error] o.t.s.m.Database [|] *********************************************************************** This means that an error occured when trying to create the database schema. Below those lines, you should get more details on the cause of the error. Possible root causes: Cassandra / Elasticsearch is unavailable: check that both database are correctly started and that TheHive can connect to them. You may try to first start both database in the kubernetes cluster before starting TheHive: set TheHive deployment to replicas: 0 to do so. Cassandra / ES contains invalid data. Elasticsearch has a role of index for Cassandra and the data between the two may not be in sync, causing errors when accessing the data. If it is the first time you are setting up the cluster, delete both database volumes/data and restart the databases and TheHive","title":"Error on database init"},{"location":"thehive/setup/installation/license/","text":"Activate a license # Community # With the default license, TheHive is free to use with 2 users and 1 organisation. Enable a license # Trial , Gold or Platinum licenses unlock advanced features and a set of accounts and organisation included. Get a license # Request a license by contacting StrangeBee - https://www.strangebee.com or contact@strangebee.com . Enable a license # Tip Default admin credentials are admin@thehive.local / secret Connect as administrator and follow this guide: Copy the challenge and send it to StrangeBee Get back the corresponding license, enter in the UI and activate","title":"Activate a license"},{"location":"thehive/setup/installation/license/#activate-a-license","text":"","title":" Activate a license"},{"location":"thehive/setup/installation/license/#community","text":"With the default license, TheHive is free to use with 2 users and 1 organisation.","title":"Community"},{"location":"thehive/setup/installation/license/#enable-a-license","text":"Trial , Gold or Platinum licenses unlock advanced features and a set of accounts and organisation included.","title":"Enable a license"},{"location":"thehive/setup/installation/license/#get-a-license","text":"Request a license by contacting StrangeBee - https://www.strangebee.com or contact@strangebee.com .","title":"Get a license"},{"location":"thehive/setup/installation/license/#enable-a-license_1","text":"Tip Default admin credentials are admin@thehive.local / secret Connect as administrator and follow this guide: Copy the challenge and send it to StrangeBee Get back the corresponding license, enter in the UI and activate","title":"Enable a license"},{"location":"thehive/setup/installation/migration/","text":"Migration from TheHive 3.x # TheHive 5.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch. Supported versions # The migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Elasticsearch version TheHive 3.4.x v6.x TheHive 3.5.x v7.x How it works # All packages of TheHive v5.x distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate . Pre-requisite # In order to migrate the data: TheHive v5.x must be installed on the system running the migration tool; TheHive must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance. Configuration of TheHive # Warning In TheHive, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive v5.x comes with a default domain named thehive.local . Starting the migration without explicitly specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ). Run the migration # Prepare, install and configure your new instance of TheHive v5.x by following the associated guides . In order to obtain the best migration performance, it is recommended to not start TheHive 5 before running the migration tool. Thus, the migration will be done without index. At the end of the migration, the index will be added. Once TheHive 5 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Warning The configuration file must not contain cluster configuration ( akka.cluster ). The migration tool works on single node only. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive 3 configuration file -o, --output <file> TheHive 5 configuration file -d, --drop-database Drop TheHive 5 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -x, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads -k, --integrity-checks run integrity checks after the migration --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 5 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommend to not start the application during the migration. Continue an incomplete migration # If the migration has been stopped, or only part of the data has been migrated, it is possible to continue the migration by running the tool again with the parameter --resume . This parameter prevents creating data if it already exists. Merge several TheHive 3 data into one TheHive 5 # The migration tool can be run several times to migrate different TheHive 3 data into one TheHive 5 instance. The name of the target organisation can be changed for each execution. In order to prevent case number collision (if a case with the same number already exists, the new case won't be created), you can shift the case number with the parameter --case-number-shift . Using authentication on Cassandra # if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ; Migration logs # The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these numbers can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older will cause observable files not being imported in TheHive 5. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827 Starting TheHive # Once the migration process is successfully completed, TheHive can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Migration from TheHive 3.x"},{"location":"thehive/setup/installation/migration/#migration-from-thehive-3x","text":"TheHive 5.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch.","title":"Migration from TheHive 3.x"},{"location":"thehive/setup/installation/migration/#supported-versions","text":"The migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Elasticsearch version TheHive 3.4.x v6.x TheHive 3.5.x v7.x","title":"Supported versions"},{"location":"thehive/setup/installation/migration/#how-it-works","text":"All packages of TheHive v5.x distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate .","title":"How it works"},{"location":"thehive/setup/installation/migration/#pre-requisite","text":"In order to migrate the data: TheHive v5.x must be installed on the system running the migration tool; TheHive must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance.","title":"Pre-requisite"},{"location":"thehive/setup/installation/migration/#configuration-of-thehive","text":"Warning In TheHive, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive v5.x comes with a default domain named thehive.local . Starting the migration without explicitly specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ).","title":"Configuration of TheHive"},{"location":"thehive/setup/installation/migration/#run-the-migration","text":"Prepare, install and configure your new instance of TheHive v5.x by following the associated guides . In order to obtain the best migration performance, it is recommended to not start TheHive 5 before running the migration tool. Thus, the migration will be done without index. At the end of the migration, the index will be added. Once TheHive 5 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Warning The configuration file must not contain cluster configuration ( akka.cluster ). The migration tool works on single node only. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive 3 configuration file -o, --output <file> TheHive 5 configuration file -d, --drop-database Drop TheHive 5 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -x, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads -k, --integrity-checks run integrity checks after the migration --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 5 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommend to not start the application during the migration.","title":"Run the migration"},{"location":"thehive/setup/installation/migration/#continue-an-incomplete-migration","text":"If the migration has been stopped, or only part of the data has been migrated, it is possible to continue the migration by running the tool again with the parameter --resume . This parameter prevents creating data if it already exists.","title":"Continue an incomplete migration"},{"location":"thehive/setup/installation/migration/#merge-several-thehive-3-data-into-one-thehive-5","text":"The migration tool can be run several times to migrate different TheHive 3 data into one TheHive 5 instance. The name of the target organisation can be changed for each execution. In order to prevent case number collision (if a case with the same number already exists, the new case won't be created), you can shift the case number with the parameter --case-number-shift .","title":"Merge several TheHive 3 data into one TheHive 5"},{"location":"thehive/setup/installation/migration/#using-authentication-on-cassandra","text":"if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ;","title":"Using authentication on Cassandra"},{"location":"thehive/setup/installation/migration/#migration-logs","text":"The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these numbers can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older will cause observable files not being imported in TheHive 5. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827","title":"Migration logs"},{"location":"thehive/setup/installation/migration/#starting-thehive","text":"Once the migration process is successfully completed, TheHive can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Starting TheHive"},{"location":"thehive/setup/installation/minio/","text":"","title":"Minio"},{"location":"thehive/setup/installation/step-by-step-guide/","text":"Step-by-Step guide # This page is a step by step installation and configuration guide to get an instance of TheHive up and running. This guide is illustrated with examples for DEB and RPM packages based systems and for installation from binary packages. This guide describes the installation of a new instance of TheHive only Java Virtual Machine # DEB RPM Other sudo apt install openjdk-11-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-11-openjdk-amd64\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-11-openjdk-amd64\" sudo update-java-alternatives --jre-headless -s java-1.11.0-openjdk-amd64 sudo yum install java-11-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-11-openjdk\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-11-openjdk\" The installation requires Java 11, so refer to your system documentation to install it. Apache Cassandra # Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 4.0.x of Cassandra. Upgrading from Cassandra 3.x If you are upgrading from Cassandra 3.x, please follow the dedicated guide . This part is relevant for fresh installation only. Installation # DEB RPM Other Add Apache repository references wget -qO - https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor -o /usr/share/keyrings/cassandra-archive.gpg echo \"deb [signed-by=/usr/share/keyrings/cassandra-archive.gpg] https://downloads.apache.org/cassandra/debian 40x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra Add Cassandra repository keys rpm --import https://downloads.apache.org/cassandra/KEYS Add the Apache repository for Cassandra to /etc/yum.repos.d/cassandra.repo /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/40x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package sudo yum install cassandra Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra . Configuration # Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. /etc/cassandra/cassandra.yaml # content from /etc/cassandra/cassandra.yaml [ .. ] cluster_name : 'thp' listen_address : 'xx.xx.xx.xx' # address for nodes rpc_address : 'xx.xx.xx.xx' # address for clients seed_provider : - class_name : org.apache.cassandra.locator.SimpleSeedProvider parameters : # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds : 'xx.xx.xx.xx' # self for the first node data_file_directories : - '/var/lib/cassandra/data' commitlog_directory : '/var/lib/cassandra/commitlog' saved_caches_directory : '/var/lib/cassandra/saved_caches' hints_directory : - '/var/lib/cassandra/hints' [ .. ] Start the service # DEB RPM sudo systemctl start cassandra Remove existing data before starting With the DEB packages, Cassandra service could start automatically before configuring it: Stop it, remove the data and restart once the configuration is updated: sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/* Run the service and ensure it restart after a reboot: sudo systemctl daemon-reload sudo service cassandra start sudo systemctl enable cassandra By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client). Additional configuration # Standalone server ONLY: disable tombstones If you are installing a standalone server, tombstones can be disabled. Check gc_grace_seconds value cqlsh -u cassandra <IP ADDRESS> -e \"SELECT table_name,gc_grace_seconds FROM system_schema.tables WHERE keyspace_name='thehive'\" Results should look like this: table_name | gc_grace_seconds -------------------------+------------------ edgestore | 864000 edgestore_lock_ | 864000 graphindex | 864000 graphindex_lock_ | 864000 janusgraph_ids | 864000 system_properties | 864000 system_properties_lock_ | 864000 systemlog | 864000 txlog | 864000 Disable by setting gc_grace_seconds to 0. Use this command line: for TABLE in edgestore edgestore_lock_ graphindex graphindex_lock_ janusgraph_ids system_properties system_properties_lock_ systemlog txlog do cqlsh -u cassandra -e \"ALTER TABLE thehive. ${ TABLE } WITH gc_grace_seconds = 0;\" done Check changes has been taken into account, by running this command again: cqlsh -u cassandra <IP ADDRESS> -e \"SELECT table_name,gc_grace_seconds FROM system_schema.tables WHERE keyspace_name='thehive'\" Results should look like this: table_name | gc_grace_seconds -------------------------+------------------ edgestore | 0 edgestore_lock_ | 0 graphindex | 0 graphindex_lock_ | 0 janusgraph_ids | 0 system_properties | 0 system_properties_lock_ | 0 systemlog | 0 txlog | 0 For additional configuration options, refer to: Cassandra documentation page Datastax documentation page Elasticsearch # TheHive requires Elasticsearch to manage data indices. Elasticsearch 7.x only is supported Installation # DEB RPM Other Add Elasticsearch repository keys wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg sudo apt-get install apt-transport-https Add the DEB repository of Elasticsearch echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list Install the package sudo apt update sudo apt install elasticsearch Add Elasticsearch repository references rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Add the RPM repository of Elasticsearch to /etc/yum.repos.d/elasticsearch.repo /etc/yum.repos.d/elasticsearch.repo [ elasticsearch ] name = Elasticsearch repository for 7 .x packages baseurl = https://artifacts.elastic.co/packages/7.x/yum gpgcheck = 1 gpgkey = https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled = 0 autorefresh = 1 type = rpm-md Install the package sudo yum install --enablerepo = elasticsearch elasticsearch References https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. Configuration # /etc/elasticsearch/elasticsearch.yml Elasticsearch configuration should contain the following lines: /etc/elasticsearch/elasticsearch.yml http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.search.queue_size : 100000 path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" xpack.security.enabled : false script.allowed_types : \"inline,stored\" Note Indexes will be created at the first start of TheHive. It can take few time Like data and files, indexes should be part of the backup policy Indexes can removed and created again Custom JVM options add the file /etc/elasticsearch/jvm.options.d/jvm.options with following lines: -Dlog4j2.formatMsgNoLookups=true -Xms4g -Xmx4g This can be updated according the amount of memory available Sart the service # DEB RPM sudo systemctl start elasticsearch Remove existing data before starting With the DEB packages, Elastic service could start automatically before configuring it: Stop it, remove the data and restart once the configuration is updated: sudo systemctl stop elasticsearch sudo rm -rf /var/lib/elasticsearch/* Run the service and ensure it restart after a reboot: sudo systemctl daemon-reload sudo service elasticsearch start sudo systemctl enable elasticsearch File storage # For standalone production and test servers, we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using NFS or S3 services; see the related guide for more details and an example with MinIO servers. Local Filesystem S3 with Min.io To store files on the local filesystem, start by choosing the dedicated folder (by default /opt/thp/thehive/files ): sudo mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files An example of installing, configuring and use Min.IO is detailed in this documentation . TheHive # This part contains instructions to install TheHive and then configure it. Installation # All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . DEB RPM wget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg sudo rpm --import https://archives.strangebee.com/keys/strangebee.gpg Install TheHive package by using the following commands: DEB RPM Other echo 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list sudo apt-get update sudo apt-get install -y thehive Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/strangebee.repo : /etc/yum.repos.d/strangebee.repo [ thehive ] enabled = 1 priority = 1 name = StrangeBee RPM repository baseurl = https://rpm.strangebee.com/thehive-5.x/noarch gpgkey = https://archives.strangebee.com/keys/strangebee.gpg gpgcheck = 1 Then install the package using yum : sudo yum install thehive Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://archives.strangebee.com/zip/thehive-latest.zip unzip thehive-latest.zip sudo ln -s thehive-x.x.x thehive Prepare the system. It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . sudo addgroup thehive sudo adduser --system thehive sudo chown -R thehive:thehive /opt/thehive sudo mkdir /etc/thehive sudo touch /etc/thehive/application.conf sudo chown root:thehive /etc/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service sudo cp thehive.service /etc/systemd/system/thehive.service Configuration # The configuration that comes with binary packages is ready for a standalone installation, everything on the same server. In this context, and at this stage, you might need to set the following parameters accordingly: /etc/thehive/application.conf [ .. ] # Service configuration application.baseUrl = \"http://localhost:9000\" # (1) play.http.context = \"/\" # (2) [ .. ] specify the scheme, hostname and port used to join the application specify if you use a custom path ( / by default) Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration Secret key configuration # Debian RPM Other The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Database & index # By default, TheHive is configured to connect to Cassandra and Elasticsearch databases installed locally. /etc/thehive/application.conf # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } File storage # By default, TheHive is configured to store files locally in /opt/thp/thehive/files . Local filesystem S3 If you chose to store files on the local filesystem: Ensure thehive user has permissions on the destination folder chown -R thehive:thehive /opt/thp/thehive/files Default values in the configuration file /etc/thehive/application.conf # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default: thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) /etc/thehive/application.conf ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force Cortex & MISP # By default the configuration file coming with packages contains following lines, enabling Cortex and MISP modules. If you are not using one them, you can comment the related line and restart the service. /etc/thehive/application.conf # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # ommenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule Run # sudo systemctl start thehive sudo systemctl enable thehive Please consider the service may take a while at the first start Once it has started, open your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password. Advanced configuration # For additional configuration options, please refer to the Configuration Guides . Usage & Licenses # By default, TheHive comes with no license token and let everyone use the application with 2 users and 1 organisation: this is the community version. To unlock advanced features, contact StrangeBee to get a license - https://wwww.strangebee.com / contact@strangebee.com Activate a license # Follow this guide: Activate a license .","title":"Step-by-Step guide"},{"location":"thehive/setup/installation/step-by-step-guide/#step-by-step-guide","text":"This page is a step by step installation and configuration guide to get an instance of TheHive up and running. This guide is illustrated with examples for DEB and RPM packages based systems and for installation from binary packages. This guide describes the installation of a new instance of TheHive only","title":"Step-by-Step guide"},{"location":"thehive/setup/installation/step-by-step-guide/#java-virtual-machine","text":"DEB RPM Other sudo apt install openjdk-11-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-11-openjdk-amd64\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-11-openjdk-amd64\" sudo update-java-alternatives --jre-headless -s java-1.11.0-openjdk-amd64 sudo yum install java-11-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-11-openjdk\" | sudo tee -a /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-11-openjdk\" The installation requires Java 11, so refer to your system documentation to install it.","title":" Java Virtual Machine"},{"location":"thehive/setup/installation/step-by-step-guide/#apache-cassandra","text":"Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 4.0.x of Cassandra. Upgrading from Cassandra 3.x If you are upgrading from Cassandra 3.x, please follow the dedicated guide . This part is relevant for fresh installation only.","title":"  Apache Cassandra"},{"location":"thehive/setup/installation/step-by-step-guide/#installation","text":"DEB RPM Other Add Apache repository references wget -qO - https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor -o /usr/share/keyrings/cassandra-archive.gpg echo \"deb [signed-by=/usr/share/keyrings/cassandra-archive.gpg] https://downloads.apache.org/cassandra/debian 40x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra Add Cassandra repository keys rpm --import https://downloads.apache.org/cassandra/KEYS Add the Apache repository for Cassandra to /etc/yum.repos.d/cassandra.repo /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/40x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package sudo yum install cassandra Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra .","title":"Installation"},{"location":"thehive/setup/installation/step-by-step-guide/#configuration","text":"Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. /etc/cassandra/cassandra.yaml # content from /etc/cassandra/cassandra.yaml [ .. ] cluster_name : 'thp' listen_address : 'xx.xx.xx.xx' # address for nodes rpc_address : 'xx.xx.xx.xx' # address for clients seed_provider : - class_name : org.apache.cassandra.locator.SimpleSeedProvider parameters : # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds : 'xx.xx.xx.xx' # self for the first node data_file_directories : - '/var/lib/cassandra/data' commitlog_directory : '/var/lib/cassandra/commitlog' saved_caches_directory : '/var/lib/cassandra/saved_caches' hints_directory : - '/var/lib/cassandra/hints' [ .. ]","title":"Configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#start-the-service","text":"DEB RPM sudo systemctl start cassandra Remove existing data before starting With the DEB packages, Cassandra service could start automatically before configuring it: Stop it, remove the data and restart once the configuration is updated: sudo systemctl stop cassandra sudo rm -rf /var/lib/cassandra/* Run the service and ensure it restart after a reboot: sudo systemctl daemon-reload sudo service cassandra start sudo systemctl enable cassandra By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client).","title":"Start the service"},{"location":"thehive/setup/installation/step-by-step-guide/#additional-configuration","text":"Standalone server ONLY: disable tombstones If you are installing a standalone server, tombstones can be disabled. Check gc_grace_seconds value cqlsh -u cassandra <IP ADDRESS> -e \"SELECT table_name,gc_grace_seconds FROM system_schema.tables WHERE keyspace_name='thehive'\" Results should look like this: table_name | gc_grace_seconds -------------------------+------------------ edgestore | 864000 edgestore_lock_ | 864000 graphindex | 864000 graphindex_lock_ | 864000 janusgraph_ids | 864000 system_properties | 864000 system_properties_lock_ | 864000 systemlog | 864000 txlog | 864000 Disable by setting gc_grace_seconds to 0. Use this command line: for TABLE in edgestore edgestore_lock_ graphindex graphindex_lock_ janusgraph_ids system_properties system_properties_lock_ systemlog txlog do cqlsh -u cassandra -e \"ALTER TABLE thehive. ${ TABLE } WITH gc_grace_seconds = 0;\" done Check changes has been taken into account, by running this command again: cqlsh -u cassandra <IP ADDRESS> -e \"SELECT table_name,gc_grace_seconds FROM system_schema.tables WHERE keyspace_name='thehive'\" Results should look like this: table_name | gc_grace_seconds -------------------------+------------------ edgestore | 0 edgestore_lock_ | 0 graphindex | 0 graphindex_lock_ | 0 janusgraph_ids | 0 system_properties | 0 system_properties_lock_ | 0 systemlog | 0 txlog | 0 For additional configuration options, refer to: Cassandra documentation page Datastax documentation page","title":"Additional configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#elasticsearch","text":"TheHive requires Elasticsearch to manage data indices. Elasticsearch 7.x only is supported","title":" Elasticsearch"},{"location":"thehive/setup/installation/step-by-step-guide/#installation_1","text":"DEB RPM Other Add Elasticsearch repository keys wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg sudo apt-get install apt-transport-https Add the DEB repository of Elasticsearch echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list Install the package sudo apt update sudo apt install elasticsearch Add Elasticsearch repository references rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Add the RPM repository of Elasticsearch to /etc/yum.repos.d/elasticsearch.repo /etc/yum.repos.d/elasticsearch.repo [ elasticsearch ] name = Elasticsearch repository for 7 .x packages baseurl = https://artifacts.elastic.co/packages/7.x/yum gpgcheck = 1 gpgkey = https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled = 0 autorefresh = 1 type = rpm-md Install the package sudo yum install --enablerepo = elasticsearch elasticsearch References https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice.","title":"Installation"},{"location":"thehive/setup/installation/step-by-step-guide/#configuration_1","text":"/etc/elasticsearch/elasticsearch.yml Elasticsearch configuration should contain the following lines: /etc/elasticsearch/elasticsearch.yml http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.search.queue_size : 100000 path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" xpack.security.enabled : false script.allowed_types : \"inline,stored\" Note Indexes will be created at the first start of TheHive. It can take few time Like data and files, indexes should be part of the backup policy Indexes can removed and created again Custom JVM options add the file /etc/elasticsearch/jvm.options.d/jvm.options with following lines: -Dlog4j2.formatMsgNoLookups=true -Xms4g -Xmx4g This can be updated according the amount of memory available","title":"Configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#sart-the-service","text":"DEB RPM sudo systemctl start elasticsearch Remove existing data before starting With the DEB packages, Elastic service could start automatically before configuring it: Stop it, remove the data and restart once the configuration is updated: sudo systemctl stop elasticsearch sudo rm -rf /var/lib/elasticsearch/* Run the service and ensure it restart after a reboot: sudo systemctl daemon-reload sudo service elasticsearch start sudo systemctl enable elasticsearch","title":"Sart the service"},{"location":"thehive/setup/installation/step-by-step-guide/#file-storage","text":"For standalone production and test servers, we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using NFS or S3 services; see the related guide for more details and an example with MinIO servers. Local Filesystem S3 with Min.io To store files on the local filesystem, start by choosing the dedicated folder (by default /opt/thp/thehive/files ): sudo mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files An example of installing, configuring and use Min.IO is detailed in this documentation .","title":" File storage"},{"location":"thehive/setup/installation/step-by-step-guide/#thehive","text":"This part contains instructions to install TheHive and then configure it.","title":" TheHive"},{"location":"thehive/setup/installation/step-by-step-guide/#installation_2","text":"All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . DEB RPM wget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg sudo rpm --import https://archives.strangebee.com/keys/strangebee.gpg Install TheHive package by using the following commands: DEB RPM Other echo 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list sudo apt-get update sudo apt-get install -y thehive Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/strangebee.repo : /etc/yum.repos.d/strangebee.repo [ thehive ] enabled = 1 priority = 1 name = StrangeBee RPM repository baseurl = https://rpm.strangebee.com/thehive-5.x/noarch gpgkey = https://archives.strangebee.com/keys/strangebee.gpg gpgcheck = 1 Then install the package using yum : sudo yum install thehive Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://archives.strangebee.com/zip/thehive-latest.zip unzip thehive-latest.zip sudo ln -s thehive-x.x.x thehive Prepare the system. It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . sudo addgroup thehive sudo adduser --system thehive sudo chown -R thehive:thehive /opt/thehive sudo mkdir /etc/thehive sudo touch /etc/thehive/application.conf sudo chown root:thehive /etc/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service sudo cp thehive.service /etc/systemd/system/thehive.service","title":"Installation"},{"location":"thehive/setup/installation/step-by-step-guide/#configuration_2","text":"The configuration that comes with binary packages is ready for a standalone installation, everything on the same server. In this context, and at this stage, you might need to set the following parameters accordingly: /etc/thehive/application.conf [ .. ] # Service configuration application.baseUrl = \"http://localhost:9000\" # (1) play.http.context = \"/\" # (2) [ .. ] specify the scheme, hostname and port used to join the application specify if you use a custom path ( / by default) Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration","title":"Configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#secret-key-configuration","text":"Debian RPM Other The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_","title":"Secret key configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#database-index","text":"By default, TheHive is configured to connect to Cassandra and Elasticsearch databases installed locally. /etc/thehive/application.conf # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } }","title":"Database &amp; index"},{"location":"thehive/setup/installation/step-by-step-guide/#file-storage_1","text":"By default, TheHive is configured to store files locally in /opt/thp/thehive/files . Local filesystem S3 If you chose to store files on the local filesystem: Ensure thehive user has permissions on the destination folder chown -R thehive:thehive /opt/thp/thehive/files Default values in the configuration file /etc/thehive/application.conf # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default: thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) /etc/thehive/application.conf ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force","title":"File storage"},{"location":"thehive/setup/installation/step-by-step-guide/#cortex-misp","text":"By default the configuration file coming with packages contains following lines, enabling Cortex and MISP modules. If you are not using one them, you can comment the related line and restart the service. /etc/thehive/application.conf # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # ommenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule","title":"Cortex &amp; MISP"},{"location":"thehive/setup/installation/step-by-step-guide/#run","text":"sudo systemctl start thehive sudo systemctl enable thehive Please consider the service may take a while at the first start Once it has started, open your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password.","title":"Run"},{"location":"thehive/setup/installation/step-by-step-guide/#advanced-configuration","text":"For additional configuration options, please refer to the Configuration Guides .","title":"Advanced configuration"},{"location":"thehive/setup/installation/step-by-step-guide/#usage-licenses","text":"By default, TheHive comes with no license token and let everyone use the application with 2 users and 1 organisation: this is the community version. To unlock advanced features, contact StrangeBee to get a license - https://wwww.strangebee.com / contact@strangebee.com","title":"Usage &amp; Licenses"},{"location":"thehive/setup/installation/step-by-step-guide/#activate-a-license","text":"Follow this guide: Activate a license .","title":"Activate a license"},{"location":"thehive/setup/installation/upgrade-cluster/","text":"Coming soon!","title":"Upgrade cluster"},{"location":"thehive/setup/installation/upgrade-from-4.x/","text":"Upgrade from TheHive 4.x # Info This guide describes how to upgrade TheHive from version 4.1.x to 5.0.x . This guide considers: The application is running on a supported Linux operating system The server meets prerequisites regarding CPU & RAM. If you are using a cluster, specific notes are highlighted to guide you. Switch to Elasticsearch as indexing engine TheHive 5.x uses Elasticsearch as indexing engine. If you used Lucene as indexing engine with TheHive 4.1.x, reindexing the data is mandatory. It might take some time regarding the size of your database. Preparation # I'm using a cluster Follow these instructions for all nodes of the cluster. The database application will be upgraded during the migration. We highly recommend making backups of the database, index and files before running the operation. FAQ Q: How to make backups ? A: Read our backup and restore guide Attention Make sure that you can login as an admin user with a password in TheHive database (the local auth provider should be enabled, by default it is enabled). Stop all running applications # Start by stopping TheHive: stop thehive service sudo systemctl stop thehive Once TheHive is successfully stopped, stop database service stop cassandra service sudo systemctl stop cassandra Only if already using Elasticsearch as indexing engine stop elasticsearch service sudo systemctl stop elasticsearch Upgrade Java # I'm using a cluster Follow these instructions for all nodes of the cluster. Follow the installation process to install the required version. Upgrade or install Elasticsearch # I'm using a cluster Elasticsearch was mandatory for cluster of TheHive 5.x. Unless an update might is necessary, you can go to Upgrade Cassandra . Follow the installation process to install and configure the required version. Upgrade Cassandra # I'm using a cluster Follow this part for all nodes of the Cassandra cluster, and ensure to restart sucessfully all nodes of the cluster before upgrading all nodes of TheHive cluster to version 5. Backup configuration file # Save the existing configuration file for Cassandra 3.x. It will be used later to configure Cassandra 4: sudo cp /etc/cassandra/cassandra.yaml /etc/cassandra/cassandra3.yaml.bak Install Cassandra # Follow the installation process to install the required version. During the installation process, replace existing configuration files ( as the old configuration is saved ): Configuration file '/etc/cassandra/cassandra.yaml' ==> Modified (by you or by a script) since installation. ==> Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version. *** cassandra.yaml (Y/I/N/O/D/Z) [default=N] ? Y Configuration # Update the new configuration file, and ensure the following parameters are correctly set with these values: cluster_name: 'thp' num_tokens: 256 Info If you had a more customized configuration file for Cassandra 3.x, review all the file and ensure to adapt it accordingly. Start the service # sudo systemctl start cassandra Upgrade sstables # On each Cassandra nodes, upgrade the sstables: nodetool upgradesstables Then repair the keyspaces: nodetool repair --full Install thehive # I'm using a cluster Before starting this part of the guide, ensure your Cassandra cluster is fully operational, by running the command nodetool status nodetool status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.1.1.2 1.41 GiB 256 100.0% ba6daa4e-6d14-4b21-a06c-d01b3bdd659d rack1 UN 10.1.1.3 1.39 GiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444043eb1cd rack1 UN 10.1.1.4 1.36 GiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252aeb86 rack1 Then, we recommend stopping all existing nodes of TheHive (4.x) and then, upgrading and starting only one node to TheHive 5. When everything works fine with this node, Other nodes can be updated and started. Prepare for the new installation # TheHive configuration file: /etc/thehive/application.conf Starting with TheHive 5.0.0, configuration has been simplified; most of the administration parameters can be configured directly in the UI. The configuration file ( /etc/thehive/application.conf ) may only contain the necessary information to start the application successfully; which are: Secret Database Indexing engine File storage Enabled connectors Akka configuration in the case of a cluster Authentication, Webhooks, Cortex and MISP configurations can be set in the UI. Note Cortex and Misp connector module keys were renamed from play.modules.enabled to scalligraph.modules Standalone server Cluster Save your current configuration file: sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak For the current use case, i.e., a standalone server, the final configuration file should look like this: sample of /etc/thehive/application.conf # TheHive configuration - application.conf # # # This is the default configuration file. # This is prepared to run with all services locally: # - Cassandra for the database # - Elasticsearch for index engine # - File storage is local in /opt/thp/thehive/files # # If this is not your setup, please refer to the documentation at: # https://docs.thehive-project.org/thehive/ # # # Secret key - used by Play Framework # If TheHive is installed with DEB/RPM package, this is automatically generated # If TheHive is not installed from DEB or RPM packages run the following # command before starting thehive: # cat > /etc/thehive/secret.conf << _EOF_ # play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |# head -n 1)\" # _EOF_ include \"/etc/thehive/secret.conf\" # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default : thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } # Define the maximum size for an attachment accepted by TheHive play.http.parser.maxDiskBuffer = 1GB # Define maximum size of http request (except attachment) play.http.parser.maxMemoryBuffer = 10M # Service configuration application.baseUrl = \"http://localhost:9000\" play.http.context = \"/\" # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # commenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule Save your current configuration file: sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak For the current use case, i.e., a standalone server, the final configuration file should look like this: sample of /etc/thehive/application.conf # TheHive configuration - application.conf # # # This is the default configuration file. # This is prepared to run with all services locally: # - Cassandra for the database # - Elasticsearch for index engine # - File storage is local in /opt/thp/thehive/files # # If this is not your setup, please refer to the documentation at: # https://docs.thehive-project.org/thehive/ # # # Secret key - used by Play Framework # If TheHive is installed with DEB/RPM package, this is automatically generated # If TheHive is not installed from DEB or RPM packages run the following # command before starting thehive: # cat > /etc/thehive/secret.conf << _EOF_ # play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |# head -n 1)\" # _EOF_ include \"/etc/thehive/secret.conf\" # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default : thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } # Define the maximum size for an attachment accepted by TheHive play.http.parser.maxDiskBuffer = 1GB # Define maximum size of http request (except attachment) play.http.parser.maxMemoryBuffer = 10M # Service configuration application.baseUrl = \"http://localhost:9000\" play.http.context = \"/\" # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # commenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule # Cluster configuration akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } ## seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Note By default, Cortex and MISP modules are enabled. If you won't use them or one of them, the corresponding line can be commented. Our recommendation : use the default configuration sample, update it with your custom-parameter values, and keep the old file to configure services in the web UI. Specific configuration required (for the upgrade only) # I'm using a cluster This part only concerns the first node , the one that will be started to perform the database and index upgrade. These lines should be added to the configuration file only while upgrading to version 5, and removed later on. db.janusgraph.forceDropAndRebuildIndex = true Install TheHive # I'm using a cluster Follow these instructions for the first node of TheHive. When starting TheHive 5.x for the first time, ensure all nodes of database clusters are up and running correctly. Once the upgrade is successful with the first node, install and start TheHive on other nodes. I'm using DEB packages I'm using RPM packages Update the repository address wget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg sudo rm /etc/apt/sources.list.d/thehive-project.list ; echo 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list Install new package - old package of thehive4 will be removed sudo apt update sudo apt install thehive Add Cassandra repository keys sudo rpm --import https://archives.strangebee.com/keys/strangebee.gpg Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/strangebee.repo : /etc/yum.repos.d/strangebee.repo [ thehive ] enabled = 1 priority = 1 name = StrangeBee RPM repository baseurl = https://rpm.strangebee.com/thehive-5.x/noarch gpgkey = https://archives.strangebee.com/keys/strangebee.gpg gpgcheck = 1 Then install the package using yum : sudo yum install thehive During the installation, if you already prepared your configuration file during Prepare for the new installation chapter, continue without updating it with the maintainer's version. Configuration file '/etc/thehive/application.conf' ==> Modified (by you or by a script) since installation. ==> Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version. *** application.conf (Y/I/N/O/D/Z) [default=N] ? N Start services # sudo systemctl daemon-reload Start Cassandra (if not already started) sudo systemctl start cassandra Start Elasticsearch (if not already started) sudo systemctl start elasticsearch Once both database services are started successfully, start TheHive sudo systemctl start thehive The first start of TheHive 5.0.x can take some time When starting for the first time, TheHive is updating first the database schema, and proceed to reindexation. Both processes can take a certain time depending on the size of the database and the amount of data. Progression can be followed in log file /var/log/thehive/application.log . See Troubleshooting for more information. Restart the service # Once the service is started successfully, update the configuration file and remove the following lines: /etc/thehive/application.conf db.janusgraph.forceDropAndRebuildIndex = true Then restart TheHive: sudo systemctl restart thehive I'm using a cluster You can install and start TheHive on all other nodes. Troubleshooting # During the update, few logs can be seen in TheHive application.log file. Example of logs and what they mean [INFO] from org.janusgraph.graphdb.database.management.GraphIndexStatusWatcher in application-akka.actor.default-dispatcher-11 [|] Some key(s) on index global2 do not currently have status(es) [REGISTERED, ENABLED]: dateValue=INSTALLED,externalLink=INSTALLED,origin=INSTALLED,patternId=INSTALLED,revoked=INSTALLED,mandatory=INSTALLED,content=INSTALLED,isAttachment=INSTALLED,writable=INSTALLED,tactic=INSTALLED,stringValue=INSTALLED,owningOrganisation=INSTALLED,permissions=INSTALLED,actionRequired=INSTALLED,integerValue=INSTALLED,details=INSTALLED,locked=INSTALLED,slug=INSTALLED,cortexId=INSTALLED,owner=INSTALLED,workerId=INSTALLED,apikey=INSTALLED,level=INSTALLED,floatValue=INSTALLED,version=INSTALLED,occurDate=INSTALLED,url=INSTALLED,report=INSTALLED,tactics=INSTALLED,booleanValue=INSTALLED,cortexJobId=INSTALLED,category=INSTALLED,workerName=INSTALLED TheHive install indexes of the new schema in the database [INFO] from org.janusgraph.graphdb.olap.job.IndexRepairJob in Thread-97 [|] Index global2 metrics: success-tx: 1 doc-updates: 100 succeeded: 100 TheHive reindexes all data * UPDATE SCHEMA OF thehive-enterprise (1): Create initial values [INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [d471d8b643d17b6d|d88fe62679b77ab1] Adding initial values for GDPRDummy [..] [INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [|] Update graph in progress (100): Add pap and ignoreSimilarity to observables Migrating data from v4. to v5 [WARN] from org.thp.thehive.enterprise.services.LicenseSrv in main [ef39c95eaa6de532|0ccf187e40a4cd34] No license found No license found. This is a normal behavior during the upgrade from versions 4 to 5 INFO] from play.core.server.AkkaHttpServer in main [|] Listening for HTTP on /0:0:0:0:0:0:0:0:9000 The service is available. Users/Administrators can log in [INFO] from org.thp.thehive.connector.cortex.services.CortexDataImportActor in application-akka.actor.default-dispatcher-16 [|] Analyzer templates already present (found 203), skipping [..] [INFO] from org.thp.thehive.services.ttp.PatternImportActor in application-akka.actor.default-dispatcher-14 [|] Import finished, 707 patterns imported Few operations are processed after making the service available, like installing MITRE Enterprise ATT&CK patterns catalog or Analyzers templates. [ERROR] from org.janusgraph.diskstorage.log.util.ProcessMessageJob in pool-22-thread-1 [|] Encountered exception when processing message [Message@2022-03-24T16:50:40.655134Z:7f0001017672-ubuntu2=0x809F9F0568850528850550850558850570850600850610850618850650850668850710850738850758850760850808850900850910850A60850A70850A78850B00850B08853520853B3885150E8941608541688541788542088542688542708581] by reader [org.janusgraph.graphdb.database.management.ManagementLogger@3e1a6eae]:java.lang.IllegalStateException: Cannot access element because its enclosing transaction is closed and unbound at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.getNextTx(StandardJanusGraphTx.java:380) at org.janusgraph.graphdb.vertices.AbstractVertex.it(AbstractVertex.java:61) at org.janusgraph.graphdb.relations.CacheVertexProperty.<init>(CacheVertexProperty.java:38) at org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:88) at org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:71) at org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:57) at org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:45) at org.janusgraph.graphdb.types.vertices.JanusGraphSchemaVertex.getDefinition(JanusGraphSchemaVertex.java:94) at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.expireSchemaElement(StandardJanusGraphTx.java:1599) at org.janusgraph.graphdb.database.management.ManagementLogger.read(ManagementLogger.java:97) at org.janusgraph.diskstorage.log.util.ProcessMessageJob.run(ProcessMessageJob.java:46) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) During indexing, Janusgraph may display this message, this error is coming from a bug in janusgraph , don't mind it as the indexing will continue normally. This will have no impact on TheHive","title":"Upgrade from TheHive 4.x"},{"location":"thehive/setup/installation/upgrade-from-4.x/#upgrade-from-thehive-4x","text":"Info This guide describes how to upgrade TheHive from version 4.1.x to 5.0.x . This guide considers: The application is running on a supported Linux operating system The server meets prerequisites regarding CPU & RAM. If you are using a cluster, specific notes are highlighted to guide you. Switch to Elasticsearch as indexing engine TheHive 5.x uses Elasticsearch as indexing engine. If you used Lucene as indexing engine with TheHive 4.1.x, reindexing the data is mandatory. It might take some time regarding the size of your database.","title":"Upgrade from TheHive 4.x"},{"location":"thehive/setup/installation/upgrade-from-4.x/#preparation","text":"I'm using a cluster Follow these instructions for all nodes of the cluster. The database application will be upgraded during the migration. We highly recommend making backups of the database, index and files before running the operation. FAQ Q: How to make backups ? A: Read our backup and restore guide Attention Make sure that you can login as an admin user with a password in TheHive database (the local auth provider should be enabled, by default it is enabled).","title":"Preparation"},{"location":"thehive/setup/installation/upgrade-from-4.x/#stop-all-running-applications","text":"Start by stopping TheHive: stop thehive service sudo systemctl stop thehive Once TheHive is successfully stopped, stop database service stop cassandra service sudo systemctl stop cassandra Only if already using Elasticsearch as indexing engine stop elasticsearch service sudo systemctl stop elasticsearch","title":"Stop all running applications"},{"location":"thehive/setup/installation/upgrade-from-4.x/#upgrade-java","text":"I'm using a cluster Follow these instructions for all nodes of the cluster. Follow the installation process to install the required version.","title":"Upgrade Java"},{"location":"thehive/setup/installation/upgrade-from-4.x/#upgrade-or-install-elasticsearch","text":"I'm using a cluster Elasticsearch was mandatory for cluster of TheHive 5.x. Unless an update might is necessary, you can go to Upgrade Cassandra . Follow the installation process to install and configure the required version.","title":"Upgrade or install Elasticsearch"},{"location":"thehive/setup/installation/upgrade-from-4.x/#upgrade-cassandra","text":"I'm using a cluster Follow this part for all nodes of the Cassandra cluster, and ensure to restart sucessfully all nodes of the cluster before upgrading all nodes of TheHive cluster to version 5.","title":"Upgrade Cassandra"},{"location":"thehive/setup/installation/upgrade-from-4.x/#backup-configuration-file","text":"Save the existing configuration file for Cassandra 3.x. It will be used later to configure Cassandra 4: sudo cp /etc/cassandra/cassandra.yaml /etc/cassandra/cassandra3.yaml.bak","title":"Backup configuration file"},{"location":"thehive/setup/installation/upgrade-from-4.x/#install-cassandra","text":"Follow the installation process to install the required version. During the installation process, replace existing configuration files ( as the old configuration is saved ): Configuration file '/etc/cassandra/cassandra.yaml' ==> Modified (by you or by a script) since installation. ==> Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version. *** cassandra.yaml (Y/I/N/O/D/Z) [default=N] ? Y","title":"Install Cassandra"},{"location":"thehive/setup/installation/upgrade-from-4.x/#configuration","text":"Update the new configuration file, and ensure the following parameters are correctly set with these values: cluster_name: 'thp' num_tokens: 256 Info If you had a more customized configuration file for Cassandra 3.x, review all the file and ensure to adapt it accordingly.","title":"Configuration"},{"location":"thehive/setup/installation/upgrade-from-4.x/#start-the-service","text":"sudo systemctl start cassandra","title":"Start the service"},{"location":"thehive/setup/installation/upgrade-from-4.x/#upgrade-sstables","text":"On each Cassandra nodes, upgrade the sstables: nodetool upgradesstables Then repair the keyspaces: nodetool repair --full","title":"Upgrade sstables"},{"location":"thehive/setup/installation/upgrade-from-4.x/#install-thehive","text":"I'm using a cluster Before starting this part of the guide, ensure your Cassandra cluster is fully operational, by running the command nodetool status nodetool status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 10.1.1.2 1.41 GiB 256 100.0% ba6daa4e-6d14-4b21-a06c-d01b3bdd659d rack1 UN 10.1.1.3 1.39 GiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444043eb1cd rack1 UN 10.1.1.4 1.36 GiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252aeb86 rack1 Then, we recommend stopping all existing nodes of TheHive (4.x) and then, upgrading and starting only one node to TheHive 5. When everything works fine with this node, Other nodes can be updated and started.","title":"Install thehive"},{"location":"thehive/setup/installation/upgrade-from-4.x/#prepare-for-the-new-installation","text":"TheHive configuration file: /etc/thehive/application.conf Starting with TheHive 5.0.0, configuration has been simplified; most of the administration parameters can be configured directly in the UI. The configuration file ( /etc/thehive/application.conf ) may only contain the necessary information to start the application successfully; which are: Secret Database Indexing engine File storage Enabled connectors Akka configuration in the case of a cluster Authentication, Webhooks, Cortex and MISP configurations can be set in the UI. Note Cortex and Misp connector module keys were renamed from play.modules.enabled to scalligraph.modules Standalone server Cluster Save your current configuration file: sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak For the current use case, i.e., a standalone server, the final configuration file should look like this: sample of /etc/thehive/application.conf # TheHive configuration - application.conf # # # This is the default configuration file. # This is prepared to run with all services locally: # - Cassandra for the database # - Elasticsearch for index engine # - File storage is local in /opt/thp/thehive/files # # If this is not your setup, please refer to the documentation at: # https://docs.thehive-project.org/thehive/ # # # Secret key - used by Play Framework # If TheHive is installed with DEB/RPM package, this is automatically generated # If TheHive is not installed from DEB or RPM packages run the following # command before starting thehive: # cat > /etc/thehive/secret.conf << _EOF_ # play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |# head -n 1)\" # _EOF_ include \"/etc/thehive/secret.conf\" # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default : thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } # Define the maximum size for an attachment accepted by TheHive play.http.parser.maxDiskBuffer = 1GB # Define maximum size of http request (except attachment) play.http.parser.maxMemoryBuffer = 10M # Service configuration application.baseUrl = \"http://localhost:9000\" play.http.context = \"/\" # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # commenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule Save your current configuration file: sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak For the current use case, i.e., a standalone server, the final configuration file should look like this: sample of /etc/thehive/application.conf # TheHive configuration - application.conf # # # This is the default configuration file. # This is prepared to run with all services locally: # - Cassandra for the database # - Elasticsearch for index engine # - File storage is local in /opt/thp/thehive/files # # If this is not your setup, please refer to the documentation at: # https://docs.thehive-project.org/thehive/ # # # Secret key - used by Play Framework # If TheHive is installed with DEB/RPM package, this is automatically generated # If TheHive is not installed from DEB or RPM packages run the following # command before starting thehive: # cat > /etc/thehive/secret.conf << _EOF_ # play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |# head -n 1)\" # _EOF_ include \"/etc/thehive/secret.conf\" # Database and index configuration # By default, TheHive is configured to connect to local Cassandra 4.x and a # local Elasticsearch services without authentication. db.janusgraph { storage { backend = cql hostname = [\"127.0.0.1\"] # Cassandra authentication (if configured) # username = \"thehive\" # password = \"password\" cql { cluster-name = thp keyspace = thehive } } index.search { backend = elasticsearch hostname = [\"127.0.0.1\"] index-name = thehive } } # Attachment storage configuration # By default, TheHive is configured to store files locally in the folder. # The path can be updated and should belong to the user/group running thehive service. (by default : thehive:thehive) storage { provider = localfs localfs.location = /opt/thp/thehive/files } # Define the maximum size for an attachment accepted by TheHive play.http.parser.maxDiskBuffer = 1GB # Define maximum size of http request (except attachment) play.http.parser.maxMemoryBuffer = 10M # Service configuration application.baseUrl = \"http://localhost:9000\" play.http.context = \"/\" # Additional modules # # TheHive is strongly integrated with Cortex and MISP. # Both modules are enabled by default. If not used, each one can be disabled by # commenting the configuration line. scalligraph.modules += org.thp.thehive.connector.cortex.CortexModule scalligraph.modules += org.thp.thehive.connector.misp.MispModule # Cluster configuration akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } ## seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Note By default, Cortex and MISP modules are enabled. If you won't use them or one of them, the corresponding line can be commented. Our recommendation : use the default configuration sample, update it with your custom-parameter values, and keep the old file to configure services in the web UI.","title":"Prepare for the new installation"},{"location":"thehive/setup/installation/upgrade-from-4.x/#specific-configuration-required-for-the-upgrade-only","text":"I'm using a cluster This part only concerns the first node , the one that will be started to perform the database and index upgrade. These lines should be added to the configuration file only while upgrading to version 5, and removed later on. db.janusgraph.forceDropAndRebuildIndex = true","title":"Specific configuration required (for the upgrade only)"},{"location":"thehive/setup/installation/upgrade-from-4.x/#install-thehive_1","text":"I'm using a cluster Follow these instructions for the first node of TheHive. When starting TheHive 5.x for the first time, ensure all nodes of database clusters are up and running correctly. Once the upgrade is successful with the first node, install and start TheHive on other nodes. I'm using DEB packages I'm using RPM packages Update the repository address wget -O- https://archives.strangebee.com/keys/strangebee.gpg | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg sudo rm /etc/apt/sources.list.d/thehive-project.list ; echo 'deb [signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.x main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list Install new package - old package of thehive4 will be removed sudo apt update sudo apt install thehive Add Cassandra repository keys sudo rpm --import https://archives.strangebee.com/keys/strangebee.gpg Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/strangebee.repo : /etc/yum.repos.d/strangebee.repo [ thehive ] enabled = 1 priority = 1 name = StrangeBee RPM repository baseurl = https://rpm.strangebee.com/thehive-5.x/noarch gpgkey = https://archives.strangebee.com/keys/strangebee.gpg gpgcheck = 1 Then install the package using yum : sudo yum install thehive During the installation, if you already prepared your configuration file during Prepare for the new installation chapter, continue without updating it with the maintainer's version. Configuration file '/etc/thehive/application.conf' ==> Modified (by you or by a script) since installation. ==> Package distributor has shipped an updated version. What would you like to do about it ? Your options are: Y or I : install the package maintainer's version N or O : keep your currently-installed version D : show the differences between the versions Z : start a shell to examine the situation The default action is to keep your current version. *** application.conf (Y/I/N/O/D/Z) [default=N] ? N","title":"Install TheHive"},{"location":"thehive/setup/installation/upgrade-from-4.x/#start-services","text":"sudo systemctl daemon-reload Start Cassandra (if not already started) sudo systemctl start cassandra Start Elasticsearch (if not already started) sudo systemctl start elasticsearch Once both database services are started successfully, start TheHive sudo systemctl start thehive The first start of TheHive 5.0.x can take some time When starting for the first time, TheHive is updating first the database schema, and proceed to reindexation. Both processes can take a certain time depending on the size of the database and the amount of data. Progression can be followed in log file /var/log/thehive/application.log . See Troubleshooting for more information.","title":"Start services"},{"location":"thehive/setup/installation/upgrade-from-4.x/#restart-the-service","text":"Once the service is started successfully, update the configuration file and remove the following lines: /etc/thehive/application.conf db.janusgraph.forceDropAndRebuildIndex = true Then restart TheHive: sudo systemctl restart thehive I'm using a cluster You can install and start TheHive on all other nodes.","title":"Restart the service"},{"location":"thehive/setup/installation/upgrade-from-4.x/#troubleshooting","text":"During the update, few logs can be seen in TheHive application.log file. Example of logs and what they mean [INFO] from org.janusgraph.graphdb.database.management.GraphIndexStatusWatcher in application-akka.actor.default-dispatcher-11 [|] Some key(s) on index global2 do not currently have status(es) [REGISTERED, ENABLED]: dateValue=INSTALLED,externalLink=INSTALLED,origin=INSTALLED,patternId=INSTALLED,revoked=INSTALLED,mandatory=INSTALLED,content=INSTALLED,isAttachment=INSTALLED,writable=INSTALLED,tactic=INSTALLED,stringValue=INSTALLED,owningOrganisation=INSTALLED,permissions=INSTALLED,actionRequired=INSTALLED,integerValue=INSTALLED,details=INSTALLED,locked=INSTALLED,slug=INSTALLED,cortexId=INSTALLED,owner=INSTALLED,workerId=INSTALLED,apikey=INSTALLED,level=INSTALLED,floatValue=INSTALLED,version=INSTALLED,occurDate=INSTALLED,url=INSTALLED,report=INSTALLED,tactics=INSTALLED,booleanValue=INSTALLED,cortexJobId=INSTALLED,category=INSTALLED,workerName=INSTALLED TheHive install indexes of the new schema in the database [INFO] from org.janusgraph.graphdb.olap.job.IndexRepairJob in Thread-97 [|] Index global2 metrics: success-tx: 1 doc-updates: 100 succeeded: 100 TheHive reindexes all data * UPDATE SCHEMA OF thehive-enterprise (1): Create initial values [INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [d471d8b643d17b6d|d88fe62679b77ab1] Adding initial values for GDPRDummy [..] [INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [|] Update graph in progress (100): Add pap and ignoreSimilarity to observables Migrating data from v4. to v5 [WARN] from org.thp.thehive.enterprise.services.LicenseSrv in main [ef39c95eaa6de532|0ccf187e40a4cd34] No license found No license found. This is a normal behavior during the upgrade from versions 4 to 5 INFO] from play.core.server.AkkaHttpServer in main [|] Listening for HTTP on /0:0:0:0:0:0:0:0:9000 The service is available. Users/Administrators can log in [INFO] from org.thp.thehive.connector.cortex.services.CortexDataImportActor in application-akka.actor.default-dispatcher-16 [|] Analyzer templates already present (found 203), skipping [..] [INFO] from org.thp.thehive.services.ttp.PatternImportActor in application-akka.actor.default-dispatcher-14 [|] Import finished, 707 patterns imported Few operations are processed after making the service available, like installing MITRE Enterprise ATT&CK patterns catalog or Analyzers templates. [ERROR] from org.janusgraph.diskstorage.log.util.ProcessMessageJob in pool-22-thread-1 [|] Encountered exception when processing message [Message@2022-03-24T16:50:40.655134Z:7f0001017672-ubuntu2=0x809F9F0568850528850550850558850570850600850610850618850650850668850710850738850758850760850808850900850910850A60850A70850A78850B00850B08853520853B3885150E8941608541688541788542088542688542708581] by reader [org.janusgraph.graphdb.database.management.ManagementLogger@3e1a6eae]:java.lang.IllegalStateException: Cannot access element because its enclosing transaction is closed and unbound at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.getNextTx(StandardJanusGraphTx.java:380) at org.janusgraph.graphdb.vertices.AbstractVertex.it(AbstractVertex.java:61) at org.janusgraph.graphdb.relations.CacheVertexProperty.<init>(CacheVertexProperty.java:38) at org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:88) at org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:71) at org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:57) at org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:45) at org.janusgraph.graphdb.types.vertices.JanusGraphSchemaVertex.getDefinition(JanusGraphSchemaVertex.java:94) at org.janusgraph.graphdb.transaction.StandardJanusGraphTx.expireSchemaElement(StandardJanusGraphTx.java:1599) at org.janusgraph.graphdb.database.management.ManagementLogger.read(ManagementLogger.java:97) at org.janusgraph.diskstorage.log.util.ProcessMessageJob.run(ProcessMessageJob.java:46) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) During indexing, Janusgraph may display this message, this error is coming from a bug in janusgraph , don't mind it as the indexing will continue normally. This will have no impact on TheHive","title":"Troubleshooting"},{"location":"thehive/setup/operations/backup-restore/","text":"Backup and restore # This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html Overview # To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index. Cassandra # Pre requisites # To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..] Backup # Following actions should be performed to backup the data successfully: stop TheHive service Save the database schema Create a snapshot Save the data and the schema Stop TheHive service # systemctl stop thehive Save the database schema # This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql Create a snapshot and an archive # Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname> Example # Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi Restore data # Pre requisites # Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten) Restore # Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done Index # Several solutions exist regarding the index: Save the Elasticsearch index and restore it ; follow Elasticsearch guides to perform this action Rebuild the index on the new server, when TheHive start for the first time. Rebuild the index # Once Cassandra database is restored, update the configuration of TheHive to rebuild the index. These lines should be added to the configuration file only for the first start of TheHive application, and removed later on. extract from /etc/thehive/application.conf db.janusgraph.forceDropAndRebuildIndex = true Once started, both lines should be removed or commented from the configuration file of TheHive Files # Backup # Wether you use local or distributed files system storage, copy the content of the folder/bucket. Restore # Restore the saved files into the destination folder/bucket that will be used by TheHive. Troubleshooting # The first start can take some time, especially it the application has to rebuild the index. Refer to this troubleshooting section to ensure everything goes well with reindexation.","title":"Backup and restore"},{"location":"thehive/setup/operations/backup-restore/#backup-and-restore","text":"This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html","title":"Backup and restore"},{"location":"thehive/setup/operations/backup-restore/#overview","text":"To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index.","title":"Overview"},{"location":"thehive/setup/operations/backup-restore/#cassandra","text":"","title":"Cassandra"},{"location":"thehive/setup/operations/backup-restore/#pre-requisites","text":"To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..]","title":"Pre requisites"},{"location":"thehive/setup/operations/backup-restore/#backup","text":"Following actions should be performed to backup the data successfully: stop TheHive service Save the database schema Create a snapshot Save the data and the schema","title":"Backup"},{"location":"thehive/setup/operations/backup-restore/#stop-thehive-service","text":"systemctl stop thehive","title":"Stop TheHive service"},{"location":"thehive/setup/operations/backup-restore/#save-the-database-schema","text":"This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql","title":"Save the database schema"},{"location":"thehive/setup/operations/backup-restore/#create-a-snapshot-and-an-archive","text":"Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname>","title":"Create a snapshot and an archive"},{"location":"thehive/setup/operations/backup-restore/#example","text":"Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi","title":"Example"},{"location":"thehive/setup/operations/backup-restore/#restore-data","text":"","title":"Restore data"},{"location":"thehive/setup/operations/backup-restore/#pre-requisites_1","text":"Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten)","title":"Pre requisites"},{"location":"thehive/setup/operations/backup-restore/#restore","text":"Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done","title":"Restore"},{"location":"thehive/setup/operations/backup-restore/#index","text":"Several solutions exist regarding the index: Save the Elasticsearch index and restore it ; follow Elasticsearch guides to perform this action Rebuild the index on the new server, when TheHive start for the first time.","title":"Index"},{"location":"thehive/setup/operations/backup-restore/#rebuild-the-index","text":"Once Cassandra database is restored, update the configuration of TheHive to rebuild the index. These lines should be added to the configuration file only for the first start of TheHive application, and removed later on. extract from /etc/thehive/application.conf db.janusgraph.forceDropAndRebuildIndex = true Once started, both lines should be removed or commented from the configuration file of TheHive","title":"Rebuild the index"},{"location":"thehive/setup/operations/backup-restore/#files","text":"","title":"Files"},{"location":"thehive/setup/operations/backup-restore/#backup_1","text":"Wether you use local or distributed files system storage, copy the content of the folder/bucket.","title":"Backup"},{"location":"thehive/setup/operations/backup-restore/#restore_1","text":"Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Restore"},{"location":"thehive/setup/operations/backup-restore/#troubleshooting","text":"The first start can take some time, especially it the application has to rebuild the index. Refer to this troubleshooting section to ensure everything goes well with reindexation.","title":"Troubleshooting"},{"location":"thehive/setup/operations/cassandra-cluster/","text":"Cassandra cluster operations # This guide contains all details to: Add a node to an existing cluster Remove an alive node Remove a dead node Add a node # You can add a Cassandra node in an existing cluster without downtime. Install a new node with the same configuration file except for listen_address and rpc_address . Set the first node IP in the seeds ( seed_provider / parameters / seeds ). Start the node Check the cluster status Display Cassandra nodes status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 3.68 GiB 256 100.0% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UJ 172.24.0.3 89.54 MiB 256 ? f192ef8f-a4fd-4e24-99a7-0f92605a7cb6 rack1 Then when the node is fully operationnal: Display Cassandra nodes status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 3.68 GiB 256 48.8% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 1.91 GiB 256 51.2% f192ef8f-a4fd-4e24-99a7-0f92605a7cb6 rack1 Creating a cluster from a standalone server In the particular case of adding a node to create a cluster from a standalone server, there is a pre-requisite ; The Cassandra configuration of the existing standalone server, should be updated to ensure that listen_address and rpc_address are set with the IP address of the host and not the localhost address anymore Increase replication factor # When you have more than one node, you can increase the replication factor. This increase the number of copies and thus the cluster is more tolerent to hardware fault. The more copies there are, the more data is used on disk and the more resilient is the cluster, but the write access could be slower. To increate the replication factor, connect to Cassandra using cqlsh and type ( thehive is the name of the keyspace defined in application.conf): ALTER KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Then, on each Cassandra nodes, run a nodetool repair -full It is recommended to increase replication factor for system keyspaces. You can display current value with the following query: Display keyspaces information using cql command > SELECT * FROM system_schema . keyspaces ; keyspace_name | durable_writes | replication --------------------+----------------+------------------------------------------------------------------------------------- system_auth | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } system_schema | True | { 'class' : 'org.apache.cassandra.locator.LocalStrategy' } system_distributed | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } system | True | { 'class' : 'org.apache.cassandra.locator.LocalStrategy' } system_traces | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } thehive | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } Remove an alive node # First ensure that the replication factor is compatible with the target number of nodes. If it is not the case, you can update replication factor. Then run in the node you want to remove: nodetool decommission Remove a dead node # If a node has crash and cannot be repaired, you can remove it from a cluster with the command nodetool removenode followed by the node id : Removing a dead node # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack DN 172.24.0.4 3.69 GiB 256 72.9% c0a96997-ea44-469e-a3e7-5660ed566ff6 rack1 UN 172.24.0.2 5.04 GiB 256 67.8% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 2.27 GiB 256 59.4% 9d5e4130-8fe0-4626-83eb-a059c5ec2872 rack1 # nodetool removenode c0a96997-ea44-469e-a3e7-5660ed566ff6 # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 6.61 GiB 256 100.0% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 3.93 GiB 256 100.0% 9d5e4130-8fe0-4626-83eb-a059c5ec2872 rack1","title":"Operations on a Cassandra Cluster"},{"location":"thehive/setup/operations/cassandra-cluster/#cassandra-cluster-operations","text":"This guide contains all details to: Add a node to an existing cluster Remove an alive node Remove a dead node","title":"Cassandra cluster operations"},{"location":"thehive/setup/operations/cassandra-cluster/#add-a-node","text":"You can add a Cassandra node in an existing cluster without downtime. Install a new node with the same configuration file except for listen_address and rpc_address . Set the first node IP in the seeds ( seed_provider / parameters / seeds ). Start the node Check the cluster status Display Cassandra nodes status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 3.68 GiB 256 100.0% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UJ 172.24.0.3 89.54 MiB 256 ? f192ef8f-a4fd-4e24-99a7-0f92605a7cb6 rack1 Then when the node is fully operationnal: Display Cassandra nodes status # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 3.68 GiB 256 48.8% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 1.91 GiB 256 51.2% f192ef8f-a4fd-4e24-99a7-0f92605a7cb6 rack1 Creating a cluster from a standalone server In the particular case of adding a node to create a cluster from a standalone server, there is a pre-requisite ; The Cassandra configuration of the existing standalone server, should be updated to ensure that listen_address and rpc_address are set with the IP address of the host and not the localhost address anymore","title":"Add a node"},{"location":"thehive/setup/operations/cassandra-cluster/#increase-replication-factor","text":"When you have more than one node, you can increase the replication factor. This increase the number of copies and thus the cluster is more tolerent to hardware fault. The more copies there are, the more data is used on disk and the more resilient is the cluster, but the write access could be slower. To increate the replication factor, connect to Cassandra using cqlsh and type ( thehive is the name of the keyspace defined in application.conf): ALTER KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Then, on each Cassandra nodes, run a nodetool repair -full It is recommended to increase replication factor for system keyspaces. You can display current value with the following query: Display keyspaces information using cql command > SELECT * FROM system_schema . keyspaces ; keyspace_name | durable_writes | replication --------------------+----------------+------------------------------------------------------------------------------------- system_auth | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } system_schema | True | { 'class' : 'org.apache.cassandra.locator.LocalStrategy' } system_distributed | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } system | True | { 'class' : 'org.apache.cassandra.locator.LocalStrategy' } system_traces | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' } thehive | True | { 'class' : 'org.apache.cassandra.locator.SimpleStrategy' , 'replication_factor' : '2' }","title":"Increase replication factor"},{"location":"thehive/setup/operations/cassandra-cluster/#remove-an-alive-node","text":"First ensure that the replication factor is compatible with the target number of nodes. If it is not the case, you can update replication factor. Then run in the node you want to remove: nodetool decommission","title":"Remove an alive node"},{"location":"thehive/setup/operations/cassandra-cluster/#remove-a-dead-node","text":"If a node has crash and cannot be repaired, you can remove it from a cluster with the command nodetool removenode followed by the node id : Removing a dead node # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack DN 172.24.0.4 3.69 GiB 256 72.9% c0a96997-ea44-469e-a3e7-5660ed566ff6 rack1 UN 172.24.0.2 5.04 GiB 256 67.8% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 2.27 GiB 256 59.4% 9d5e4130-8fe0-4626-83eb-a059c5ec2872 rack1 # nodetool removenode c0a96997-ea44-469e-a3e7-5660ed566ff6 # nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 172.24.0.2 6.61 GiB 256 100.0% 048a870f-d6d5-405e-8d0d-43dbc12be747 rack1 UN 172.24.0.3 3.93 GiB 256 100.0% 9d5e4130-8fe0-4626-83eb-a059c5ec2872 rack1","title":"Remove a dead node"},{"location":"thehive/setup/operations/cassandra-security/","text":"Security in Apache Cassandra # References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl Authentication with Cassandra # Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } } Cassandra node to node encryption # This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false Cassandra dedicated port for SSL (optional) # Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file). Client to node encryption # This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes. Requirements # The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution. Configuring Cassandra # Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted. Configuring TheHive # db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Security in Apache Cassandra"},{"location":"thehive/setup/operations/cassandra-security/#security-in-apache-cassandra","text":"References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl","title":"Security in Apache Cassandra"},{"location":"thehive/setup/operations/cassandra-security/#authentication-with-cassandra","text":"Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } }","title":"Authentication with Cassandra"},{"location":"thehive/setup/operations/cassandra-security/#cassandra-node-to-node-encryption","text":"This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false","title":"Cassandra node to node encryption"},{"location":"thehive/setup/operations/cassandra-security/#cassandra-dedicated-port-for-ssl-optional","text":"Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file).","title":"Cassandra dedicated port for SSL (optional)"},{"location":"thehive/setup/operations/cassandra-security/#client-to-node-encryption","text":"This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes.","title":"Client to node encryption"},{"location":"thehive/setup/operations/cassandra-security/#requirements","text":"The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution.","title":"Requirements"},{"location":"thehive/setup/operations/cassandra-security/#configuring-cassandra","text":"Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted.","title":"Configuring Cassandra"},{"location":"thehive/setup/operations/cassandra-security/#configuring-thehive","text":"db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Configuring TheHive"},{"location":"thehive/setup/operations/fail2ban/","text":"Fail2ban # Adding TheHive into Fail2Ban # Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload Manage banned IP addresses # Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Fail2ban"},{"location":"thehive/setup/operations/fail2ban/#fail2ban","text":"","title":"Fail2ban"},{"location":"thehive/setup/operations/fail2ban/#adding-thehive-into-fail2ban","text":"Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload","title":"Adding TheHive into Fail2Ban"},{"location":"thehive/setup/operations/fail2ban/#manage-banned-ip-addresses","text":"Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Manage banned IP addresses"},{"location":"thehive/setup/operations/https/","text":"","title":"Https"},{"location":"thehive/setup/operations/minio-cluster/","text":"MinIO cluster operations # Replace a disk # If a disk is faulty, it can be replaced without downtime as MinIO supports hot-swapping disks: 1. unmount the disk 2. replace it with healthy disk and mount the partition in the same location 3. check if MinIO has recognized the new disk in MinIO logs 4. run a disk heal with the command mc admin heal . Replace a node # If a server has crashed and cannot be recovered, you can install a new server with MinIO. Ensure the new node is accessible like the old one, with same ip/hostname. Once the service is started and it has joined the cluster, run a disk heal with the command mc admin heal . Add a node to cluster # Adding a new server in an existing cluster, requires restarting all MinIO nodes. The configuration of all nodes must be modififed to take in account the new node. Once the cluster is started, run a disk heal with the command mc admin heal . MinIO strongly recommends restarting all nodes simultaneously. Do not perform \"rolling\" (e.g. one node at a time) restarts. Remove a node from cluster # Removing a node from a cluster follows the same requirements than adding a node: update the configuration file and restart all the cluster. MINIO and high availability # Only one S3 endpoint can be configured in TheHive. To setup high availability on all nodes, we recommend using load lalancers a virtual IP to connect to MINIO nodes.","title":"Operations on a MINIO cluster"},{"location":"thehive/setup/operations/minio-cluster/#minio-cluster-operations","text":"","title":"MinIO cluster operations"},{"location":"thehive/setup/operations/minio-cluster/#replace-a-disk","text":"If a disk is faulty, it can be replaced without downtime as MinIO supports hot-swapping disks: 1. unmount the disk 2. replace it with healthy disk and mount the partition in the same location 3. check if MinIO has recognized the new disk in MinIO logs 4. run a disk heal with the command mc admin heal .","title":"Replace a disk"},{"location":"thehive/setup/operations/minio-cluster/#replace-a-node","text":"If a server has crashed and cannot be recovered, you can install a new server with MinIO. Ensure the new node is accessible like the old one, with same ip/hostname. Once the service is started and it has joined the cluster, run a disk heal with the command mc admin heal .","title":"Replace a node"},{"location":"thehive/setup/operations/minio-cluster/#add-a-node-to-cluster","text":"Adding a new server in an existing cluster, requires restarting all MinIO nodes. The configuration of all nodes must be modififed to take in account the new node. Once the cluster is started, run a disk heal with the command mc admin heal . MinIO strongly recommends restarting all nodes simultaneously. Do not perform \"rolling\" (e.g. one node at a time) restarts.","title":"Add a node to cluster"},{"location":"thehive/setup/operations/minio-cluster/#remove-a-node-from-cluster","text":"Removing a node from a cluster follows the same requirements than adding a node: update the configuration file and restart all the cluster.","title":"Remove a node from cluster"},{"location":"thehive/setup/operations/minio-cluster/#minio-and-high-availability","text":"Only one S3 endpoint can be configured in TheHive. To setup high availability on all nodes, we recommend using load lalancers a virtual IP to connect to MINIO nodes.","title":"MINIO and high availability"},{"location":"thehive/setup/operations/troubleshooting/","text":"Troubleshooting # For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS. Stop TheHive service and ensure it is stopped # service thehive stop Ensure the service is stopped with the following command: service thehive status Renew application.log file # in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak Update log configuration # Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file. Restart the service # service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop. Save the logs # Copy the log file in a safe place. cp /var/log/thehive/application.log /root Share it with us # Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces Revert # To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Troubleshooting"},{"location":"thehive/setup/operations/troubleshooting/#troubleshooting","text":"For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS.","title":"Troubleshooting"},{"location":"thehive/setup/operations/troubleshooting/#stop-thehive-service-and-ensure-it-is-stopped","text":"service thehive stop Ensure the service is stopped with the following command: service thehive status","title":"Stop TheHive service and ensure it is stopped"},{"location":"thehive/setup/operations/troubleshooting/#renew-applicationlog-file","text":"in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak","title":"Renew application.log file"},{"location":"thehive/setup/operations/troubleshooting/#update-log-configuration","text":"Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file.","title":"Update log configuration"},{"location":"thehive/setup/operations/troubleshooting/#restart-the-service","text":"service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop.","title":"Restart the service"},{"location":"thehive/setup/operations/troubleshooting/#save-the-logs","text":"Copy the log file in a safe place. cp /var/log/thehive/application.log /root","title":"Save the logs"},{"location":"thehive/setup/operations/troubleshooting/#share-it-with-us","text":"Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces","title":"Share it with us"},{"location":"thehive/setup/operations/troubleshooting/#revert","text":"To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Revert"}]}